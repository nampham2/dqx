{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DQX - Data Quality Excellence","text":"<p>Data quality as code. Works with your warehouse, scales with your needs.</p> <p> </p>"},{"location":"#why-dqx","title":"Why DQX?","text":"<ul> <li>Write validation logic as testable Python functions - No more complex SQL scripts scattered across your codebase</li> <li>Execute efficiently on any SQL backend - DuckDB, BigQuery, Snowflake, or your existing data warehouse</li> <li>No clusters or complex infrastructure needed - Runs wherever your data lives</li> <li>Integrates seamlessly with existing workflows - Drop it into your current pipeline</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>pip install dqlib\n</code></pre> <p>Define your data quality checks as Python functions:</p> <pre><code>import pyarrow as pa\nfrom dqx.api import check, VerificationSuite, MetricProvider, Context\nfrom dqx.common import ResultKey\nfrom dqx.datasource import DuckRelationDataSource\nfrom dqx.orm.repositories import InMemoryMetricDB\n\n\n# Define your validation rules\n@check(name=\"Revenue integrity\")\ndef validate_revenue(mp: MetricProvider, ctx: Context) -&gt; None:\n    # Verify reported revenue is positive\n    reported = mp.sum(\"revenue\")\n    ctx.assert_that(reported).where(\n        name=\"Revenue is positive\", severity=\"P0\"\n    ).is_positive()\n\n    # Check average transaction size is reasonable\n    avg_revenue = mp.average(\"revenue\")\n    ctx.assert_that(avg_revenue).where(\n        name=\"Average transaction size\", severity=\"P1\"\n    ).is_between(10, 100)\n\n\n# Your own metric store\ndb = InMemoryMetricDB()\nsuite = VerificationSuite([validate_revenue], db, \"Daily validation\")\n\n# Data comes from your warehouse\ndata = pa.Table.from_pydict(\n    {\"price\": [10.5, 20.0, 15.5], \"quantity\": [2, 1, 3], \"revenue\": [21.0, 20.0, 46.5]}\n)\ndatasource = DuckRelationDataSource.from_arrow(data)\n\n# Validate your data\nsuite.run([datasource], ResultKey())\n# \u2713 Revenue integrity: OK\n</code></pre>"},{"location":"#real-world-examples","title":"Real-World Examples","text":""},{"location":"#1-data-completeness","title":"1. Data Completeness","text":""},{"location":"#monitor-critical-fields-arent-missing","title":"Monitor critical fields aren't missing","text":"<pre><code>@check(name=\"Customer data quality\")\ndef check_completeness(mp: MetricProvider, ctx: Context) -&gt; None:\n    # Flag if more than 5% of emails are missing\n    null_rate = mp.null_count(\"email\") / mp.num_rows()\n    ctx.assert_that(null_rate).where(name=\"Email completeness\", severity=\"P0\").is_lt(\n        0.05\n    )\n\n    # Ensure all orders have customer IDs\n    ctx.assert_that(mp.null_count(\"customer_id\")).where(\n        name=\"Customer ID required\", severity=\"P0\"\n    ).is_eq(0)\n</code></pre>"},{"location":"#2-revenue-integrity","title":"2. Revenue Integrity","text":""},{"location":"#catch-calculation-errors-in-financial-data","title":"Catch calculation errors in financial data","text":"<pre><code>@check(name=\"Financial accuracy\")\ndef validate_financials(mp: MetricProvider, ctx: Context) -&gt; None:\n    # Verify totals match across systems\n    total_revenue = mp.sum(\"revenue\")\n    total_collected = mp.sum(\"payments\")\n\n    ctx.assert_that(total_collected / total_revenue).where(\n        name=\"Payment collection rate\", severity=\"P1\"\n    ).is_between(\n        0.95, 1.05\n    )  # 5% tolerance\n\n    # Check for negative prices\n    ctx.assert_that(mp.minimum(\"price\")).where(\n        name=\"No negative prices\", severity=\"P0\"\n    ).is_geq(0)\n</code></pre>"},{"location":"#3-trend-monitoring","title":"3. Trend Monitoring","text":""},{"location":"#alert-on-unexpected-metric-changes","title":"Alert on unexpected metric changes","text":"<pre><code>@check(name=\"Business metrics stability\")\ndef monitor_trends(mp: MetricProvider, ctx: Context) -&gt; None:\n    # Alert on significant daily changes\n    daily_change = mp.sum(\"revenue\") / mp.sum(\"revenue\", lag=1)\n    ctx.assert_that(daily_change).where(\n        name=\"Daily revenue stability\", severity=\"P0\"\n    ).is_between(\n        0.8, 1.2\n    )  # \u00b120% change\n\n    # Track week-over-week growth\n    wow_change = mp.sum(\"revenue\") / mp.sum(\"revenue\", lag=7)\n    ctx.assert_that(wow_change).where(\n        name=\"Weekly revenue trend\", severity=\"P1\"\n    ).is_geq(\n        0.95\n    )  # Allow 5% decline\n</code></pre>"},{"location":"#4-cross-dataset-validation","title":"4. Cross-Dataset Validation","text":""},{"location":"#ensure-consistency-across-environments","title":"Ensure consistency across environments","text":"<pre><code>@check(name=\"Production vs Staging\", datasets=[\"production\", \"staging\"])\ndef validate_environments(mp: MetricProvider, ctx: Context) -&gt; None:\n    # Compare row counts\n    prod_count = mp.num_rows(dataset=\"production\")\n    staging_count = mp.num_rows(dataset=\"staging\")\n\n    ctx.assert_that(prod_count).where(name=\"Row count match\", severity=\"P1\").is_between(\n        staging_count - 100, staging_count + 100\n    )  # Allow 100 row difference\n\n    # Verify key metrics align\n    prod_revenue = mp.sum(\"revenue\", dataset=\"production\")\n    staging_revenue = mp.sum(\"revenue\", dataset=\"staging\")\n\n    ctx.assert_that((prod_revenue - staging_revenue) / prod_revenue).where(\n        name=\"Revenue consistency\", severity=\"P0\"\n    ).is_lt(\n        0.01\n    )  # Less than 1% difference\n</code></pre>"},{"location":"#5-data-quality-slas","title":"5. Data Quality SLAs","text":""},{"location":"#track-quality-metrics-with-severity-levels","title":"Track quality metrics with severity levels","text":"<pre><code>@check(name=\"Data quality SLAs\")\ndef enforce_slas(mp: MetricProvider, ctx: Context) -&gt; None:\n    # P0: Critical - No duplicate transactions\n    ctx.assert_that(mp.duplicate_count([\"transaction_id\"])).where(\n        name=\"Transaction uniqueness\", severity=\"P0\"\n    ).is_eq(0)\n\n    # P1: High - Recent activity\n    recent_count = mp.count_values(\"status\", \"active\")\n    total_count = mp.num_rows()\n    active_rate = recent_count / total_count\n\n    ctx.assert_that(active_rate).where(\n        name=\"Active record percentage\", severity=\"P1\"\n    ).is_gt(\n        0.5\n    )  # At least 50% active\n\n    # P2: Medium - Cardinality checks\n    unique_users = mp.unique_count(\"user_id\")\n    ctx.assert_that(unique_users).where(\n        name=\"Active user threshold\", severity=\"P2\"\n    ).is_gt(1000)\n</code></pre>"},{"location":"#quick-reference","title":"Quick Reference","text":""},{"location":"#available-metrics","title":"Available Metrics","text":"Metric Description Example <code>num_rows()</code> Total row count <code>mp.num_rows()</code> <code>sum(col)</code> Sum of values <code>mp.sum(\"revenue\")</code> <code>average(col)</code> Mean value <code>mp.average(\"price\")</code> <code>minimum(col)</code> / <code>maximum(col)</code> Min/max values <code>mp.minimum(\"age\")</code> <code>first(col)</code> First value in column <code>mp.first(\"timestamp\")</code> <code>variance(col)</code> Statistical variance <code>mp.variance(\"score\")</code> <code>null_count(col)</code> Count of null values <code>mp.null_count(\"email\")</code> <code>duplicate_count([cols])</code> Count of duplicate rows <code>mp.duplicate_count([\"id\"])</code> <code>count_values(col, val)</code> Count specific values <code>mp.count_values(\"status\", \"active\")</code> <code>unique_count(col)</code> Distinct value count <code>mp.unique_count(\"user_id\")</code>"},{"location":"#extended-metrics","title":"Extended Metrics","text":"Metric Description Example <code>ext.day_over_day(metric)</code> Day-over-day change <code>mp.ext.day_over_day(mp.sum(\"revenue\"))</code> <code>ext.week_over_week(metric)</code> Week-over-week change <code>mp.ext.week_over_week(mp.average(\"price\"))</code> <code>ext.stddev(metric, offset, n)</code> Standard deviation over window <code>mp.ext.stddev(mp.sum(\"sales\"), offset=0, n=7)</code>"},{"location":"#available-assertions","title":"Available Assertions","text":"Assertion Description Example <code>is_eq(value, tol)</code> Equals with tolerance <code>.is_eq(100, tol=0.01)</code> <code>is_neq(value, tol)</code> Not equals with tolerance <code>.is_neq(100, tol=0.01)</code> <code>is_between(min, max)</code> In range (inclusive) <code>.is_between(0, 100)</code> <code>is_positive()</code> Greater than zero <code>.is_positive()</code> <code>is_zero()</code> Equals zero <code>.is_zero()</code> <code>is_negative()</code> Less than zero <code>.is_negative()</code> <code>is_none()</code> Equals None <code>.is_none()</code> <code>is_not_none()</code> Not equals None <code>.is_not_none()</code> <code>is_gt(val)</code> / <code>is_geq(val)</code> Greater than (or equal) <code>.is_gt(0.95)</code> <code>is_lt(val)</code> / <code>is_leq(val)</code> Less than (or equal) <code>.is_lt(0.05)</code> <code>noop()</code> No validation (collect only) <code>.noop()</code>"},{"location":"#license","title":"License","text":"<p>MIT License. See LICENSE for details.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for DQX (Data Quality Excellence).</p>"},{"location":"api-reference/#core-api","title":"Core API","text":""},{"location":"api-reference/#dataqualityvalidator","title":"DataQualityValidator","text":"<p>The main entry point for data validation.</p> <pre><code>from dqx import DataQualityValidator\n\nvalidator = DataQualityValidator(\n    name=\"MyValidator\",\n    description=\"Validates customer data\",\n    fail_fast=False,\n    parallel=True,\n)\n</code></pre>"},{"location":"api-reference/#parameters","title":"Parameters","text":"<ul> <li><code>name</code> (str, optional): Name of the validator instance</li> <li><code>description</code> (str, optional): Description of the validator's purpose</li> <li><code>fail_fast</code> (bool, default=False): Stop validation on first failure</li> <li><code>parallel</code> (bool, default=True): Enable parallel processing</li> </ul>"},{"location":"api-reference/#methods","title":"Methods","text":""},{"location":"api-reference/#validate","title":"validate()","text":"<p>Run validation checks on data.</p> <pre><code>results = validator.validate(data, checks, sample_size=None, context=None)\n</code></pre> <p>Parameters: - <code>data</code>: DataFrame, file path, or data source to validate - <code>checks</code>: List of Check objects or CheckGroup - <code>sample_size</code> (int, optional): Number of rows to sample - <code>context</code> (dict, optional): Additional context for validation</p> <p>Returns: <code>ValidationResults</code> object</p>"},{"location":"api-reference/#create_checks","title":"create_checks()","text":"<p>Create a check builder for fluent API.</p> <pre><code>checks = (\n    validator.create_checks(data).is_not_null(\"column1\").is_unique(\"column2\").build()\n)\n</code></pre>"},{"location":"api-reference/#check-classes","title":"Check Classes","text":""},{"location":"api-reference/#basecheck","title":"BaseCheck","text":"<p>Abstract base class for all checks.</p> <pre><code>class BaseCheck:\n    def __init__(self, name, description=None, severity=\"error\"):\n        self.name = name\n        self.description = description\n        self.severity = severity\n</code></pre>"},{"location":"api-reference/#built-in-checks","title":"Built-in Checks","text":""},{"location":"api-reference/#notnullcheck","title":"NotNullCheck","text":"<pre><code>from dqx.checks import NotNullCheck\n\ncheck = NotNullCheck(column=\"user_id\", name=\"User ID Required\", severity=\"critical\")\n</code></pre>"},{"location":"api-reference/#rangecheck","title":"RangeCheck","text":"<pre><code>from dqx.checks import RangeCheck\n\ncheck = RangeCheck(\n    column=\"age\", min_value=0, max_value=120, inclusive=True, name=\"Valid Age Range\"\n)\n</code></pre>"},{"location":"api-reference/#patterncheck","title":"PatternCheck","text":"<pre><code>from dqx.checks import PatternCheck\n\ncheck = PatternCheck(\n    column=\"email\", pattern=r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\", name=\"Valid Email Format\"\n)\n</code></pre>"},{"location":"api-reference/#uniquecheck","title":"UniqueCheck","text":"<pre><code>from dqx.checks import UniqueCheck\n\ncheck = UniqueCheck(\n    columns=[\"user_id\"], name=\"Unique User ID\"  # Can be single column or list\n)\n</code></pre>"},{"location":"api-reference/#checkbuilder-api","title":"CheckBuilder API","text":"<p>Fluent interface for building checks.</p> <pre><code>checks = (\n    CheckBuilder(df)\n    # Null checks\n    .is_not_null(\"column\")\n    .are_not_null([\"col1\", \"col2\"])\n    .has_no_nulls()\n    # Range checks\n    .is_between(\"age\", 0, 120)\n    .is_positive(\"amount\")\n    .is_negative(\"loss\")\n    .is_greater_than(\"score\", 50)\n    .is_less_than(\"cost\", 1000)\n    # Pattern checks\n    .matches_pattern(\"email\", r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\")\n    .matches_date_format(\"date\", \"%Y-%m-%d\")\n    .has_length(\"zip_code\", 5)\n    .starts_with(\"product_code\", \"PRD\")\n    .ends_with(\"filename\", \".csv\")\n    # Uniqueness checks\n    .is_unique(\"id\")\n    .has_unique_combination([\"first_name\", \"last_name\"])\n    .has_no_duplicates()\n    # Statistical checks\n    .mean_between(\"score\", 70, 90)\n    .std_dev_less_than(\"variance\", 10)\n    .percentile_between(\"income\", 0.25, 10000, 50000)\n    # Build final check list\n    .build()\n)\n</code></pre>"},{"location":"api-reference/#validationresults","title":"ValidationResults","text":"<p>Results container with analysis methods.</p> <pre><code>class ValidationResults:\n    @property\n    def passed(self) -&gt; bool:\n        \"\"\"Whether all checks passed\"\"\"\n\n    @property\n    def failed_count(self) -&gt; int:\n        \"\"\"Number of failed checks\"\"\"\n\n    @property\n    def pass_rate(self) -&gt; float:\n        \"\"\"Percentage of passed checks\"\"\"\n\n    def summary(self) -&gt; dict:\n        \"\"\"Get summary statistics\"\"\"\n\n    def failed_checks(self) -&gt; List[CheckResult]:\n        \"\"\"Get only failed check results\"\"\"\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert results to DataFrame\"\"\"\n\n    def to_json(self, filepath: str = None) -&gt; str:\n        \"\"\"Export results as JSON\"\"\"\n\n    def to_html(self, filepath: str) -&gt; None:\n        \"\"\"Generate HTML report\"\"\"\n</code></pre>"},{"location":"api-reference/#data-sources","title":"Data Sources","text":""},{"location":"api-reference/#pandasdatasource","title":"PandasDataSource","text":"<pre><code>from dqx.sources import PandasDataSource\n\nsource = PandasDataSource(df)\n</code></pre>"},{"location":"api-reference/#sqldatasource","title":"SQLDataSource","text":"<pre><code>from dqx.sources import SQLDataSource\n\nsource = SQLDataSource(\n    connection_string=\"postgresql://user:pass@host/db\", query=\"SELECT * FROM table\"\n)\n</code></pre>"},{"location":"api-reference/#filedatasource","title":"FileDataSource","text":"<pre><code>from dqx.sources import FileDataSource\n\nsource = FileDataSource(filepath=\"data.csv\", format=\"csv\", **read_options)\n</code></pre>"},{"location":"api-reference/#clouddatasource","title":"CloudDataSource","text":"<pre><code>from dqx.sources import CloudDataSource\n\nsource = CloudDataSource(\n    uri=\"s3://bucket/path/data.parquet\", credentials=aws_credentials\n)\n</code></pre>"},{"location":"api-reference/#advanced-features","title":"Advanced Features","text":""},{"location":"api-reference/#custom-checks","title":"Custom Checks","text":"<p>Create custom validation logic.</p> <pre><code>from dqx import custom_check\n\n\n@custom_check\ndef business_rule_check(df):\n    \"\"\"Custom validation function\"\"\"\n    return (df[\"status\"] == \"active\") &amp; (df[\"balance\"] &gt; 0)\n\n\n# Or as a class\nclass CustomBusinessCheck(BaseCheck):\n    def validate(self, df):\n        valid_mask = self._apply_business_logic(df)\n        return CheckResult(\n            check_name=self.name,\n            passed=valid_mask.all(),\n            failed_rows=df[~valid_mask].index.tolist(),\n        )\n</code></pre>"},{"location":"api-reference/#conditional-validation","title":"Conditional Validation","text":"<p>Apply checks conditionally.</p> <pre><code>from dqx import ConditionalCheck\n\ncheck = ConditionalCheck(\n    condition=lambda df: df[\"region\"] == \"US\",\n    check=PatternCheck(\"phone\", r\"^\\d{3}-\\d{3}-\\d{4}$\"),\n)\n</code></pre>"},{"location":"api-reference/#check-groups","title":"Check Groups","text":"<p>Organize related checks.</p> <pre><code>from dqx import CheckGroup\n\ncustomer_checks = CheckGroup(\"Customer Validation\")\ncustomer_checks.add_checks(\n    [\n        NotNullCheck(\"customer_id\"),\n        UniqueCheck(\"customer_id\"),\n        PatternCheck(\"email\", email_regex),\n    ]\n)\n\norder_checks = CheckGroup(\"Order Validation\")\norder_checks.add_checks([NotNullCheck(\"order_id\"), RangeCheck(\"amount\", min_value=0)])\n</code></pre>"},{"location":"api-reference/#validation-pipelines","title":"Validation Pipelines","text":"<p>Chain validation stages.</p> <pre><code>from dqx import ValidationPipeline\n\npipeline = ValidationPipeline()\n\n# Add stages\npipeline.add_stage(\"basic\", basic_checks, fail_fast=True)\npipeline.add_stage(\"business\", business_checks)\npipeline.add_stage(\"statistical\", stats_checks)\n\n# Run pipeline\nresults = pipeline.run(data)\n</code></pre>"},{"location":"api-reference/#configuration","title":"Configuration","text":""},{"location":"api-reference/#global-configuration","title":"Global Configuration","text":"<pre><code>from dqx import config\n\n# Set global defaults\nconfig.set_defaults(fail_fast=False, parallel=True, max_workers=4, sample_size=100000)\n\n# Configure logging\nconfig.set_logging(\n    level=\"INFO\", format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\n</code></pre>"},{"location":"api-reference/#validator-configuration","title":"Validator Configuration","text":"<pre><code># From file\nvalidator = DataQualityValidator.from_config(\"config.yaml\")\n\n# Or programmatically\nvalidator.configure(parallel=True, max_workers=8, memory_limit=\"4GB\")\n</code></pre>"},{"location":"api-reference/#metrics-and-monitoring","title":"Metrics and Monitoring","text":""},{"location":"api-reference/#metrics-collection","title":"Metrics Collection","text":"<pre><code>from dqx.metrics import MetricsCollector\n\ncollector = MetricsCollector()\nvalidator.add_metrics_collector(collector)\n\n# After validation\nmetrics = collector.get_metrics()\nprint(f\"Validation time: {metrics['duration_ms']}ms\")\nprint(f\"Rows processed: {metrics['rows_processed']}\")\n</code></pre>"},{"location":"api-reference/#export-formats","title":"Export Formats","text":"<pre><code># Prometheus format\nprometheus_metrics = results.to_prometheus()\n\n# StatsD format\nstatsd_metrics = results.to_statsd()\n\n# Custom format\ncustom_metrics = results.export_metrics(formatter=lambda m: f\"{m['name']}:{m['value']}\")\n</code></pre>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/#exception-types","title":"Exception Types","text":"<pre><code>from dqx.exceptions import (\n    ValidationError,\n    DataSourceError,\n    CheckConfigurationError,\n    ConnectionError,\n)\n\ntry:\n    results = validator.validate(data, checks)\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\nexcept DataSourceError as e:\n    print(f\"Data source error: {e}\")\n</code></pre>"},{"location":"api-reference/#error-recovery","title":"Error Recovery","text":"<pre><code># Retry logic\nvalidator.validate_with_retry(data, checks, max_retries=3, retry_delay=1.0)\n\n# Fallback handling\nresults = validator.validate_with_fallback(\n    primary_source=sql_source, fallback_source=file_source, checks=checks\n)\n</code></pre>"},{"location":"api-reference/#utilities","title":"Utilities","text":""},{"location":"api-reference/#data-profiling","title":"Data Profiling","text":"<pre><code>from dqx.utils import DataProfiler\n\nprofiler = DataProfiler()\nprofile = profiler.analyze(df)\n\nprint(profile.summary())\n# Shows: column types, null counts, unique values, etc.\n</code></pre>"},{"location":"api-reference/#check-generator","title":"Check Generator","text":"<pre><code>from dqx.utils import CheckGenerator\n\ngenerator = CheckGenerator()\nsuggested_checks = generator.suggest_checks(\n    df, include_statistical=True, confidence_level=0.95\n)\n</code></pre>"},{"location":"api-reference/#migration-tools","title":"Migration Tools","text":"<pre><code>from dqx.utils import migrate_rules\n\n# Migrate from other formats\ndqx_checks = migrate_rules(source=\"great_expectations\", rules_file=\"expectations.json\")\n</code></pre>"},{"location":"api-reference/#integration-apis","title":"Integration APIs","text":""},{"location":"api-reference/#rest-api-client","title":"REST API Client","text":"<pre><code>from dqx.api import DQXClient\n\nclient = DQXClient(base_url=\"https://dqx-api.company.com\", api_key=\"your-api-key\")\n\n# Submit validation job\njob_id = client.submit_validation(\n    dataset_id=\"customers\", check_suite_id=\"customer_checks\"\n)\n\n# Get results\nresults = client.get_results(job_id)\n</code></pre>"},{"location":"api-reference/#webhook-integration","title":"Webhook Integration","text":"<pre><code>from dqx.integrations import WebhookNotifier\n\nnotifier = WebhookNotifier(\n    url=\"https://hooks.company.com/dqx\", headers={\"Authorization\": \"Bearer token\"}\n)\n\nvalidator.add_notifier(notifier)\n</code></pre>"},{"location":"api-reference/#type-definitions","title":"Type Definitions","text":"<pre><code>from typing import TypedDict, List, Optional, Union\n\n\nclass CheckResult(TypedDict):\n    check_name: str\n    passed: bool\n    failed_count: int\n    failed_rows: List[int]\n    error_message: Optional[str]\n\n\nclass ValidationSummary(TypedDict):\n    total_checks: int\n    passed_checks: int\n    failed_checks: int\n    pass_rate: float\n    duration_ms: float\n\n\nDataSource = Union[pd.DataFrame, str, SQLDataSource, FileDataSource]\n</code></pre>"},{"location":"api-reference/#constants-and-enums","title":"Constants and Enums","text":"<pre><code>from dqx.constants import Severity, CheckType\n\n\nclass Severity(Enum):\n    INFO = \"info\"\n    WARNING = \"warning\"\n    ERROR = \"error\"\n    CRITICAL = \"critical\"\n\n\nclass CheckType(Enum):\n    COMPLETENESS = \"completeness\"\n    CONSISTENCY = \"consistency\"\n    ACCURACY = \"accuracy\"\n    UNIQUENESS = \"uniqueness\"\n    CUSTOM = \"custom\"\n</code></pre> <p>For more examples and detailed usage, see the User Guide and examples directory.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to DQX are documented here. This changelog is automatically included from the project's root CHANGELOG.md.</p>"},{"location":"changelog/#v060-tbd","title":"v0.6.0 (TBD)","text":""},{"location":"changelog/#breaking-change","title":"BREAKING CHANGE","text":""},{"location":"changelog/#dql-profiles-removed","title":"DQL Profiles Removed","text":"<p>Profile definitions (<code>profile \"Name\" { ... }</code>) are no longer supported in DQL syntax. Profiles must now be defined in YAML configuration files or passed programmatically via the Python API.</p> <ul> <li>Migration required for any DQL files containing <code>profile</code> blocks</li> <li>See Migration Guide for step-by-step instructions</li> <li>Rationale: Separates validation logic (DQL) from runtime behavior (profiles), enabling environment-specific configuration</li> </ul> <p>Before (DQL with profiles): <pre><code>suite \"Orders\" {\n    profile \"Holiday\" {\n        from 2024-12-20 to 2025-01-05\n        disable check \"Volume\"\n    }\n}\n</code></pre></p> <p>After (YAML config): <pre><code>profiles:\n  - name: \"Holiday\"\n    type: \"seasonal\"\n    start_date: \"2024-12-20\"\n    end_date: \"2025-01-05\"\n    rules:\n      - action: \"disable\"\n        target: \"check\"\n        name: \"Volume\"\n</code></pre></p>"},{"location":"changelog/#holidayprofile-renamed-to-seasonalprofile","title":"HolidayProfile Renamed to SeasonalProfile","text":"<p>The <code>HolidayProfile</code> class has been renamed to <code>SeasonalProfile</code> for better semantic clarity.</p> <ul> <li>All imports of <code>HolidayProfile</code> must be updated to <code>SeasonalProfile</code></li> <li>Behavior remains unchanged - this is a naming-only change</li> <li>The new name better reflects its purpose: seasonal date-range-based profiles</li> </ul> <p>Migration: <pre><code># Before (v0.5.x)\nfrom dqx import HolidayProfile\n\nholiday = HolidayProfile(\n    name=\"Holiday\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[...],\n)\n\n# After (v0.6.0)\nfrom dqx import SeasonalProfile\n\nholiday = SeasonalProfile(\n    name=\"Holiday\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[...],\n)\n</code></pre></p>"},{"location":"changelog/#interpreter-class-removed","title":"Interpreter Class Removed","text":"<p>The <code>dqx.dql.Interpreter</code> class has been removed. Use <code>VerificationSuite(dql=...)</code> instead.</p> <ul> <li><code>Interpreter.run()</code> \u2192 <code>VerificationSuite(dql=...).run()</code></li> <li><code>SuiteResults</code> and <code>AssertionResult</code> types removed from <code>dqx.dql</code></li> <li>All DQL execution logic now integrated directly into <code>VerificationSuite</code></li> </ul> <p>Before (Interpreter): <pre><code>from dqx.dql import Interpreter\n\ninterp = Interpreter(db=db)\nresults = interp.run(Path(\"suite.dql\"), datasources, date.today())\n</code></pre></p> <p>After (VerificationSuite): <pre><code>from dqx.api import VerificationSuite\nfrom dqx.common import ResultKey\n\nsuite = VerificationSuite(\n    dql=Path(\"suite.dql\"),\n    db=db,\n    config=Path(\"config.yaml\"),\n)\nsuite.run(datasources, ResultKey(date.today(), {}))\nresults = suite.collect_results()\n</code></pre></p>"},{"location":"changelog/#feat","title":"Feat","text":"<ul> <li>dql: integrate DQL directly into VerificationSuite via <code>dql</code> parameter</li> <li>api: add <code>dql</code> parameter accepting <code>str</code> or <code>Path</code> to VerificationSuite constructor</li> <li>api: implement mutual exclusion validation between <code>checks</code> and <code>dql</code> parameters</li> <li>config: profiles now load from YAML configuration files alongside tunables</li> </ul>"},{"location":"changelog/#refactor","title":"Refactor","text":"<ul> <li>dql: remove profile syntax from DQL grammar, AST, and parser</li> <li>dql: move all DQL execution logic from Interpreter into VerificationSuite</li> <li>dql: simplify DQL language to focus exclusively on validation logic</li> <li>api: unify Python and DQL APIs under single VerificationSuite class</li> </ul>"},{"location":"changelog/#v0512-2026-01-19","title":"v0.5.12 (2026-01-19)","text":""},{"location":"changelog/#breaking-change_1","title":"BREAKING CHANGE","text":"<ul> <li>Graph is now built during VerificationSuite initialization</li> <li>DQL grammar syntax has been significantly simplified</li> </ul>"},{"location":"changelog/#feat_1","title":"Feat","text":"<ul> <li>config: add YAML configuration file support for tunable values (#58)</li> <li>display: substitute tunable values in assertion result expressions (#57)</li> <li>api: add tunable support to comparison methods (#56)</li> <li>dql: simplify profiles by removing recurring type and date functions (#54)</li> <li>dql: support dict tags in interpreter for key-value tag pairs (#52)</li> <li>dql: Add collect keyword for noop assertions (#51)</li> <li>dql: add interpreter with profile support and date functions (#48)</li> <li>dql: replace const with tunable keyword and remove import/export (#46)</li> <li>add reset() method to VerificationSuite for AI agent tuning (#43)</li> <li>dql: add complex DQL scenarios and comprehensive tests (#41)</li> <li>add DQL prerequisites - is_neq, is_none, is_not_none, order_by, coalesce (#39)</li> <li>add DQL prerequisites for RL agent integration</li> </ul>"},{"location":"changelog/#fix","title":"Fix","text":"<ul> <li>tunables: resolve SymPy caching issue causing test isolation failures (#55)</li> <li>resolve README-code discrepancies and achieve 100% coverage (#44)</li> </ul>"},{"location":"changelog/#refactor_1","title":"Refactor","text":"<ul> <li>dql: unify run_file and run_string into single run method (#49)</li> <li>api: remove is_none and is_not_none methods (#45)</li> </ul>"},{"location":"changelog/#v0511-2025-12-26","title":"v0.5.11 (2025-12-26)","text":""},{"location":"changelog/#feat_2","title":"Feat","text":"<ul> <li>api: add tags support to assertions (#34)</li> <li>profiles: add metric_multiplier support for assertion overrides (#35)</li> <li>add YAML/JSON configuration support for verification suites (#36)</li> </ul>"},{"location":"changelog/#docs","title":"Docs","text":"<ul> <li>add DQL language design specification (#37)</li> </ul>"},{"location":"changelog/#refactor_2","title":"Refactor","text":"<ul> <li>remove BigQuery e2e test and ERROR assertion status (#33)</li> </ul>"},{"location":"changelog/#v0510-2025-11-11","title":"v0.5.10 (2025-11-11)","text":""},{"location":"changelog/#feat_3","title":"Feat","text":"<ul> <li>ops: add CustomSQL operation with universal parameter support (#29)</li> <li>date-exclusion: implement comprehensive date exclusion with data availability tracking (#28)</li> <li>bkng-integration: refactor logger API and enhance type safety for integration (#27)</li> </ul>"},{"location":"changelog/#v059-2025-11-04","title":"v0.5.9 (2025-11-04)","text":""},{"location":"changelog/#fix_1","title":"Fix","text":"<ul> <li>tests: remove format_string parameter from logger tests</li> <li>plugins: enhance audit plugin display formatting and add logging support</li> <li>correct DoD/WoW calculations to use percentage change (#26)</li> <li>enforce string-only values in Tags type</li> <li>tests: consolidate logger tests and update to use setup_logger</li> </ul>"},{"location":"changelog/#v058-2025-11-03","title":"v0.5.8 (2025-11-03)","text":""},{"location":"changelog/#fix_2","title":"Fix","text":"<ul> <li>correct DoD/WoW calculations to use percentage change</li> <li>BigQuery SQL generation compatibility</li> <li>BigQuery SQL generation compatibility fixes</li> </ul>"},{"location":"changelog/#v057-2025-11-03","title":"v0.5.7 (2025-11-03)","text":""},{"location":"changelog/#perf","title":"Perf","text":"<ul> <li>analyzer: optimize SQL logging and formatting</li> </ul>"},{"location":"changelog/#v057a4-2025-11-03","title":"v0.5.7a4 (2025-11-03)","text":""},{"location":"changelog/#fix_3","title":"Fix","text":"<ul> <li>analyzer: ensure correct date alignment in analyze_sql_ops</li> <li>analyzer: handle BigQuery dict format in batch query results</li> </ul>"},{"location":"changelog/#v057a1-2025-11-03","title":"v0.5.7a1 (2025-11-03)","text":""},{"location":"changelog/#fix_4","title":"Fix","text":"<ul> <li>analyzer: handle BigQuery uppercase KEY in batch query results</li> </ul>"},{"location":"changelog/#refactor_3","title":"Refactor","text":"<ul> <li>analyzer: consolidate SQL ops analysis into single batch function</li> </ul>"},{"location":"changelog/#v057a0-2025-11-03","title":"v0.5.7a0 (2025-11-03)","text":""},{"location":"changelog/#fix_5","title":"Fix","text":"<ul> <li>correct package name in version import</li> <li>resolve BigQuery UNION ALL incompatibility in batch optimization</li> </ul>"},{"location":"changelog/#refactor_4","title":"Refactor","text":"<ul> <li>remove backward compatibility in analyzer</li> <li>remove numpy dependency and cleanup project structure (#24)</li> </ul>"},{"location":"changelog/#v056-2025-11-03","title":"v0.5.6 (2025-11-03)","text":""},{"location":"changelog/#fix_6","title":"Fix","text":"<ul> <li>improve metric handling and analyzer architecture (#19)</li> </ul>"},{"location":"changelog/#refactor_5","title":"Refactor","text":"<ul> <li>remove numpy dependency from analyzer</li> <li>cache: enhance metric storage and add performance tracking (#23)</li> <li>cache: enhance metric storage and add performance tracking</li> </ul>"},{"location":"changelog/#perf_1","title":"Perf","text":"<ul> <li>optimize metric cache and improve code quality (#20)</li> </ul>"},{"location":"changelog/#v055-2025-10-30","title":"v0.5.5 (2025-10-30)","text":""},{"location":"changelog/#feat_4","title":"Feat","text":"<ul> <li>Add CommercialDataSource with date filtering support (#17)</li> <li>add metric expiration methods to MetricDB (#15)</li> <li>ci: standardize workflows and add GitHub settings (#9)</li> </ul>"},{"location":"changelog/#fix_7","title":"Fix","text":"<ul> <li>resolve deprecation warning for invalid escape sequence (#16)</li> <li>orm: remove unnecessary lambda from uuid default value (#14)</li> <li>address Python special method protocol violation in test (#11)</li> <li>resolve GitHub release character limit issue (#8)</li> </ul>"},{"location":"changelog/#v054-2025-10-29","title":"v0.5.4 (2025-10-29)","text":""},{"location":"changelog/#fix_8","title":"Fix","text":"<ul> <li>resolve GitHub release character limit issue</li> <li>resolve release workflow issues and update dqlib documentation (#7)</li> </ul>"},{"location":"changelog/#v053-2025-10-29","title":"v0.5.3 (2025-10-29)","text":""},{"location":"changelog/#fix_9","title":"Fix","text":"<ul> <li>resolve release workflow issues and update dqlib documentation</li> </ul>"},{"location":"changelog/#v052-2025-10-29","title":"v0.5.2 (2025-10-29)","text":""},{"location":"changelog/#fix_10","title":"Fix","text":"<ul> <li>ci: remove docker-publish job from release workflow (#5)</li> </ul>"},{"location":"changelog/#v051-2025-10-29","title":"v0.5.1 (2025-10-29)","text":""},{"location":"changelog/#fix_11","title":"Fix","text":"<ul> <li>ci: remove docker-publish job from release workflow</li> </ul>"},{"location":"changelog/#v050-2025-10-29","title":"v0.5.0 (2025-10-29)","text":""},{"location":"changelog/#feat_5","title":"Feat","text":"<ul> <li>add version attribute to dqx module</li> </ul>"},{"location":"changelog/#fix_12","title":"Fix","text":"<ul> <li>ci: fix test-release job to handle PEP 668 system Python protection</li> <li>ci: remove redundant pull_request_target from release-drafter workflow</li> <li>ci: add pull-requests write permission to docs workflow</li> <li>ci: correct codecov action parameter from 'file' to 'files'</li> <li>ci: add --system flag to release workflow test installation</li> </ul> <p>For more detailed release notes, see the GitHub Releases page.</p>"},{"location":"ci-cd-setup/","title":"CI/CD Setup Guide for DQX","text":"<p>This guide documents the complete CI/CD setup for the DQX project, including all required secrets and configurations.</p>"},{"location":"ci-cd-setup/#related-documentation","title":"Related Documentation","text":"<ul> <li>GitHub CI/CD Setup Guide - Step-by-step setup instructions</li> <li>GitHub CI/CD Operations Guide - Daily operations and usage</li> </ul> <p>This document provides a technical overview and reference. For practical guides, see the documents above.</p>"},{"location":"ci-cd-setup/#github-actions-workflows","title":"GitHub Actions Workflows","text":""},{"location":"ci-cd-setup/#1-test-coverage-workflow","title":"1. Test &amp; Coverage Workflow","text":"<p>File: <code>.github/workflows/test.yml</code> - Runs on: Push to main/develop, pull requests - Tests multiple Python versions (3.11, 3.12, 3.13) - Generates coverage reports and badges - Includes type coverage reporting</p>"},{"location":"ci-cd-setup/#2-documentation-workflow","title":"2. Documentation Workflow","text":"<p>File: <code>.github/workflows/docs.yml</code> - Builds MkDocs documentation - Tests documentation build - Publishes to GitHub Pages (optional)</p>"},{"location":"ci-cd-setup/#3-release-workflow","title":"3. Release Workflow","text":"<p>File: <code>.github/workflows/release.yml</code> - Triggers on: GitHub releases - Builds Python packages (wheel and sdist) - Publishes to PyPI - Creates GitHub release artifacts</p>"},{"location":"ci-cd-setup/#4-security-scanning","title":"4. Security Scanning","text":"<p>File: <code>.github/workflows/codeql.yml</code> - Runs CodeQL analysis - Performs dependency review on PRs - Runs pip-audit for vulnerability scanning</p>"},{"location":"ci-cd-setup/#5-example-validation","title":"5. Example Validation","text":"<p>File: <code>.github/workflows/examples.yml</code> - Validates all example scripts - Ensures examples stay up-to-date</p>"},{"location":"ci-cd-setup/#required-secrets","title":"Required Secrets","text":""},{"location":"ci-cd-setup/#pypi-publishing","title":"PyPI Publishing","text":"<ul> <li><code>PYPI_API_TOKEN</code>: PyPI API token for package publishing</li> <li>Create at: https://pypi.org/manage/account/token/</li> <li>Scope: Can upload to project \"dqx\"</li> </ul>"},{"location":"ci-cd-setup/#test-pypi-optional","title":"Test PyPI (Optional)","text":"<ul> <li><code>TEST_PYPI_API_TOKEN</code>: Test PyPI API token</li> <li>Create at: https://test.pypi.org/manage/account/token/</li> <li>Use for testing releases before production</li> </ul>"},{"location":"ci-cd-setup/#github-pages-optional","title":"GitHub Pages (Optional)","text":"<ul> <li><code>GITHUB_TOKEN</code>: Automatically provided by GitHub Actions</li> <li>No setup required</li> <li>Used for publishing documentation</li> </ul>"},{"location":"ci-cd-setup/#google-analytics-optional","title":"Google Analytics (Optional)","text":"<ul> <li><code>GOOGLE_ANALYTICS_KEY</code>: Google Analytics tracking ID</li> <li>Format: <code>G-XXXXXXXXXX</code></li> <li>Used in MkDocs for documentation analytics</li> </ul>"},{"location":"ci-cd-setup/#external-services","title":"External Services","text":""},{"location":"ci-cd-setup/#1-readthedocs","title":"1. ReadTheDocs","text":"<p>Configuration: <code>.readthedocs.yml</code> - Sign up at: https://readthedocs.org - Import your GitHub repository - Documentation will be available at: https://dqx.readthedocs.io</p>"},{"location":"ci-cd-setup/#2-coderabbit","title":"2. CodeRabbit","text":"<p>Configuration: <code>.coderabbit.yaml</code> - Install from: https://github.com/marketplace/coderabbit - Provides AI-powered code reviews - No additional configuration needed</p>"},{"location":"ci-cd-setup/#3-dependabot","title":"3. Dependabot","text":"<p>Configuration: <code>.github/dependabot.yml</code> - Automatically enabled for GitHub repositories - Creates PRs for dependency updates - Review and merge security updates promptly</p>"},{"location":"ci-cd-setup/#4-release-drafter","title":"4. Release Drafter","text":"<p>Configuration: <code>.github/release-drafter.yml</code> - Automatically creates draft releases - Categorizes changes based on PR labels - Updates release notes with each merge to main</p>"},{"location":"ci-cd-setup/#setting-up-secrets","title":"Setting Up Secrets","text":""},{"location":"ci-cd-setup/#via-github-web-interface","title":"Via GitHub Web Interface","text":"<ol> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click \"New repository secret\"</li> <li>Add each secret with its name and value</li> </ol>"},{"location":"ci-cd-setup/#via-github-cli","title":"Via GitHub CLI","text":"<pre><code># Install GitHub CLI\nbrew install gh  # macOS\n# or visit: https://cli.github.com/\n\n# Authenticate\ngh auth login\n\n# Add secrets\ngh secret set PYPI_API_TOKEN --body=\"pypi-...\"\ngh secret set TEST_PYPI_API_TOKEN --body=\"pypi-...\"\n</code></pre>"},{"location":"ci-cd-setup/#workflow-permissions","title":"Workflow Permissions","text":"<p>Ensure workflows have proper permissions:</p> <ol> <li>Go to Settings \u2192 Actions \u2192 General</li> <li>Under \"Workflow permissions\", select:</li> <li>\"Read and write permissions\"</li> <li>\"Allow GitHub Actions to create and approve pull requests\"</li> </ol>"},{"location":"ci-cd-setup/#branch-protection","title":"Branch Protection","text":"<p>Recommended branch protection rules for <code>main</code>:</p> <ol> <li>Require pull request reviews</li> <li>Require status checks:</li> <li><code>test (3.11)</code> - Main test suite</li> <li><code>analyze</code> - CodeQL security</li> <li><code>docs</code> - Documentation build</li> <li>Include administrators</li> <li>Allow force pushes (only for admins)</li> </ol>"},{"location":"ci-cd-setup/#local-development","title":"Local Development","text":""},{"location":"ci-cd-setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># Install pre-commit hooks\npre-commit install\n\n# Run hooks manually\nuv run hooks --all\n\n# Skip specific hooks\nSKIP=mypy git commit -m \"...\"\n</code></pre>"},{"location":"ci-cd-setup/#testing-workflows-locally","title":"Testing Workflows Locally","text":"<pre><code># Install act (GitHub Actions local runner)\nbrew install act  # macOS\n\n# Run specific workflow\nact -W .github/workflows/test.yml\n\n# Run with secrets\nact -W .github/workflows/release.yml --secret-file .secrets\n</code></pre>"},{"location":"ci-cd-setup/#monitoring-maintenance","title":"Monitoring &amp; Maintenance","text":""},{"location":"ci-cd-setup/#github-actions-dashboard","title":"GitHub Actions Dashboard","text":"<ul> <li>Monitor at: <code>https://github.com/&lt;owner&gt;/dqx/actions</code></li> <li>Set up notifications for failed workflows</li> <li>Review workflow run times and optimize if needed</li> </ul>"},{"location":"ci-cd-setup/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Review Dependabot PRs weekly</li> <li>Run security audits: <code>uv run pip-audit</code></li> <li>Keep GitHub Actions versions updated</li> </ul>"},{"location":"ci-cd-setup/#documentation","title":"Documentation","text":"<ul> <li>Verify ReadTheDocs builds after each release</li> <li>Check for broken links: <code>mkdocs serve --strict</code></li> <li>Update screenshots and examples regularly</li> </ul>"},{"location":"ci-cd-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ci-cd-setup/#common-issues","title":"Common Issues","text":"<ol> <li>PyPI Upload Fails</li> <li>Verify API token has upload permissions</li> <li>Check package version isn't already published</li> <li> <p>Ensure package builds locally: <code>uv build</code></p> </li> <li> <p>Documentation Build Fails</p> </li> <li>Check MkDocs configuration: <code>mkdocs build --strict</code></li> <li>Verify all referenced files exist</li> <li> <p>Check for Python version compatibility</p> </li> <li> <p>Tests Fail on CI but Pass Locally</p> </li> <li>Check for environment differences</li> <li>Verify all test dependencies are installed</li> <li> <p>Look for timing-dependent tests</p> </li> <li> <p>Coverage Reports Not Updating</p> </li> <li>Ensure coverage files are generated</li> <li>Check GitHub token permissions</li> <li>Verify badge URLs are correct</li> </ol>"},{"location":"ci-cd-setup/#release-process","title":"Release Process","text":"<ol> <li> <p>Prepare Release <pre><code># Update version in pyproject.toml\n# Update CHANGELOG.md\ngit commit -m \"chore: prepare release v0.4.0\"\ngit tag v0.4.0\ngit push origin main --tags\n</code></pre></p> </li> <li> <p>Create GitHub Release</p> </li> <li>Go to Releases \u2192 Draft a new release</li> <li>Choose tag <code>v0.4.0</code></li> <li>Release notes are auto-generated</li> <li> <p>Publish release</p> </li> <li> <p>Verify Deployment</p> </li> <li>Check PyPI: https://pypi.org/project/dqx/</li> <li>Verify docs: https://dqx.readthedocs.io</li> <li>Test installation: <code>pip install dqx==0.4.0</code></li> </ol>"},{"location":"ci-cd-setup/#security-considerations","title":"Security Considerations","text":"<ul> <li>Rotate API tokens annually</li> <li>Use environment-specific secrets</li> <li>Enable 2FA on PyPI and GitHub accounts</li> <li>Review security alerts promptly</li> <li>Keep workflows up-to-date</li> </ul>"},{"location":"ci-cd-setup/#contact-support","title":"Contact &amp; Support","text":"<p>For CI/CD issues: - Check GitHub Actions logs - Review workflow configuration - Open an issue with the <code>ci</code> label - Tag @phamducnam for urgent issues</p>"},{"location":"design/","title":"DQX Design Document","text":""},{"location":"design/#1-overview","title":"1. Overview","text":""},{"location":"design/#what-is-dqx","title":"What is DQX","text":"<p>DQX is a data quality analysis framework that enables developers to define and run data quality checks using native Python code. It provides a SQL-based backend for efficient computation and supports extensible metrics, validators, and data sources.</p>"},{"location":"design/#core-design-philosophy","title":"Core Design Philosophy","text":"<p>DQX prioritizes simplicity, composability, and performance: - Code over configuration: Define checks in Python, not through configuration - SQL-powered: Execute queries directly on data warehouses - Modular architecture: Extend with custom metrics and validators - Symbolic computation: Express complex business rules naturally</p>"},{"location":"design/#2-native-python-api-vs-configuration","title":"2. Native Python API vs Configuration","text":""},{"location":"design/#design-decision-code-over-config","title":"Design Decision: Code over Config","text":"<p>Unlike configuration-based systems that use YAML or JSON, DQX uses Python code as the primary interface for defining data quality checks. This fundamental design choice drives the entire architecture.</p>"},{"location":"design/#advantages","title":"Advantages","text":"<ul> <li>IDE support: Full autocomplete, type checking, and inline documentation</li> <li>Composable &amp; reusable functions: Build complex checks from simple components</li> <li>Version control friendly: Standard code review processes apply</li> <li>Dynamic check generation: Use loops, conditions, and functions to create checks programmatically</li> </ul>"},{"location":"design/#3-modular-extensible-architecture","title":"3. Modular &amp; Extensible Architecture","text":""},{"location":"design/#plugin-based-design","title":"Plugin-Based Design","text":"<p>DQX uses a protocol-based architecture that allows users to extend the framework without modifying core code.</p>"},{"location":"design/#extension-points","title":"Extension Points","text":"<ul> <li>Custom Metrics: Implement the <code>MetricSpec</code> protocol to create new metrics</li> <li>Custom Validators: Implement validation functions following the standard signature</li> <li>Custom Data Sources: Implement the <code>SqlDataSource</code> protocol for new data backends</li> <li>Custom SQL Dialects: Add support for database-specific SQL syntax</li> </ul>"},{"location":"design/#4-symbolic-metrics","title":"4. Symbolic Metrics","text":""},{"location":"design/#how-symbolic-metrics-work","title":"How Symbolic Metrics Work","text":"<p>Metrics in DQX are symbolic expressions powered by SymPy, Python's symbolic mathematics library. The framework evaluates metrics by collecting symbol values through SQL queries at execution time.</p>"},{"location":"design/#key-advantages","title":"Key Advantages","text":"<ul> <li>Mathematical operations on metrics: Combine metrics using any SymPy functions</li> <li>Cross-datasource validation: Combine metrics from different data sources in a single expression</li> <li>Lazy evaluation: SQL generation happens only when needed</li> <li>Declarative expressions: Write business rules as mathematical formulas</li> </ul>"},{"location":"design/#5-sql-backend-architecture","title":"5. SQL Backend Architecture","text":""},{"location":"design/#why-sql-over-spark","title":"Why SQL over Spark","text":"<p>DQX uses SQL as its computation backend instead of distributed frameworks like Spark:</p> <ul> <li>Faster execution for single-node operations: Direct SQL execution without cluster overhead</li> <li>No Spark cluster required: Simpler infrastructure and lower operational costs</li> <li>Lower operational complexity: No JVM tuning or cluster management</li> <li>Rich analytical functions: Leverage database-native window functions and aggregations</li> </ul>"},{"location":"design/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Single-pass computation for multiple metrics for multiple days.</li> <li>Efficient CTE-based query generation</li> </ul>"},{"location":"design/#6-severity-levels","title":"6. Severity Levels","text":""},{"location":"design/#p0-p3-definitions","title":"P0-P3 Definitions","text":"<ul> <li>P0 (Crisis): Data corruption or complete failure requiring immediate intervention</li> <li>P1 (Major): Significant issues affecting data reliability and business decisions</li> <li>P2 (Degraded): Quality degradation that needs investigation but not immediate action</li> <li>P3 (Minor Noise): Informational alerts for monitoring trends and anomalies</li> </ul>"},{"location":"design/#7-api-reference-supported-operations","title":"7. API Reference &amp; Supported Operations","text":""},{"location":"design/#supported-dialects","title":"Supported Dialects","text":"Dialect Description DuckDB Default dialect, in-memory analytical database BigQuery Google Cloud data warehouse PyArrow Arrow tables via DuckDB integration"},{"location":"design/#supported-metrics","title":"Supported Metrics","text":"Metric Method Description Row Count <code>num_rows</code> Total number of rows First <code>first</code> First value in column Average <code>average</code> Mean value of column Sum <code>sum</code> Sum of column values Minimum <code>minimum</code> Minimum value Maximum <code>maximum</code> Maximum value Variance <code>variance</code> Statistical variance Null Count <code>null_count</code> Count of null values Duplicate Count <code>duplicate_count</code> Count of duplicate rows"},{"location":"design/#assertion-methods","title":"Assertion Methods","text":"Method Description <code>is_eq</code> Assert equals with tolerance <code>is_neq</code> Assert not equal to <code>is_gt</code> Assert greater than <code>is_lt</code> Assert less than <code>is_geq</code> Assert greater than or equal to <code>is_leq</code> Assert less than or equal to <code>is_between</code> Assert in range (inclusive) <code>is_positive</code> Assert value &gt; 0 <code>is_negative</code> Assert value &lt; 0 <code>is_zero</code> Assert value is effectively zero <code>is_none</code> Assert value is None <code>is_not_none</code> Assert value is not None <code>within_tol</code> Assert within relative or absolute tolerance"},{"location":"dqguard-to-dqx-comparison/","title":"DQGuard to DQX: Evolution of Data Quality at Scale","text":""},{"location":"dqguard-to-dqx-comparison/#executive-summary","title":"Executive Summary","text":"<p>DQX represents the next generation of data quality tooling, evolving from DQGuard's configuration-based approach to a code-first framework. While DQGuard pioneered automated quality monitoring through JSON configurations, DQX advances the field with mathematical expressions, type safety, and modern architecture.</p> <p>This document guides teams through understanding the evolution, key improvements, and migration considerations.</p>"},{"location":"dqguard-to-dqx-comparison/#the-evolution-story","title":"The Evolution Story","text":""},{"location":"dqguard-to-dqx-comparison/#where-we-started-dqguard","title":"Where We Started: DQGuard","text":"<p>DQGuard solved critical problems: - High overhead in quality monitoring across teams - Need for long-term reliability measurement - Slow incident response times - Unreliable business-critical data</p> <p>The solution: JSON-based configurations that automated metric collection and validation.</p>"},{"location":"dqguard-to-dqx-comparison/#where-were-going-dqx","title":"Where We're Going: DQX","text":"<p>DQX addresses modern challenges: - Complex validation logic requiring mathematical expressions - Efficient data processing through modern architecture - Developer productivity through type safety and IDE support - Flexible assertions beyond time-series patterns</p>"},{"location":"dqguard-to-dqx-comparison/#key-improvements","title":"Key Improvements","text":""},{"location":"dqguard-to-dqx-comparison/#1-from-configuration-to-code","title":"1. From Configuration to Code","text":"<p>DQGuard: Define checks in JSON <pre><code>{\n    \"name\": \"default.reservation_flatter\",\n    \"type\": \"table\",\n    \"metrics\": [\"num_rows\"],\n    \"validators\": [{\n        \"name\": \"is_geq\",\n        \"threshold\": 1000000\n    }]\n}\n</code></pre></p> <p>DQX: Express checks as Python functions <pre><code>@check(\"Reservations have sufficient volume\")\ndef validate_reservations(mp: MetricProvider, ctx: Context) -&gt; None:\n    ctx.assert_that(mp.num_rows()).where(name=\"Daily volume check\").is_geq(1000000)\n</code></pre></p> <p>Benefits: - Type checking catches errors before runtime - IDE autocompletion speeds development - Version control shows meaningful diffs - Reusable logic through standard Python patterns</p>"},{"location":"dqguard-to-dqx-comparison/#2-mathematical-expressions","title":"2. Mathematical Expressions","text":"<p>DQGuard: Fixed validator patterns <pre><code>{\n    \"validators\": [\"within_2_sd\", \"wow_change\"]\n}\n</code></pre></p> <p>DQX: Arbitrary mathematical assertions <pre><code># Revenue integrity check\ncalculated = mp.sum(\"price\") * mp.sum(\"quantity\")\nreported = mp.sum(\"revenue\")\nerror_rate = sp.Abs(calculated - reported) / reported\n\nctx.assert_that(error_rate).where(name=\"Revenue calculation accuracy\").is_lt(0.001)\n</code></pre></p>"},{"location":"dqguard-to-dqx-comparison/#3-processing-architecture","title":"3. Processing Architecture","text":"<p>DQGuard: - Spark-based processing - Separate metric collection and validation passes - Scales with cluster resources</p> <p>DQX: - DuckDB columnar engine - Single-pass optimization - Efficient query execution</p>"},{"location":"dqguard-to-dqx-comparison/#4-developer-experience","title":"4. Developer Experience","text":"<p>DQGuard: <pre><code># Edit JSON\nvim lib/quality_check.json\n\n# Run in workflow\n&lt;action name=\"quality_check\"&gt;\n    &lt;sub-workflow&gt;\n        &lt;app-path&gt;${workflowsBaseDir}/data-quality-library/production-app&lt;/app-path&gt;\n    &lt;/sub-workflow&gt;\n&lt;/action&gt;\n</code></pre></p> <p>DQX: <pre><code># Write with IDE support\ndef validate_orders(mp: MetricProvider, ctx: Context) -&gt; None:\n    # Autocompletion shows available metrics\n    ctx.assert_that(mp.average(\"price\")).where(name=\"Price validation\").is_positive()\n\n\n# Run directly\nsuite = VerificationSuite([validate_orders], db)\nsuite.run(datasources, key)\n</code></pre></p>"},{"location":"dqguard-to-dqx-comparison/#feature-comparison","title":"Feature Comparison","text":"Feature DQGuard DQX Configuration JSON files Python code Type Safety Runtime validation Compile-time checking Metrics 15 predefined Extensible + custom Validators 12 patterns Unlimited expressions Engine Spark clusters DuckDB Processing Model Multiple passes Single pass Architecture Distributed Columnar engine IDE Support JSON schemas Full Python tooling Testing Manual validation Standard unit tests Debugging Log analysis Interactive debugging Deployment Oozie workflows Any Python environment"},{"location":"dqguard-to-dqx-comparison/#migration-guide","title":"Migration Guide","text":""},{"location":"dqguard-to-dqx-comparison/#assessment-phase","title":"Assessment Phase","text":"<ol> <li>Inventory Current Checks</li> <li>List all DQGuard configurations</li> <li>Identify custom validators</li> <li> <p>Note integration points</p> </li> <li> <p>Complexity Analysis</p> </li> <li>Simple threshold checks \u2192 Direct migration</li> <li>Time-series validators \u2192 May need custom logic</li> <li>Complex preprocessors \u2192 Evaluate alternatives</li> </ol>"},{"location":"dqguard-to-dqx-comparison/#migration-patterns","title":"Migration Patterns","text":""},{"location":"dqguard-to-dqx-comparison/#pattern-1-simple-metrics","title":"Pattern 1: Simple Metrics","text":"<p>DQGuard: <pre><code>{\n    \"metrics\": [\"num_rows\", \"null_count\"],\n    \"validators\": [{\"name\": \"is_geq\", \"threshold\": 0}]\n}\n</code></pre></p> <p>DQX: <pre><code>ctx.assert_that(mp.num_rows()).where(name=\"Has rows\").is_positive()\nctx.assert_that(mp.null_count(\"id\")).where(name=\"No null IDs\").is_zero()\n</code></pre></p>"},{"location":"dqguard-to-dqx-comparison/#pattern-2-time-series-validation","title":"Pattern 2: Time-Series Validation","text":"<p>DQGuard: <pre><code>{\n    \"validators\": [\"within_2_sd\"],\n    \"time_range\": \"7 days\"\n}\n</code></pre></p> <p>DQX: <pre><code># Collect historical metrics\nhistory = [mp.sum(\"revenue\", key=ctx.key.lag(i)) for i in range(1, 8)]\nmean = sum(history) / len(history)\nstd = calculate_std(history)\n\n# Apply validation\ncurrent = mp.sum(\"revenue\")\nz_score = abs(current - mean) / std\nctx.assert_that(z_score).where(name=\"Within 2 SD\").is_lt(2)\n</code></pre></p>"},{"location":"dqguard-to-dqx-comparison/#pattern-3-duplicate-detection","title":"Pattern 3: Duplicate Detection","text":"<p>DQGuard: <pre><code>{\n    \"metrics\": [{\n        \"name\": \"has_duplicate\",\n        \"columns\": [\"transaction_id\"]\n    }]\n}\n</code></pre></p> <p>DQX: <pre><code># Built-in duplicate detection\nduplicate_count = mp.duplicate_count(\"transaction_id\")\nctx.assert_that(duplicate_count).where(name=\"No duplicates\").is_zero()\n</code></pre></p>"},{"location":"dqguard-to-dqx-comparison/#coexistence-strategy","title":"Coexistence Strategy","text":"<p>Teams can run both systems during migration:</p> <ol> <li>Phase 1: Shadow Mode</li> <li>Keep DQGuard running</li> <li>Add DQX checks in parallel</li> <li> <p>Compare results</p> </li> <li> <p>Phase 2: Gradual Cutover</p> </li> <li>Migrate one dataset at a time</li> <li>Maintain critical DQGuard checks</li> <li> <p>Build confidence in DQX</p> </li> <li> <p>Phase 3: Full Migration</p> </li> <li>Retire DQGuard configurations</li> <li>Leverage DQX-only features</li> <li>Optimize for performance</li> </ol>"},{"location":"dqguard-to-dqx-comparison/#advanced-dqx-capabilities","title":"Advanced DQX Capabilities","text":""},{"location":"dqguard-to-dqx-comparison/#1-cross-dataset-validation","title":"1. Cross-Dataset Validation","text":"<pre><code>@check(\"Production matches staging\")\ndef compare_environments(mp: MetricProvider, ctx: Context) -&gt; None:\n    prod = mp.sum(\"revenue\", dataset=\"production\")\n    staging = mp.sum(\"revenue\", dataset=\"staging\")\n\n    ctx.assert_that(prod).where(name=\"Environment parity\").is_eq(staging, tol=0.01)\n</code></pre>"},{"location":"dqguard-to-dqx-comparison/#2-custom-metric-extensions","title":"2. Custom Metric Extensions","text":"<pre><code># DQX supports custom metrics through extensions\nday_over_day = mp.ext.day_over_day(specs.Average(\"response_time\"))\nctx.assert_that(day_over_day).where(name=\"Response time trend\").is_between(-0.1, 0.1)\n</code></pre>"},{"location":"dqguard-to-dqx-comparison/#3-symbolic-mathematics","title":"3. Symbolic Mathematics","text":"<pre><code># Complex business rules as mathematical expressions\nmargin = (mp.sum(\"revenue\") - mp.sum(\"cost\")) / mp.sum(\"revenue\")\ntarget_margin = 0.3\n\nctx.assert_that(margin).where(name=\"Profit margin target\", severity=\"P0\").is_geq(\n    target_margin\n)\n</code></pre>"},{"location":"dqguard-to-dqx-comparison/#best-practices","title":"Best Practices","text":""},{"location":"dqguard-to-dqx-comparison/#1-name-every-assertion","title":"1. Name Every Assertion","text":"<pre><code># Good: Clear, specific names\nctx.assert_that(metric).where(name=\"Daily revenue within 10% of average\")\n\n# Avoid: Generic names\nctx.assert_that(metric).where(name=\"Revenue check\")\n</code></pre>"},{"location":"dqguard-to-dqx-comparison/#2-group-related-checks","title":"2. Group Related Checks","text":"<pre><code>@check(\"Payment integrity\", datasets=[\"payments\"])\ndef validate_payments(mp: MetricProvider, ctx: Context) -&gt; None:\n    # All payment validations in one check\n    ctx.assert_that(mp.null_count(\"payment_id\")).where(\n        name=\"Payment ID completeness\"\n    ).is_zero()\n\n    ctx.assert_that(mp.average(\"amount\")).where(\n        name=\"Average payment reasonable\"\n    ).is_between(10, 1000)\n</code></pre>"},{"location":"dqguard-to-dqx-comparison/#3-use-severity-levels","title":"3. Use Severity Levels","text":"<pre><code>ctx.assert_that(critical_metric).where(\n    name=\"Critical business metric\", severity=\"P0\"  # Pages on-call\n).is_positive()\n\nctx.assert_that(quality_metric).where(\n    name=\"Data quality indicator\", severity=\"P2\"  # Daily review\n).is_within_range()\n</code></pre>"},{"location":"dqguard-to-dqx-comparison/#conclusion","title":"Conclusion","text":"<p>DQX builds upon DQGuard's foundation while addressing modern data quality challenges. The evolution from configuration to code enables:</p> <ul> <li>Better expressiveness through mathematical assertions</li> <li>Modern architecture with DuckDB's columnar processing</li> <li>Enhanced productivity with type safety and IDE support</li> <li>Greater flexibility for complex business rules</li> </ul> <p>Teams should evaluate their current DQGuard usage and plan migration based on complexity and criticality. The coexistence strategy allows gradual adoption while maintaining quality coverage.</p> <p>For teams starting fresh, DQX provides a modern, flexible solution for data quality validation.</p>"},{"location":"github-cicd-operations-guide/","title":"GitHub CI/CD Operations Guide","text":"<p>This guide covers daily operations and interactions with the CI/CD system after initial setup.</p>"},{"location":"github-cicd-operations-guide/#daily-development-workflow","title":"Daily Development Workflow","text":""},{"location":"github-cicd-operations-guide/#working-with-pre-commit-hooks","title":"Working with Pre-commit Hooks","text":"<p>Pre-commit hooks run automatically before each commit to catch issues early.</p>"},{"location":"github-cicd-operations-guide/#running-hooks-locally","title":"Running Hooks Locally","text":"<pre><code># Run all hooks on staged files\nuv run hooks\n\n# Run all hooks on all files\nuv run hooks --all\n\n# Run specific hook\nuv run hooks mypy\n\n# See available options\nuv run hooks --help\n</code></pre>"},{"location":"github-cicd-operations-guide/#common-hook-failures-and-fixes","title":"Common Hook Failures and Fixes","text":"<p>Ruff formatting issues: <pre><code># Auto-fix formatting\nuv run ruff check --fix .\nuv run ruff format .\n\n# Then stage the fixes\ngit add -u\n</code></pre></p> <p>Mypy type errors: - Add missing type hints - Fix type inconsistencies - Use <code># type: ignore[error-code]</code> sparingly with justification</p> <p>Trailing whitespace: - Most editors can auto-remove on save - The hook will fix automatically</p>"},{"location":"github-cicd-operations-guide/#bypassing-hooks-use-sparingly","title":"Bypassing Hooks (Use Sparingly)","text":"<pre><code># Skip all hooks - use only when necessary\ngit commit --no-verify -m \"emergency: fix critical bug\"\n\n# Skip specific hooks\nSKIP=mypy git commit -m \"wip: debugging in progress\"\n</code></pre>"},{"location":"github-cicd-operations-guide/#commit-standards","title":"Commit Standards","text":"<p>Follow conventional commits for clear history and automatic versioning.</p>"},{"location":"github-cicd-operations-guide/#format","title":"Format","text":"<pre><code>type(scope): subject\n\nbody (optional)\n\nfooter (optional)\n</code></pre>"},{"location":"github-cicd-operations-guide/#common-types","title":"Common Types","text":"<ul> <li><code>feat</code>: New feature (triggers minor version)</li> <li><code>fix</code>: Bug fix (triggers patch version)</li> <li><code>docs</code>: Documentation only</li> <li><code>style</code>: Code style (no logic changes)</li> <li><code>refactor</code>: Code restructure</li> <li><code>perf</code>: Performance improvement</li> <li><code>test</code>: Test additions/fixes</li> <li><code>chore</code>: Maintenance tasks</li> </ul>"},{"location":"github-cicd-operations-guide/#examples","title":"Examples","text":"<pre><code>git commit -m \"feat(analyzer): add batch processing support\"\ngit commit -m \"fix(validator): handle null values correctly\"\ngit commit -m \"docs: update API examples\"\n</code></pre>"},{"location":"github-cicd-operations-guide/#creating-pull-requests","title":"Creating Pull Requests","text":"<ol> <li> <p>Create feature branch: <pre><code>git checkout -b feat/your-feature-name\n</code></pre></p> </li> <li> <p>Push and create PR: <pre><code>git push -u origin feat/your-feature-name\ngh pr create --title \"feat: your feature\" --body \"Description\"\n</code></pre></p> </li> <li> <p>Required elements:</p> </li> <li>Descriptive title following commit conventions</li> <li>Clear description of changes</li> <li>Link to related issue (if any)</li> <li>Screenshots for UI changes</li> </ol>"},{"location":"github-cicd-operations-guide/#code-review-process","title":"Code Review Process","text":""},{"location":"github-cicd-operations-guide/#understanding-coderabbit-ai-reviews","title":"Understanding CodeRabbit AI Reviews","text":"<p>CodeRabbit automatically reviews PRs within 5 minutes.</p>"},{"location":"github-cicd-operations-guide/#review-components","title":"Review Components","text":"<ol> <li>Summary Section</li> <li>High-level overview</li> <li>Key changes identified</li> <li> <p>Potential issues flagged</p> </li> <li> <p>Detailed Comments</p> </li> <li>Line-by-line suggestions</li> <li>Best practice violations</li> <li>Security concerns</li> <li> <p>Performance issues</p> </li> <li> <p>Review Status</p> </li> <li>\u2705 Approved: No critical issues</li> <li>\ud83d\udcac Comments: Suggestions provided</li> <li>\ud83d\udd04 Changes requested: Critical issues found</li> </ol>"},{"location":"github-cicd-operations-guide/#interacting-with-coderabbit","title":"Interacting with CodeRabbit","text":"<p>Common commands in PR comments: <pre><code>@coderabbitai review\n@coderabbitai resolve\n@coderabbitai ignore\n@coderabbitai help\n</code></pre></p> <p>Responding to suggestions: 1. Accept suggestion: Click \"Accept\" on the comment 2. Dismiss: Reply explaining why it's not applicable 3. Ask for clarification: Reply with your question</p>"},{"location":"github-cicd-operations-guide/#customizing-reviews","title":"Customizing Reviews","text":"<p>Edit <code>.coderabbit.yaml</code> to adjust: - Review strictness - Custom patterns - Excluded paths - Project-specific rules</p>"},{"location":"github-cicd-operations-guide/#handling-review-feedback","title":"Handling Review Feedback","text":"<ol> <li>Address all comments - Even if just to explain why not changed</li> <li>Batch fixes - Make all changes before requesting re-review</li> <li>Update PR description - Note what feedback was addressed</li> <li>Request re-review - Use GitHub's re-review feature</li> </ol>"},{"location":"github-cicd-operations-guide/#managing-dependencies","title":"Managing Dependencies","text":""},{"location":"github-cicd-operations-guide/#dependabot-pull-requests","title":"Dependabot Pull Requests","text":"<p>Dependabot creates PRs for dependency updates automatically.</p>"},{"location":"github-cicd-operations-guide/#understanding-update-prs","title":"Understanding Update PRs","text":"<p>PR Title Format: <pre><code>Bump package-name from 1.2.3 to 1.2.4\n</code></pre></p> <p>PR includes: - Changelog excerpts - Commit list - Compatibility score - Release notes link</p>"},{"location":"github-cicd-operations-guide/#review-process","title":"Review Process","text":"<ol> <li>Check CI status - All tests must pass</li> <li>Review changelog - Look for breaking changes</li> <li>Check compatibility - Verify with your Python versions</li> <li>Test locally if needed: <pre><code>gh pr checkout 123\nuv sync\nuv run pytest tests/\n</code></pre></li> </ol>"},{"location":"github-cicd-operations-guide/#dependabot-commands","title":"Dependabot Commands","text":"<p>Use these in PR comments:</p> <pre><code>@dependabot rebase\n@dependabot recreate\n@dependabot merge\n@dependabot squash and merge\n@dependabot cancel merge\n@dependabot reopen\n@dependabot close\n@dependabot ignore this major version\n@dependabot ignore this minor version\n@dependabot ignore this dependency\n</code></pre>"},{"location":"github-cicd-operations-guide/#merge-strategies","title":"Merge Strategies","text":"<p>Security updates: Merge immediately after tests pass</p> <p>Minor updates: - Group weekly - Test together - Merge as batch</p> <p>Major updates: - Test thoroughly - Check migration guides - Consider compatibility</p>"},{"location":"github-cicd-operations-guide/#managing-multiple-updates","title":"Managing Multiple Updates","text":"<pre><code># List all Dependabot PRs\ngh pr list --label dependencies\n\n# Bulk operations with GitHub CLI\ngh pr list --label dependencies --json number --jq '.[].number' | \\\n  xargs -I {} gh pr comment {} --body \"@dependabot rebase\"\n</code></pre>"},{"location":"github-cicd-operations-guide/#release-process","title":"Release Process","text":""},{"location":"github-cicd-operations-guide/#preparing-a-release","title":"Preparing a Release","text":"<ol> <li> <p>Update version in <code>pyproject.toml</code>: <pre><code>version = \"0.4.0\"\n</code></pre></p> </li> <li> <p>Update CHANGELOG.md:</p> </li> <li>Add release date</li> <li>Review all changes</li> <li> <p>Highlight breaking changes</p> </li> <li> <p>Create release PR: <pre><code>git checkout -b release/v0.4.0\ngit add pyproject.toml CHANGELOG.md\ngit commit -m \"chore: prepare release v0.4.0\"\ngit push -u origin release/v0.4.0\ngh pr create --title \"Release v0.4.0\" --label release\n</code></pre></p> </li> </ol>"},{"location":"github-cicd-operations-guide/#using-release-drafter","title":"Using Release Drafter","text":"<p>Release Drafter automatically maintains draft release notes.</p>"},{"location":"github-cicd-operations-guide/#how-it-works","title":"How It Works","text":"<ol> <li>Monitors merged PRs - Collects since last release</li> <li>Categorizes by labels - Groups changes by type</li> <li>Determines version - Based on change types</li> <li>Updates draft - After each PR merge</li> </ol>"},{"location":"github-cicd-operations-guide/#pr-labels-for-versioning","title":"PR Labels for Versioning","text":"<p>Major version (breaking changes): - <code>breaking-change</code> - <code>breaking</code></p> <p>Minor version (features): - <code>feature</code> - <code>enhancement</code> - <code>feat</code></p> <p>Patch version (fixes): - <code>fix</code> - <code>bug</code> - <code>perf</code> - <code>docs</code></p>"},{"location":"github-cicd-operations-guide/#editing-release-notes","title":"Editing Release Notes","text":"<ol> <li>Go to Releases \u2192 Draft release</li> <li>Review auto-generated notes</li> <li>Add highlights section</li> <li>Include migration guide for breaking changes</li> <li>Preview before publishing</li> </ol>"},{"location":"github-cicd-operations-guide/#publishing-a-release","title":"Publishing a Release","text":"<ol> <li>Final checks:</li> <li>All CI passes on main</li> <li>Version updated in code</li> <li> <p>Documentation updated</p> </li> <li> <p>Create and push tag: <pre><code>git checkout main\ngit pull origin main\ngit tag v0.4.0\ngit push origin v0.4.0\n</code></pre></p> </li> <li> <p>Publish GitHub release:</p> </li> <li>Go to draft release</li> <li>Select the tag</li> <li>Review notes one more time</li> <li> <p>Click \"Publish release\"</p> </li> <li> <p>Monitor deployment:</p> </li> <li>Check Actions tab for release workflow</li> <li>Verify PyPI upload</li> <li>Test installation: <code>pip install dqx==0.4.0</code></li> </ol>"},{"location":"github-cicd-operations-guide/#post-release-tasks","title":"Post-Release Tasks","text":"<ol> <li> <p>Verify deployment: <pre><code># Test from PyPI\npip install dqx==0.4.0\npython -c \"import dqx; print(dqx.__version__)\"\n</code></pre></p> </li> <li> <p>Update documentation:</p> </li> <li>Check ReadTheDocs built new version</li> <li>Update version references</li> <li> <p>Announce in relevant channels</p> </li> <li> <p>Create next development cycle: <pre><code># Bump to next dev version\n# Update pyproject.toml to 0.5.0-dev\ngit commit -m \"chore: bump version to 0.5.0-dev\"\n</code></pre></p> </li> </ol>"},{"location":"github-cicd-operations-guide/#monitoring-cicd","title":"Monitoring CI/CD","text":""},{"location":"github-cicd-operations-guide/#github-actions-dashboard","title":"GitHub Actions Dashboard","text":"<p>Access at: <code>https://github.com/&lt;owner&gt;/dqx/actions</code></p>"},{"location":"github-cicd-operations-guide/#key-metrics","title":"Key Metrics","text":"<ul> <li>Success rate by workflow</li> <li>Average run time</li> <li>Recent failures</li> <li>Usage minutes</li> </ul>"},{"location":"github-cicd-operations-guide/#workflow-management","title":"Workflow Management","text":"<p>Re-run failed jobs: 1. Click on failed workflow run 2. Click \"Re-run failed jobs\" 3. Or \"Re-run all jobs\" if needed</p> <p>Cancel stuck workflows: <pre><code>gh run list --status in_progress\ngh run cancel &lt;run-id&gt;\n</code></pre></p> <p>Download artifacts: <pre><code>gh run download &lt;run-id&gt; -n &lt;artifact-name&gt;\n</code></pre></p>"},{"location":"github-cicd-operations-guide/#debugging-ci-failures","title":"Debugging CI Failures","text":"<ol> <li>Check summary - Look for error annotations</li> <li>Expand failed step - Find exact error</li> <li>Download logs - For detailed analysis</li> <li>Run locally - Try to reproduce</li> </ol>"},{"location":"github-cicd-operations-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Test failures on CI only: - Check for environment differences - Look for timing issues - Verify test data availability</p> <p>Timeout errors: - Increase timeout in workflow - Split long-running tests - Add progress output</p> <p>Permission errors: - Check token permissions - Verify secrets are set - Review repository settings</p>"},{"location":"github-cicd-operations-guide/#quick-reference","title":"Quick Reference","text":""},{"location":"github-cicd-operations-guide/#essential-commands","title":"Essential Commands","text":"<pre><code># Pre-commit hooks\nuv run hooks --all           # Run all hooks\nuv run hooks --help          # Show options\n\n# GitHub CLI\ngh pr create                 # Create PR\ngh pr list                   # List PRs\ngh pr checks                 # Show CI status\ngh workflow run              # Trigger workflow\n\n# Git\ngit tag v0.4.0              # Create version tag\ngit push origin v0.4.0      # Push tag\n\n# Python/uv\nuv sync                     # Install dependencies\nuv run pytest               # Run tests\nuv build                    # Build package\n</code></pre>"},{"location":"github-cicd-operations-guide/#status-badge-urls","title":"Status Badge URLs","text":"<pre><code>![Tests](https://github.com/&lt;owner&gt;/dqx/workflows/tests/badge.svg)\n![Coverage](https://github.com/&lt;owner&gt;/dqx/coverage.svg)\n![Docs](https://readthedocs.org/projects/dqx/badge/)\n![PyPI](https://badge.fury.io/py/dqx.svg)\n</code></pre>"},{"location":"github-cicd-operations-guide/#useful-links","title":"Useful Links","text":"<ul> <li>GitHub Actions Docs</li> <li>Conventional Commits</li> <li>CodeRabbit Docs</li> <li>Dependabot Docs</li> </ul> <p>For initial setup instructions, see the GitHub CI/CD Setup Guide.</p>"},{"location":"github-cicd-setup-guide/","title":"GitHub CI/CD Setup Guide","text":"<p>This guide walks through setting up GitHub Actions CI/CD for the DQX project.</p>"},{"location":"github-cicd-setup-guide/#prerequisites","title":"Prerequisites","text":"<ul> <li>GitHub account with repository access</li> <li>PyPI account for package publishing</li> <li>Basic understanding of GitHub Actions</li> </ul>"},{"location":"github-cicd-setup-guide/#step-1-create-pypi-account-and-api-token","title":"Step 1: Create PyPI Account and API Token","text":""},{"location":"github-cicd-setup-guide/#create-pypi-account","title":"Create PyPI Account","text":"<ol> <li>Visit pypi.org</li> <li>Click \"Register\"</li> <li>Fill the registration form:</li> <li>Username: Choose a unique identifier</li> <li>Email: Use your primary email</li> <li>Password: Create a strong password</li> <li>Verify your email address</li> </ol>"},{"location":"github-cicd-setup-guide/#generate-api-token","title":"Generate API Token","text":"<ol> <li>Log into PyPI</li> <li>Go to Account Settings \u2192 API tokens</li> <li>Click \"Add API token\"</li> <li>Configure the token:</li> <li>Token name: <code>github-actions-dqx</code></li> <li>Scope: \"Project: dqx\" (or \"Entire account\" for first release)</li> <li>Click \"Add token\"</li> <li>Copy the token immediately - it shows only once</li> <li>Format: <code>pypi-AgEIcHlwaS5vcmcCJGE4ZjY5...</code></li> <li>Save it securely</li> </ol>"},{"location":"github-cicd-setup-guide/#set-up-test-pypi-optional-but-recommended","title":"Set Up Test PyPI (Optional but Recommended)","text":"<ol> <li>Visit test.pypi.org</li> <li>Create a separate account (can use same email)</li> <li>Generate API token following same steps</li> <li>Use for testing releases before production</li> </ol>"},{"location":"github-cicd-setup-guide/#step-2-configure-github-repository-secrets","title":"Step 2: Configure GitHub Repository Secrets","text":""},{"location":"github-cicd-setup-guide/#add-pypi-token","title":"Add PyPI Token","text":"<ol> <li>Go to your GitHub repository</li> <li>Navigate to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click \"New repository secret\"</li> <li>Add the secret:</li> <li>Name: <code>PYPI_API_TOKEN</code></li> <li>Value: Paste your PyPI token</li> <li>Click \"Add secret\"</li> </ol>"},{"location":"github-cicd-setup-guide/#add-test-pypi-token-optional","title":"Add Test PyPI Token (Optional)","text":"<p>Repeat the above with: - Name: <code>TEST_PYPI_API_TOKEN</code> - Value: Your Test PyPI token</p>"},{"location":"github-cicd-setup-guide/#verify-secrets","title":"Verify Secrets","text":"<p>Your secrets page should show: - <code>PYPI_API_TOKEN</code> (updated just now) - <code>TEST_PYPI_API_TOKEN</code> (updated just now)</p>"},{"location":"github-cicd-setup-guide/#step-3-set-up-readthedocs","title":"Step 3: Set Up ReadTheDocs","text":""},{"location":"github-cicd-setup-guide/#create-account","title":"Create Account","text":"<ol> <li>Visit readthedocs.org</li> <li>Sign up with GitHub OAuth (recommended) or create account</li> <li>Authorize ReadTheDocs to access your repositories</li> </ol>"},{"location":"github-cicd-setup-guide/#import-project","title":"Import Project","text":"<ol> <li>Click \"Import a Project\"</li> <li>Select your repository from the list</li> <li>Or click \"Import Manually\" and enter repo URL</li> <li>Configure project details:</li> <li>Name: <code>dqx</code></li> <li>Repository URL: <code>https://github.com/yourusername/dqx</code></li> <li>Repository type: Git</li> <li>Default branch: <code>main</code></li> <li>Click \"Next\"</li> </ol>"},{"location":"github-cicd-setup-guide/#configure-build","title":"Configure Build","text":"<ol> <li>In project dashboard, go to Admin \u2192 Advanced Settings</li> <li>Set Python configuration:</li> <li>Python interpreter: CPython 3.11</li> <li>Install method: pip</li> <li>Requirements file: Leave empty (using pyproject.toml)</li> <li>Enable these options:</li> <li>Build pull requests</li> <li>Public versions</li> <li>Single version docs (if you want /latest/ only)</li> <li>Save changes</li> </ol>"},{"location":"github-cicd-setup-guide/#first-build","title":"First Build","text":"<ol> <li>Go to Builds tab</li> <li>Click \"Build Version\"</li> <li>Monitor the build log</li> <li>Once successful, view docs at <code>https://dqx.readthedocs.io</code></li> </ol>"},{"location":"github-cicd-setup-guide/#webhook-setup-automatic","title":"Webhook Setup (Automatic)","text":"<p>ReadTheDocs automatically creates a webhook in your GitHub repo. Verify: 1. GitHub repo \u2192 Settings \u2192 Webhooks 2. Look for ReadTheDocs webhook 3. Should trigger on push events</p>"},{"location":"github-cicd-setup-guide/#step-4-install-github-apps","title":"Step 4: Install GitHub Apps","text":""},{"location":"github-cicd-setup-guide/#coderabbit","title":"CodeRabbit","text":"<ol> <li>Visit GitHub Marketplace - CodeRabbit</li> <li>Click \"Set up a plan\"</li> <li>Choose pricing plan (free tier available)</li> <li>Select repositories:</li> <li>Choose \"Only select repositories\"</li> <li>Select your DQX repository</li> <li>Complete installation</li> </ol> <p>CodeRabbit will automatically review PRs once installed.</p>"},{"location":"github-cicd-setup-guide/#dependabot-already-enabled","title":"Dependabot (Already Enabled)","text":"<p>Dependabot is automatically available. Ensure it's active: 1. Go to Settings \u2192 Security &amp; analysis 2. Enable:    - Dependency graph    - Dependabot alerts    - Dependabot security updates</p>"},{"location":"github-cicd-setup-guide/#release-drafter","title":"Release Drafter","text":"<p>The Release Drafter workflow is already configured. It will: - Auto-generate release notes from PRs - Categorize changes by labels - Create draft releases automatically</p>"},{"location":"github-cicd-setup-guide/#step-5-configure-github-actions-permissions","title":"Step 5: Configure GitHub Actions Permissions","text":""},{"location":"github-cicd-setup-guide/#repository-permissions","title":"Repository Permissions","text":"<ol> <li>Go to Settings \u2192 Actions \u2192 General</li> <li>Under \"Actions permissions\":</li> <li>Select \"Allow all actions and reusable workflows\"</li> <li>Under \"Workflow permissions\":</li> <li>Select \"Read and write permissions\"</li> <li>Check \"Allow GitHub Actions to create and approve pull requests\"</li> <li>Click \"Save\"</li> </ol>"},{"location":"github-cicd-setup-guide/#branch-protection","title":"Branch Protection","text":"<ol> <li>Go to Settings \u2192 Branches</li> <li>Click \"Add rule\"</li> <li>Configure protection for <code>main</code>:</li> <li>Branch name pattern: <code>main</code></li> <li>Enable:<ul> <li>Require pull request before merging</li> <li>Require status checks:</li> <li><code>test (3.11)</code></li> <li><code>analyze</code></li> <li><code>docs</code></li> <li>Require branches to be up to date</li> <li>Include administrators (optional)</li> </ul> </li> <li>Click \"Create\"</li> </ol>"},{"location":"github-cicd-setup-guide/#step-6-test-the-setup","title":"Step 6: Test the Setup","text":""},{"location":"github-cicd-setup-guide/#verify-workflows","title":"Verify Workflows","text":"<ol> <li> <p>Create a test branch:    <pre><code>git checkout -b test/ci-setup\n</code></pre></p> </li> <li> <p>Make a small change:    <pre><code>echo \"# Test\" &gt;&gt; test.md\ngit add test.md\ngit commit -m \"test: verify CI setup\"\ngit push origin test/ci-setup\n</code></pre></p> </li> <li> <p>Create a pull request</p> </li> <li>Watch the checks run:</li> <li>Tests should pass</li> <li>CodeRabbit should comment</li> <li>All status checks should be green</li> </ol>"},{"location":"github-cicd-setup-guide/#test-documentation-build","title":"Test Documentation Build","text":"<ol> <li> <p>Make a docs change:    <pre><code>echo \"Test content\" &gt;&gt; docs/test.md\ngit add docs/test.md\ngit commit -m \"docs: test documentation build\"\ngit push\n</code></pre></p> </li> <li> <p>Check ReadTheDocs:</p> </li> <li>Go to your project builds</li> <li>Verify build triggered and passed</li> </ol>"},{"location":"github-cicd-setup-guide/#test-release-process-dry-run","title":"Test Release Process (Dry Run)","text":"<ol> <li>Update version in <code>pyproject.toml</code></li> <li> <p>Create and push a tag:    <pre><code>git tag v0.3.1-rc1\ngit push origin v0.3.1-rc1\n</code></pre></p> </li> <li> <p>Check Actions tab for release workflow</p> </li> <li>Verify it would publish correctly (without creating release)</li> </ol>"},{"location":"github-cicd-setup-guide/#step-7-local-documentation-development","title":"Step 7: Local Documentation Development","text":""},{"location":"github-cicd-setup-guide/#install-mkdocs","title":"Install MkDocs","text":"<pre><code># In your project directory\nuv pip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre>"},{"location":"github-cicd-setup-guide/#run-documentation-server","title":"Run Documentation Server","text":"<pre><code># Start local server\nmkdocs serve\n\n# Output:\n# INFO - Building documentation...\n# INFO - Cleaning site directory\n# INFO - Documentation built in 2.34 seconds\n# INFO - Serving on http://127.0.0.1:8000\n</code></pre>"},{"location":"github-cicd-setup-guide/#live-development","title":"Live Development","text":"<ol> <li>Open browser to <code>http://localhost:8000</code></li> <li>Edit any markdown file in <code>docs/</code></li> <li>Save the file</li> <li>Browser auto-refreshes with changes</li> </ol>"},{"location":"github-cicd-setup-guide/#build-documentation","title":"Build Documentation","text":"<pre><code># Build static site\nmkdocs build\n\n# Output creates site/ directory\n# Upload site/ contents to any web server\n</code></pre>"},{"location":"github-cicd-setup-guide/#test-strict-mode","title":"Test Strict Mode","text":"<pre><code># Catch broken links and references\nmkdocs build --strict\n\n# Fails on warnings - same as CI\n</code></pre>"},{"location":"github-cicd-setup-guide/#preview-different-themes","title":"Preview Different Themes","text":"<p>Edit <code>mkdocs.yml</code>: <pre><code>theme:\n  name: material\n  palette:\n    scheme: slate  # Try 'default' for light\n</code></pre></p> <p>Save and see instant changes.</p>"},{"location":"github-cicd-setup-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"github-cicd-setup-guide/#pypi-upload-fails","title":"PyPI Upload Fails","text":"<p>Token scope too narrow: - First release needs \"Entire account\" scope - After first release, can limit to project</p> <p>Version already exists: - PyPI never allows reusing versions - Increment version in <code>pyproject.toml</code></p> <p>Authentication failed: - Verify secret name is exactly <code>PYPI_API_TOKEN</code> - Check token starts with <code>pypi-</code> - Regenerate token if needed</p>"},{"location":"github-cicd-setup-guide/#readthedocs-build-fails","title":"ReadTheDocs Build Fails","text":"<p>Import error: - Check <code>.readthedocs.yml</code> syntax - Verify Python version matches project</p> <p>MkDocs not found: - Ensure MkDocs installed in build commands - Check pip install succeeds in logs</p> <p>Theme missing: - Add <code>mkdocs-material</code> to dependencies - Clear build cache and retry</p>"},{"location":"github-cicd-setup-guide/#github-actions-timeout","title":"GitHub Actions Timeout","text":"<p>Long test runs: - Add <code>timeout-minutes: 30</code> to job - Split tests into parallel jobs - Cache dependencies</p> <p>Hanging process: - Check for infinite loops - Add proper test timeouts - Kill subprocess in tests</p>"},{"location":"github-cicd-setup-guide/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Rotate tokens annually</li> <li>Set calendar reminder</li> <li> <p>Update both PyPI and GitHub</p> </li> <li> <p>Use environment protection</p> </li> <li>Limit production deployments</li> <li> <p>Require reviews for releases</p> </li> <li> <p>Enable 2FA everywhere</p> </li> <li>GitHub (required for Actions)</li> <li>PyPI (highly recommended)</li> <li> <p>ReadTheDocs</p> </li> <li> <p>Monitor security alerts</p> </li> <li>Check GitHub Security tab weekly</li> <li>Act on Dependabot alerts quickly</li> <li>Review CodeQL findings</li> </ol>"},{"location":"github-cicd-setup-guide/#next-steps","title":"Next Steps","text":"<p>With CI/CD configured:</p> <ol> <li>Make your first release</li> <li>Update version and changelog</li> <li>Create GitHub release</li> <li> <p>Monitor PyPI publication</p> </li> <li> <p>Customize workflows</p> </li> <li>Add deployment environments</li> <li>Include performance tests</li> <li> <p>Add container builds</p> </li> <li> <p>Enhance documentation</p> </li> <li>Add API references</li> <li>Include tutorials</li> <li> <p>Create video guides</p> </li> <li> <p>Monitor metrics</p> </li> <li>Track build times</li> <li>Review test coverage</li> <li>Analyze deployment frequency</li> </ol> <p>For issues, check workflow logs first, then consult this guide's troubleshooting section.</p>"},{"location":"github-cicd-setup-guide/#related-documentation","title":"Related Documentation","text":"<ul> <li>GitHub CI/CD Operations Guide - Daily operations and interactions</li> <li>CI/CD Overview - Technical reference and file descriptions</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>DQX can be installed using pip from PyPI or directly from source.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or higher</li> <li>pip package manager</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>The simplest way to install DQX:</p> <pre><code>pip install dqx\n</code></pre> <p>For specific version:</p> <pre><code>pip install dqx==0.3.0\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<p>To install the latest development version:</p> <pre><code>git clone https://github.com/yourusername/dqx.git\ncd dqx\npip install -e .\n</code></pre>"},{"location":"installation/#install-with-extras","title":"Install with Extras","text":"<p>DQX provides optional dependencies for specific features:</p>"},{"location":"installation/#development-dependencies","title":"Development Dependencies","text":"<p>For contributing to DQX:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This includes: - pytest for testing - ruff for linting - mypy for type checking - pre-commit hooks</p>"},{"location":"installation/#documentation-dependencies","title":"Documentation Dependencies","text":"<p>For building documentation:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre>"},{"location":"installation/#all-dependencies","title":"All Dependencies","text":"<p>To install all optional dependencies:</p> <pre><code>pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"installation/#using-uv-recommended-for-development","title":"Using UV (Recommended for Development)","text":"<p>DQX uses uv for dependency management:</p> <pre><code># Install uv\npip install uv\n\n# Clone and setup\ngit clone https://github.com/yourusername/dqx.git\ncd dqx\n\n# Install all dependencies\nuv sync\n\n# Run tests\nuv run pytest\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify DQX is working:</p> <pre><code>import dqx\n\nprint(dqx.__version__)\n</code></pre> <p>Or from command line:</p> <pre><code>python -c \"import dqx; print(dqx.__version__)\"\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#python-version","title":"Python Version","text":"<p>DQX requires Python 3.11+. Check your version:</p> <pre><code>python --version\n</code></pre>"},{"location":"installation/#virtual-environment","title":"Virtual Environment","text":"<p>We recommend using a virtual environment:</p> <pre><code># Create virtual environment\npython -m venv venv\n\n# Activate it\nsource venv/bin/activate  # On Unix/macOS\nvenv\\Scripts\\activate     # On Windows\n\n# Install DQX\npip install dqx\n</code></pre>"},{"location":"installation/#permission-issues","title":"Permission Issues","text":"<p>If you encounter permission errors:</p> <pre><code>pip install --user dqx\n</code></pre>"},{"location":"installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you have dependency conflicts, try installing in a clean environment:</p> <pre><code>python -m venv clean_env\nsource clean_env/bin/activate\npip install dqx\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quick Start Guide to create your first data quality checks</li> <li>Read the User Guide for detailed usage instructions</li> <li>Check out examples for real-world usage</li> </ul>"},{"location":"installation/#uninstalling","title":"Uninstalling","text":"<p>To remove DQX:</p> <pre><code>pip uninstall dqx\n</code></pre> <p>For issues with installation, please open an issue on GitHub.</p>"},{"location":"plugin_system/","title":"DQX Plugin System Documentation","text":"<p>The DQX Plugin System allows extending validation result processing with custom plugins. Plugins can generate reports, send notifications, store metrics, or perform any custom processing after validation completes.</p>"},{"location":"plugin_system/#overview","title":"Overview","text":"<p>The plugin system provides: - Extensibility: Add custom result processors without modifying core DQX code - Isolation: Plugin failures don't affect validation execution - Performance: Built-in timeouts prevent slow plugins from blocking - Rich Context: Access to all validation results, symbols, and metadata</p>"},{"location":"plugin_system/#architecture","title":"Architecture","text":"<pre><code>VerificationSuite\n    \u2502\n    \u251c\u2500\u2500 Executes validations\n    \u2502\n    \u2514\u2500\u2500 PluginManager.process_all()\n            \u2502\n            \u251c\u2500\u2500 AuditPlugin (built-in)\n            \u251c\u2500\u2500 CustomPlugin1\n            \u2514\u2500\u2500 CustomPlugin2\n</code></pre>"},{"location":"plugin_system/#creating-a-plugin","title":"Creating a Plugin","text":"<p>Plugins must implement the <code>PostProcessor</code> protocol:</p> <pre><code>from dqx.common import PluginExecutionContext, PluginMetadata\nfrom dqx.plugins import PostProcessor\n\n\nclass MyPlugin:\n    @staticmethod\n    def metadata() -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"my_plugin\",\n            version=\"1.0.0\",\n            author=\"Your Name\",\n            description=\"Description of what your plugin does\",\n            capabilities={\"reporting\", \"notifications\"},  # Optional\n        )\n\n    def process(self, context: PluginExecutionContext) -&gt; None:\n        \"\"\"Process validation results.\"\"\"\n        # Your plugin logic here\n        pass\n</code></pre>"},{"location":"plugin_system/#plugin-context","title":"Plugin Context","text":"<p>The <code>PluginExecutionContext</code> provides comprehensive information:</p> <pre><code>@dataclass\nclass PluginExecutionContext:\n    suite_name: str  # Name of the verification suite\n    datasources: list[str]  # Data sources used\n    key: ResultKey  # Date and tags\n    timestamp: float  # Unix timestamp\n    duration_ms: float  # Execution time in milliseconds\n    results: list[AssertionResult]  # All assertion results\n    symbols: list[SymbolInfo]  # All computed symbols\n</code></pre> <p>Available convenience methods: - <code>total_assertions() -&gt; int</code> - Total number of assertions - <code>failed_assertions() -&gt; int</code> - Number of failed assertions - <code>passed_assertions() -&gt; int</code> - Number of passed assertions - <code>assertion_pass_rate() -&gt; float</code> - Percentage of passed assertions - <code>total_symbols() -&gt; int</code> - Total number of symbols - <code>failed_symbols() -&gt; int</code> - Number of failed symbols - <code>assertions_by_severity() -&gt; dict[str, int]</code> - Count by severity - <code>failures_by_severity() -&gt; dict[str, int]</code> - Failures by severity</p>"},{"location":"plugin_system/#built-in-plugins","title":"Built-in Plugins","text":""},{"location":"plugin_system/#auditplugin","title":"AuditPlugin","text":"<p>The built-in audit plugin displays a Rich-formatted summary of validation results:</p> <pre><code>from dqx.api import VerificationSuite\n\n# Create suite - plugins are managed internally\nsuite = VerificationSuite(checks, db, \"MyDataQuality\")\n\n# Plugins are enabled by default when run() is called\nsuite.run(datasources, key)  # Plugins enabled\n\n# Or explicitly disable plugins during run\nsuite.run(datasources, key, enable_plugins=False)\n</code></pre> <p>Output example: <pre><code>\u2550\u2550\u2550 DQX Audit Report \u2550\u2550\u2550\nSuite: MyDataQuality\nDate: 2025-10-18\nDuration: 342.50ms\nDatasets: products, inventory\n\n     Execution Summary\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Metric           \u2502 Count \u2502  Rate \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Total Assertions \u2502     4 \u2502       \u2502\n\u2502 Passed \u2713         \u2502     3 \u2502 75.0% \u2502\n\u2502 Failed \u2717         \u2502     1 \u2502 25.0% \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"plugin_system/#example-plugins","title":"Example Plugins","text":""},{"location":"plugin_system/#json-reporter","title":"JSON Reporter","text":"<pre><code>class JSONReporterPlugin:\n    @staticmethod\n    def metadata() -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"json_reporter\",\n            version=\"1.0.0\",\n            author=\"Your Team\",\n            description=\"Outputs results as JSON\",\n        )\n\n    def process(self, context: PluginExecutionContext) -&gt; None:\n        import json\n\n        report = {\n            \"suite\": context.suite_name,\n            \"date\": context.key.yyyy_mm_dd.isoformat(),\n            \"duration_ms\": context.duration_ms,\n            \"summary\": {\n                \"total\": context.total_assertions(),\n                \"passed\": context.passed_assertions(),\n                \"failed\": context.failed_assertions(),\n                \"pass_rate\": context.assertion_pass_rate(),\n            },\n            \"failures_by_severity\": context.failures_by_severity(),\n        }\n\n        with open(\"validation_report.json\", \"w\") as f:\n            json.dump(report, f, indent=2)\n</code></pre>"},{"location":"plugin_system/#slack-notifier","title":"Slack Notifier","text":"<pre><code>class SlackNotifierPlugin:\n    def __init__(self, webhook_url: str):\n        self.webhook_url = webhook_url\n\n    @staticmethod\n    def metadata() -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"slack_notifier\",\n            version=\"1.0.0\",\n            author=\"DevOps Team\",\n            description=\"Sends validation alerts to Slack\",\n        )\n\n    def process(self, context: PluginExecutionContext) -&gt; None:\n        if context.failed_assertions() &gt; 0:\n            import requests\n\n            failures = context.failures_by_severity()\n            message = {\n                \"text\": f\"\u26a0\ufe0f Data Quality Alert: {context.suite_name}\",\n                \"blocks\": [\n                    {\n                        \"type\": \"section\",\n                        \"text\": {\n                            \"type\": \"mrkdwn\",\n                            \"text\": f\"*Suite:* {context.suite_name}\\n\"\n                            f\"*Pass Rate:* {context.assertion_pass_rate():.1f}%\\n\"\n                            f\"*Failures:* {failures}\",\n                        },\n                    }\n                ],\n            }\n\n            requests.post(self.webhook_url, json=message)\n</code></pre>"},{"location":"plugin_system/#metrics-collector","title":"Metrics Collector","text":"<pre><code>class MetricsCollectorPlugin:\n    @staticmethod\n    def metadata() -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"metrics_collector\",\n            version=\"1.0.0\",\n            author=\"Analytics Team\",\n            description=\"Collects validation metrics\",\n        )\n\n    def process(self, context: PluginExecutionContext) -&gt; None:\n        # Send metrics to monitoring system\n        import statsd\n\n        client = statsd.StatsClient(\"localhost\", 8125)\n\n        # Send gauges\n        client.gauge(\n            f\"dqx.{context.suite_name}.assertions.total\", context.total_assertions()\n        )\n        client.gauge(\n            f\"dqx.{context.suite_name}.assertions.failed\", context.failed_assertions()\n        )\n        client.gauge(\n            f\"dqx.{context.suite_name}.pass_rate\", context.assertion_pass_rate()\n        )\n\n        # Send timing\n        client.timing(f\"dqx.{context.suite_name}.duration\", context.duration_ms)\n</code></pre>"},{"location":"plugin_system/#plugin-registration","title":"Plugin Registration","text":""},{"location":"plugin_system/#method-1-entry-points-recommended","title":"Method 1: Entry Points (Recommended)","text":"<p>Register plugins in your package's <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"dqx.plugins\"]\njson_reporter = \"mypackage.plugins:JSONReporterPlugin\"\nslack_notifier = \"mypackage.plugins:SlackNotifierPlugin\"\n</code></pre> <p>This approach supports automatic plugin discovery when your package is installed.</p>"},{"location":"plugin_system/#method-2-manual-registration","title":"Method 2: Manual Registration","text":"<p>For testing or dynamic plugin loading:</p> <pre><code>from dqx.api import VerificationSuite\n\n# Create suite\nsuite = VerificationSuite(checks, db, \"MyData\")\n\n# Register custom plugins using fully qualified class names\nsuite.plugin_manager.register_plugin(\"mypackage.plugins.MyCustomPlugin\")\n\n# Clear default plugins if needed\nsuite.plugin_manager.clear_plugins()\n\n# Register only your plugins\nsuite.plugin_manager.register_plugin(\"mypackage.plugins.JSONReporterPlugin\")\n\n# Note: For plugins with constructor arguments, you'll need to use entry points\n# or create a wrapper class that handles initialization\n</code></pre>"},{"location":"plugin_system/#error-handling","title":"Error Handling","text":"<p>Plugins are executed with comprehensive error handling:</p> <ol> <li>Plugin Exceptions: Caught and logged, suite continues</li> <li>Timeouts: Plugins exceeding 60 seconds are terminated</li> <li>Invalid Plugins: Rejected during discovery</li> </ol> <p>Example error handling:</p> <pre><code>class RobustPlugin:\n    def process(self, context: PluginExecutionContext) -&gt; None:\n        try:\n            # Risky operation\n            external_api_call()\n        except Exception as e:\n            # Plugin should handle its own errors gracefully\n            logger.error(f\"Plugin error: {e}\")\n            # Can still do partial processing\n            self.write_local_backup(context)\n</code></pre>"},{"location":"plugin_system/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Timeouts: Default 60-second timeout per plugin</li> <li>Async Operations: Plugins run synchronously; use threads/async internally if needed</li> <li>Resource Usage: Plugins should clean up resources in case of timeout</li> </ol> <pre><code>class EfficientPlugin:\n    def process(self, context: PluginExecutionContext) -&gt; None:\n        # Use context methods for efficiency\n        if context.assertion_pass_rate() &gt; 99.0:\n            # Skip expensive processing for perfect runs\n            return\n\n        # Process only failures\n        for result in context.results:\n            if result.status == \"FAILURE\":\n                self.process_failure(result)\n</code></pre>"},{"location":"plugin_system/#testing-plugins","title":"Testing Plugins","text":"<pre><code>import pytest\nfrom datetime import datetime\nfrom dqx.common import PluginExecutionContext, ResultKey, AssertionResult\nfrom returns.result import Success\n\n\ndef test_my_plugin():\n    # Create test context\n    context = PluginExecutionContext(\n        suite_name=\"TestSuite\",\n        datasources=[\"test_db\"],\n        key=ResultKey(datetime.now().date(), {}),\n        timestamp=time.time(),\n        duration_ms=100.0,\n        results=[\n            AssertionResult(\n                yyyy_mm_dd=datetime.now().date(),\n                suite=\"TestSuite\",\n                check=\"test_check\",\n                assertion=\"test_assertion\",\n                severity=\"P1\",\n                status=\"OK\",\n                metric=Success(1.0),\n            )\n        ],\n        symbols=[],\n    )\n\n    # Test plugin\n    plugin = MyPlugin()\n    plugin.process(context)\n\n    # Assert expected behavior\n    assert os.path.exists(\"expected_output.json\")\n</code></pre>"},{"location":"plugin_system/#best-practices","title":"Best Practices","text":"<ol> <li>Be Resilient: Handle errors gracefully</li> <li>Be Fast: Complete processing quickly</li> <li>Be Focused: Do one thing well</li> <li>Be Testable: Write unit tests for your plugins</li> <li>Be Documented: Include clear metadata and docstrings</li> </ol>"},{"location":"plugin_system/#future-enhancements","title":"Future Enhancements","text":"<p>The plugin system is designed for extension. Potential future features: - Async plugin execution - Plugin dependencies - Plugin configuration system - Built-in plugin marketplace - Plugin health monitoring</p>"},{"location":"plugin_system/#see-also","title":"See Also","text":"<ul> <li>Plugin Demo - Complete working example</li> <li>API Reference - Full API documentation</li> <li>Architecture - System design details</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get started with DQX in 5 minutes! This guide shows you how to create your first data quality checks.</p>"},{"location":"quickstart/#basic-example","title":"Basic Example","text":"<p>Here's a simple example checking data quality on a pandas DataFrame:</p> <pre><code>import pandas as pd\nfrom dqx import DataQualityValidator\n\n# Sample data\ndf = pd.DataFrame(\n    {\n        \"user_id\": [1, 2, 3, 4, 5],\n        \"age\": [25, 30, -5, 150, 35],\n        \"email\": [\n            \"john@example.com\",\n            \"invalid-email\",\n            \"jane@test.com\",\n            None,\n            \"bob@demo.com\",\n        ],\n        \"score\": [85.5, 92.0, 78.5, None, 88.0],\n    }\n)\n\n# Create validator\nvalidator = DataQualityValidator()\n\n# Define checks\nchecks = (\n    validator.create_checks(df)\n    .is_not_null(\"user_id\")\n    .is_between(\"age\", 0, 120)\n    .matches_pattern(\"email\", r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\")\n    .is_not_null(\"score\")\n)\n\n# Run validation\nresults = validator.validate(df, checks)\n\n# Display results\nprint(results.summary())\n</code></pre>"},{"location":"quickstart/#understanding-results","title":"Understanding Results","text":"<p>DQX provides detailed validation results:</p> <pre><code># Check if validation passed\nif results.passed:\n    print(\"\u2705 All checks passed!\")\nelse:\n    print(f\"\u274c {results.failed_count} checks failed\")\n\n# Get detailed results\nfor check_result in results.details:\n    print(f\"{check_result.check_name}: {check_result.status}\")\n    if not check_result.passed:\n        print(f\"  Failed rows: {check_result.failed_rows}\")\n</code></pre>"},{"location":"quickstart/#common-data-quality-checks","title":"Common Data Quality Checks","text":""},{"location":"quickstart/#1-null-value-checks","title":"1. Null Value Checks","text":"<pre><code># Check for nulls\nvalidator.create_checks(df).is_not_null(\n    \"column_name\"\n).has_no_nulls()  # Check all columns\n</code></pre>"},{"location":"quickstart/#2-range-validation","title":"2. Range Validation","text":"<pre><code># Numeric ranges\nvalidator.create_checks(df).is_between(\"age\", 0, 120).is_positive(\n    \"amount\"\n).is_greater_than(\"score\", 0)\n</code></pre>"},{"location":"quickstart/#3-pattern-matching","title":"3. Pattern Matching","text":"<pre><code># String patterns\nvalidator.create_checks(df).matches_pattern(\n    \"email\", r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n).matches_pattern(\"phone\", r\"^\\d{3}-\\d{3}-\\d{4}$\").has_length(\"zip_code\", 5)\n</code></pre>"},{"location":"quickstart/#4-uniqueness-checks","title":"4. Uniqueness Checks","text":"<pre><code># Unique values\nvalidator.create_checks(df).is_unique(\"user_id\").has_no_duplicates(\n    [\"first_name\", \"last_name\"]\n)\n</code></pre>"},{"location":"quickstart/#5-statistical-checks","title":"5. Statistical Checks","text":"<pre><code># Statistical validation\nvalidator.create_checks(df).mean_between(\"score\", 70, 90).std_dev_less_than(\n    \"price\", 100\n).percentile_between(\"income\", 0.25, 10000, 50000)\n</code></pre>"},{"location":"quickstart/#working-with-different-data-sources","title":"Working with Different Data Sources","text":""},{"location":"quickstart/#sql-databases","title":"SQL Databases","text":"<pre><code>from dqx import SQLDataSource\n\n# Connect to database\ndatasource = SQLDataSource(connection_string=\"postgresql://...\")\n\n# Validate query results\nquery = \"SELECT * FROM users WHERE created_at &gt; '2024-01-01'\"\nresults = validator.validate_query(datasource, query, checks)\n</code></pre>"},{"location":"quickstart/#csv-files","title":"CSV Files","text":"<pre><code># Validate CSV directly\nresults = validator.validate_file(\"data.csv\", checks)\n\n# Or load and validate\ndf = pd.read_csv(\"data.csv\")\nresults = validator.validate(df, checks)\n</code></pre>"},{"location":"quickstart/#parquet-files","title":"Parquet Files","text":"<pre><code># Validate Parquet files\nresults = validator.validate_file(\"data.parquet\", checks)\n</code></pre>"},{"location":"quickstart/#custom-validation-rules","title":"Custom Validation Rules","text":"<p>Create custom validation logic:</p> <pre><code>from dqx import custom_check\n\n\n@custom_check\ndef business_rule_check(df):\n    \"\"\"Custom business rule validation\"\"\"\n    mask = (df[\"status\"] == \"active\") &amp; (df[\"balance\"] &gt; 0)\n    return mask\n\n\n# Use custom check\nvalidator.create_checks(df).add_custom_check(\n    business_rule_check, \"active_positive_balance\"\n)\n</code></pre>"},{"location":"quickstart/#validation-reporting","title":"Validation Reporting","text":""},{"location":"quickstart/#summary-report","title":"Summary Report","text":"<pre><code># Get summary statistics\nsummary = results.summary()\nprint(f\"Total checks: {summary['total']}\")\nprint(f\"Passed: {summary['passed']}\")\nprint(f\"Failed: {summary['failed']}\")\nprint(f\"Pass rate: {summary['pass_rate']:.1%}\")\n</code></pre>"},{"location":"quickstart/#detailed-report","title":"Detailed Report","text":"<pre><code># Generate detailed HTML report\nresults.to_html(\"validation_report.html\")\n\n# Or get DataFrame of results\nresults_df = results.to_dataframe()\n</code></pre>"},{"location":"quickstart/#export-results","title":"Export Results","text":"<pre><code># Export to various formats\nresults.to_json(\"results.json\")\nresults.to_csv(\"results.csv\")\n</code></pre>"},{"location":"quickstart/#error-handling","title":"Error Handling","text":"<p>DQX provides clear error messages:</p> <pre><code>try:\n    results = validator.validate(df, checks)\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\nexcept DataSourceError as e:\n    print(f\"Data source error: {e}\")\n</code></pre>"},{"location":"quickstart/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with basic null and range checks</li> <li>Incremental Validation: Add checks gradually</li> <li>Use Descriptive Names: Name your checks clearly</li> <li>Set Appropriate Thresholds: Be realistic with ranges</li> <li>Monitor Trends: Track validation results over time</li> </ol>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the User Guide for advanced features</li> <li>Learn about Plugin System for extending DQX</li> <li>Check API Reference for detailed documentation</li> <li>See Examples for real-world use cases</li> </ul>"},{"location":"quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Full Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udce7 Contact Support</li> </ul> <p>Ready to dive deeper? Check out the User Guide for comprehensive documentation.</p>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the DQX User Guide! This comprehensive guide covers all aspects of using DQX for data quality validation.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>DQX (Data Quality Excellence) transforms data validation into mathematical expressions, making it easy to find data quality issues before they impact your business.</p>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/#1-validators","title":"1. Validators","text":"<p>Validators are the heart of DQX. They orchestrate the validation process:</p> <pre><code>from dqx import DataQualityValidator\n\nvalidator = DataQualityValidator()\n</code></pre>"},{"location":"user-guide/#2-checks","title":"2. Checks","text":"<p>Checks define what to validate. DQX provides many built-in checks:</p> <ul> <li>Completeness: Null checks, missing value detection</li> <li>Consistency: Pattern matching, format validation</li> <li>Accuracy: Range checks, statistical validation</li> <li>Uniqueness: Duplicate detection, key validation</li> </ul>"},{"location":"user-guide/#3-data-sources","title":"3. Data Sources","text":"<p>DQX supports multiple data sources:</p> <ul> <li>Pandas DataFrames</li> <li>SQL databases (PostgreSQL, MySQL, SQLite)</li> <li>CSV, Parquet, JSON files</li> <li>Cloud storage (S3, GCS, Azure)</li> </ul>"},{"location":"user-guide/#4-results","title":"4. Results","text":"<p>Validation results provide detailed insights:</p> <ul> <li>Pass/fail status</li> <li>Row-level details</li> <li>Statistical summaries</li> <li>Trend analysis</li> </ul>"},{"location":"user-guide/#validation-workflow","title":"Validation Workflow","text":"<pre><code>graph LR\n    A[Data Source] --&gt; B[Define Checks]\n    B --&gt; C[Run Validation]\n    C --&gt; D[Analyze Results]\n    D --&gt; E[Take Action]</code></pre>"},{"location":"user-guide/#detailed-features","title":"Detailed Features","text":""},{"location":"user-guide/#completeness-validation","title":"Completeness Validation","text":"<p>Check for missing or null values:</p> <pre><code># Single column\nchecks.is_not_null(\"customer_id\")\n\n# Multiple columns\nchecks.are_not_null([\"name\", \"email\", \"phone\"])\n\n# Completeness threshold\nchecks.has_completeness(\"optional_field\", threshold=0.95)\n</code></pre>"},{"location":"user-guide/#consistency-validation","title":"Consistency Validation","text":"<p>Ensure data follows expected patterns:</p> <pre><code># Email format\nchecks.matches_pattern(\"email\", r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\")\n\n# Date format\nchecks.matches_date_format(\"birth_date\", \"%Y-%m-%d\")\n\n# Custom formats\nchecks.matches_format(\"product_code\", \"XXX-####\")\n</code></pre>"},{"location":"user-guide/#accuracy-validation","title":"Accuracy Validation","text":"<p>Validate data accuracy and ranges:</p> <pre><code># Numeric ranges\nchecks.is_between(\"age\", 0, 120)\nchecks.is_positive(\"amount\")\nchecks.is_not_negative(\"balance\")\n\n# Date ranges\nchecks.date_between(\"order_date\", \"2020-01-01\", \"2024-12-31\")\n\n# Statistical checks\nchecks.mean_between(\"score\", 70, 90)\nchecks.std_dev_less_than(\"variance\", 10)\n</code></pre>"},{"location":"user-guide/#uniqueness-validation","title":"Uniqueness Validation","text":"<p>Detect duplicates and ensure uniqueness:</p> <pre><code># Single column uniqueness\nchecks.is_unique(\"user_id\")\n\n# Composite key uniqueness\nchecks.has_unique_combination([\"first_name\", \"last_name\", \"birth_date\"])\n\n# Duplicate detection\nchecks.has_no_duplicates()\nchecks.duplicate_count_less_than(\"email\", 5)\n</code></pre>"},{"location":"user-guide/#cross-column-validation","title":"Cross-Column Validation","text":"<p>Validate relationships between columns:</p> <pre><code># Column comparison\nchecks.column_greater_than(\"end_date\", \"start_date\")\nchecks.columns_match(\"billing_address\", \"shipping_address\")\n\n# Conditional validation\nchecks.when('status == \"active\"').then(\"balance &gt; 0\")\n\n# Complex rules\nchecks.satisfies(lambda df: df[\"price\"] * df[\"quantity\"] == df[\"total\"])\n</code></pre>"},{"location":"user-guide/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/#custom-validation-functions","title":"Custom Validation Functions","text":"<p>Create domain-specific validations:</p> <pre><code>@validator.custom_check\ndef validate_business_rule(df):\n    \"\"\"Custom business logic\"\"\"\n    valid_mask = (\n        (df[\"customer_type\"] == \"premium\") &amp; (df[\"credit_limit\"] &gt;= 10000)\n    ) | ((df[\"customer_type\"] == \"standard\") &amp; (df[\"credit_limit\"] &lt;= 5000))\n    return valid_mask\n</code></pre>"},{"location":"user-guide/#validation-pipelines","title":"Validation Pipelines","text":"<p>Chain multiple validations:</p> <pre><code>pipeline = validator.create_pipeline()\n\n# Stage 1: Basic checks\npipeline.add_stage(\"basic\", [checks.is_not_null(\"id\"), checks.is_unique(\"id\")])\n\n# Stage 2: Business rules\npipeline.add_stage(\n    \"business\",\n    [checks.is_positive(\"revenue\"), checks.date_not_future(\"transaction_date\")],\n)\n\n# Run pipeline\nresults = pipeline.run(df)\n</code></pre>"},{"location":"user-guide/#conditional-validation","title":"Conditional Validation","text":"<p>Apply checks based on conditions:</p> <pre><code># Different rules for different segments\nchecks.when('region == \"US\"').then(\n    checks.matches_pattern(\"phone\", r\"^\\d{3}-\\d{3}-\\d{4}$\")\n)\n\nchecks.when('region == \"UK\"').then(checks.matches_pattern(\"phone\", r\"^\\+44\\d{10}$\"))\n</code></pre>"},{"location":"user-guide/#sampling-and-performance","title":"Sampling and Performance","text":"<p>Optimize validation for large datasets:</p> <pre><code># Sample validation\nvalidator.validate_sample(df, sample_size=10000, checks=checks)\n\n# Chunk processing\nvalidator.validate_chunks(df, chunk_size=50000, checks=checks)\n\n# Parallel validation\nvalidator.validate_parallel(df, checks=checks, n_jobs=4)\n</code></pre>"},{"location":"user-guide/#reporting-and-monitoring","title":"Reporting and Monitoring","text":""},{"location":"user-guide/#html-reports","title":"HTML Reports","text":"<p>Generate interactive reports:</p> <pre><code>results.generate_report(\n    \"validation_report.html\", include_charts=True, include_failed_rows=True\n)\n</code></pre>"},{"location":"user-guide/#dashboard-integration","title":"Dashboard Integration","text":"<p>Export metrics for monitoring:</p> <pre><code># Prometheus metrics\nmetrics = results.to_prometheus()\n\n# JSON for APIs\njson_results = results.to_json()\n\n# Time series data\ntime_series = results.to_time_series()\n</code></pre>"},{"location":"user-guide/#alerting","title":"Alerting","text":"<p>Set up alerts for validation failures:</p> <pre><code># Email alerts\nif not results.passed:\n    validator.send_alert(\n        to=[\"data-team@company.com\"], subject=\"Data Quality Alert\", results=results\n    )\n\n# Slack integration\nvalidator.notify_slack(webhook_url=\"https://hooks.slack.com/...\", results=results)\n</code></pre>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/#1-start-with-critical-checks","title":"1. Start with Critical Checks","text":"<p>Begin with the most important validations: - Primary key uniqueness - Not null for required fields - Basic range validations</p>"},{"location":"user-guide/#2-use-descriptive-names","title":"2. Use Descriptive Names","text":"<pre><code># Good\nchecks.is_not_null(\"customer_id\").with_name(\"Customer ID Required\")\n\n# Better\nchecks.is_not_null(\"customer_id\").with_name(\n    \"Customer ID Required\", description=\"Every order must have a valid customer ID\"\n)\n</code></pre>"},{"location":"user-guide/#3-set-appropriate-thresholds","title":"3. Set Appropriate Thresholds","text":"<pre><code># Allow some nulls in optional fields\nchecks.has_completeness(\"middle_name\", threshold=0.8)\n\n# Strict for critical fields\nchecks.has_completeness(\"email\", threshold=1.0)\n</code></pre>"},{"location":"user-guide/#4-group-related-checks","title":"4. Group Related Checks","text":"<pre><code>customer_checks = CheckGroup(\"Customer Validation\")\ncustomer_checks.add(\n    [\n        checks.is_not_null(\"customer_id\"),\n        checks.matches_pattern(\"email\", email_regex),\n        checks.is_between(\"age\", 18, 120),\n    ]\n)\n</code></pre>"},{"location":"user-guide/#5-version-your-validations","title":"5. Version Your Validations","text":"<pre><code># Track validation rules in version control\nvalidator.save_rules(\"validations/v1.0.0/customer_rules.json\")\n\n# Load versioned rules\nvalidator.load_rules(\"validations/v1.0.0/customer_rules.json\")\n</code></pre>"},{"location":"user-guide/#integration-examples","title":"Integration Examples","text":""},{"location":"user-guide/#with-pandas","title":"With Pandas","text":"<pre><code># Direct DataFrame validation\ndf = pd.read_csv(\"data.csv\")\nresults = validator.validate(df, checks)\n</code></pre>"},{"location":"user-guide/#with-sql-databases","title":"With SQL Databases","text":"<pre><code># PostgreSQL\nconn_string = \"postgresql://user:pass@localhost/db\"\nresults = validator.validate_query(conn_string, \"SELECT * FROM customers\", checks)\n</code></pre>"},{"location":"user-guide/#with-apache-spark","title":"With Apache Spark","text":"<pre><code># Spark DataFrame validation\nspark_df = spark.read.parquet(\"data.parquet\")\nresults = validator.validate_spark(spark_df, checks)\n</code></pre>"},{"location":"user-guide/#with-airflow","title":"With Airflow","text":"<pre><code># DQX Airflow operator\nfrom dqx.airflow import DQXValidationOperator\n\nvalidation_task = DQXValidationOperator(\n    task_id=\"validate_customer_data\",\n    source=\"s3://bucket/data.parquet\",\n    checks=customer_checks,\n    fail_on_error=True,\n)\n</code></pre>"},{"location":"user-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/#common-issues","title":"Common Issues","text":"<ol> <li>Memory errors with large datasets</li> <li>Use sampling or chunk processing</li> <li>Increase available memory</li> <li> <p>Use distributed processing</p> </li> <li> <p>Slow validation performance</p> </li> <li>Create indexes on validated columns</li> <li>Use parallel processing</li> <li> <p>Optimize regex patterns</p> </li> <li> <p>Connection issues</p> </li> <li>Check database credentials</li> <li>Verify network connectivity</li> <li>Check firewall rules</li> </ol>"},{"location":"user-guide/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\nvalidator = DataQualityValidator(debug=True)\n</code></pre>"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Plugin System to extend DQX</li> <li>Read the API Reference for detailed documentation</li> <li>Check out real-world examples</li> <li>Join the community</li> </ul> <p>Need help? Check our FAQ or open an issue.</p>"},{"location":"design/dql-language/","title":"DQL: A Language for Data Quality","text":""},{"location":"design/dql-language/#goals","title":"Goals","text":"<p>DQL (Data Quality Language) is designed with three goals:</p> <ol> <li>Simple, dedicated syntax \u2014 A language built specifically for data quality checks, where intent is clear and boilerplate is minimal</li> <li>DQX engine integration \u2014 Runs directly on the DQX runtime, leveraging its metric computation, profiling, and validation capabilities</li> <li>AI-native \u2014 Structured and unambiguous enough for AI agents to read, write, and modify autonomously when investigating and resolving data quality issues</li> </ol>"},{"location":"design/dql-language/#solution","title":"Solution","text":"<p>DQL expresses data quality checks in a syntax built for the task. DQX validates and executes checks directly against your database.</p> <pre><code>suite \"Order Validation\" {\n    check \"Completeness\" on orders {\n        assert null_count(customer_id) == 0  severity P0\n        assert null_count(email) / num_rows() &lt; 5%\n    }\n}\n</code></pre> <p>Run:</p> <pre><code>dql run suite.dql --connection databricks://... --date 2024-12-25\n</code></pre>"},{"location":"design/dql-language/#structure","title":"Structure","text":"<p>A DQL file contains one suite. A suite contains checks and tunables.</p> <pre><code>suite\n\u251c\u2500\u2500 metadata (name, threshold)\n\u251c\u2500\u2500 tunables\n\u2514\u2500\u2500 checks\n    \u2514\u2500\u2500 assertions\n</code></pre>"},{"location":"design/dql-language/#suite","title":"Suite","text":"<p>Every DQL file begins with a suite declaration:</p> <pre><code>suite \"E-Commerce Data Quality\" {\n    availability_threshold 80%\n\n    # checks and tunables go here\n}\n</code></pre> Field Required Description <code>name</code> Yes Identifies the suite <code>availability_threshold</code> No Minimum data availability (default: 90%)"},{"location":"design/dql-language/#check","title":"Check","text":"<p>A check groups related assertions. Each check targets one or more datasets.</p> <pre><code>check \"Price Validation\" on orders {\n    assert average(price) &gt; 0\n    assert maximum(price) &lt; 10000\n}\n</code></pre> <p>Target multiple datasets:</p> <pre><code>check \"Cross-Dataset\" on orders, returns {\n    assert num_rows(dataset=returns) / num_rows(dataset=orders) &lt; 15%\n}\n</code></pre> <p>Cross-dataset semantics: - Each dataset resolves independently for the target date - Division by zero (empty dataset) returns <code>None</code>, triggering assertion failure - No implicit join\u2014metrics compute per-dataset, then combine</p>"},{"location":"design/dql-language/#assertion","title":"Assertion","text":"<p>An assertion validates one metric against one condition. Write assertions as declarative statements:</p> <pre><code>assert average(price) &gt; 0\n</code></pre> <p>Add a name, severity, tolerance, or tags:</p> <pre><code>assert average(price) &gt; 0\n    name \"Average price is positive\"\n    severity P0\n    tags [pricing, critical]\n</code></pre> <p>With tolerance for floating-point comparison:</p> <pre><code>assert average(price) / average(price, lag=1) == 1.0 tolerance 0.05\n    name \"Ratio near 1.0\"\n</code></pre> <p>ASCII tolerance alternatives: <code>tolerance 0.05</code> or <code>+/- 0.05</code> (Unicode <code>\u00b1</code> also supported).</p> Element Required Description <code>name</code> No Descriptive label (recommended) <code>metric</code> Yes Expression to evaluate <code>condition</code> Yes Comparison or keyword <code>severity</code> No P0, P1, P2, or P3 (default: P1) <code>tolerance</code> No Margin for <code>==</code> comparisons (expands to range check) <code>tags</code> No Labels for profile targeting <code>sample</code> No Sample data before computing metric"},{"location":"design/dql-language/#sampling","title":"Sampling","text":"<p>For large datasets, sample data before computing metrics:</p> <pre><code>assert average(price) &gt; 0 sample 10%\n    name \"Price positive (10% sample)\"\n\nassert average(price) &gt; 0 sample 10000 rows\n    name \"Price positive (10k rows)\"\n</code></pre> <p>With seed for reproducibility:</p> <pre><code>assert average(price) &gt; 0 sample 10% seed 42\n    name \"Price positive (reproducible)\"\n</code></pre> Syntax Meaning <code>sample N%</code> Random sample of N percent of rows <code>sample N rows</code> Random sample of N rows <code>seed M</code> Random seed for reproducibility <p>Database compatibility:</p> Database Method DuckDB <code>USING SAMPLE N%</code> Databricks <code>TABLESAMPLE (N PERCENT)</code> PostgreSQL <code>TABLESAMPLE BERNOULLI(N)</code> BigQuery <code>WHERE RAND() &lt; N/100</code> <p>Semantics: - Sampling applies to the entire metric expression - Without <code>seed</code>, results are non-deterministic across runs - Row-based sampling (<code>N rows</code>) uses reservoir sampling for exact counts</p>"},{"location":"design/dql-language/#assertion-naming-convention","title":"Assertion Naming Convention","text":"<p>Names serve as unique identifiers for programmatic reference. Use hierarchical names for machine-friendly identification:</p> <pre><code>check \"Completeness\" on orders {\n    assert null_count(email) / num_rows() &lt; 5%\n        name \"orders.completeness.email_null\"\n\n    assert null_count(phone) / num_rows() &lt; 10%\n        name \"orders.completeness.phone_null\"\n}\n</code></pre> <p>Convention: <code>dataset.check.assertion</code> \u2014 enables algorithms to reference, add, or remove assertions by name.</p>"},{"location":"design/dql-language/#assertion-annotations","title":"Assertion Annotations","text":"<p>Annotations provide metadata for algorithmic optimization:</p> <pre><code>check \"Volume\" on orders {\n    @experimental                    # Algorithm-proposed, not yet validated\n    assert day_over_day(sum(tax)) &lt; 0.5\n        name \"orders.volume.tax_stability\"\n\n    @cost(false_positive=1, false_negative=100)\n    assert num_rows() &gt;= 1000\n        name \"orders.volume.min_rows\"\n        severity P0\n}\n</code></pre> Annotation Description <code>@experimental</code> Marks assertion as algorithm-proposed, pending validation <code>@required</code> Assertion cannot be removed or disabled by algorithms <code>@cost(false_positive=N, false_negative=M)</code> Cost of false positive (N) and false negative (M) for RL reward <p>Cost semantics: - <code>false_positive</code> \u2014 Cost when assertion fails but data is actually fine (alert fatigue) - <code>false_negative</code> \u2014 Cost when assertion passes but data has issues (missed problem) - Higher <code>fn</code> relative to <code>fp</code> makes the algorithm prefer stricter thresholds</p> <p>Safety constraints:</p> <p>Prevent algorithms from gaming the system by removing critical checks:</p> <pre><code>check \"Critical\" on orders {\n    @required                        # Cannot be removed by RL agent\n    @cost(false_positive=1, false_negative=1000)\n    assert null_count(customer_id) == 0\n        name \"orders.critical.customer_id\"\n        severity P0\n}\n</code></pre> Constraint Behavior <code>@required</code> Critical assertion that must always run <code>severity P0</code> Implied required unless explicitly <code>@experimental</code> Non-tunable threshold Threshold cannot be modified (no <code>tunable</code> keyword) <p>Constraint enforcement: Tunable constants can only be adjusted within their declared bounds. Non-tunable thresholds and <code>@required</code> assertions represent fixed business logic that algorithms cannot modify.</p> <p>Design principle: Algorithms can only tune what humans explicitly marked as tunable. Critical business logic stays protected.</p>"},{"location":"design/dql-language/#conditions","title":"Conditions","text":"<p>Conditions specify how to validate a metric:</p> Syntax Meaning <code>&gt; N</code> Greater than N <code>&gt;= N</code> Greater than or equal to N <code>&lt; N</code> Less than N <code>&lt;= N</code> Less than or equal to N <code>== N</code> Equal to N <code>!= N</code> Not equal to N <code>between A and B</code> In range [A, B] <code>is positive</code> Greater than zero <code>is negative</code> Less than zero <code>is None</code> Value is None <code>is not None</code> Value is not None"},{"location":"design/dql-language/#metrics","title":"Metrics","text":"<p>Metrics compute values from data. DQL provides built-in metrics and supports arithmetic combinations.</p>"},{"location":"design/dql-language/#built-in-metrics","title":"Built-in Metrics","text":"<p>Aggregate metrics compute over all rows:</p> <pre><code>num_rows()              # Row count\naverage(price)          # Mean value\nsum(revenue)            # Total\nminimum(quantity)       # Smallest value\nmaximum(temperature)    # Largest value\nvariance(score)         # Statistical variance\n</code></pre> <p>Completeness metrics measure data presence:</p> <pre><code>null_count(email)           # Count of None values\nunique_count(customer_id)   # Count of distinct values\nduplicate_count([id, date]) # Count of duplicate combinations\n</code></pre> <p>Value metrics check specific values:</p> <pre><code>count_values(status, \"pending\")  # Rows where status == \"pending\"\nfirst(timestamp)                 # First value by row order\nfirst(timestamp, order_by=price) # First value ordered by price\n</code></pre> <p><code>first()</code> returns the first value in storage order unless <code>order_by</code> specifies a column.</p> <p>Warning: Without <code>order_by</code>, results are non-deterministic for distributed storage (Parquet, Delta Lake). Row order depends on file layout and query execution. Always use <code>order_by</code> for reproducible results.</p> <p>Custom SQL executes arbitrary expressions:</p> <pre><code>sql(\"SUM(amount) / COUNT(*)\")\n</code></pre> <p><code>sql()</code> limitations:</p> <ul> <li>No validation \u2014 The interpreter cannot verify column names, syntax, or types</li> <li>No interpolation \u2014 Constants cannot be inserted into SQL strings (prevents SQL injection)</li> <li>Dialect-specific \u2014 SQL syntax varies across databases; DQL does not translate</li> </ul> <pre><code># Preferred: interpreter validates\nassert sum(amount) / num_rows() &gt; 10\n\n# Escape hatch: no validation\nassert sql(\"SUM(amount) / COUNT(*)\") &gt; 10\n\n# ERROR: This example shows unsupported syntax for illustration\n# DQL does not support string interpolation in sql() functions\ntunable MY_COL = \"amount\" bounds [\"amount\", \"revenue\"]  # ERROR: String interpolation not supported\nassert sql(\"SUM({MY_COL})\") &gt; 0  # ERROR: Interpolation in sql() not permitted\n</code></pre> <p>Use built-in metrics when possible; reserve <code>sql()</code> for expressions DQL cannot represent.</p>"},{"location":"design/dql-language/#parameters","title":"Parameters","text":"<p>Metrics accept optional parameters:</p> <pre><code>average(price, lag=1)           # Yesterday's average\naverage(price, dataset=orders)  # Specific dataset\n</code></pre> Parameter Description <code>lag</code> Calendar days offset (1 = yesterday, 7 = one week ago) <code>dataset</code> Restrict metric to named dataset <p>Lag semantics: The <code>lag</code> parameter offsets by calendar days, not partition offsets. For a weekly-partitioned table queried on Monday with <code>lag 1</code>, VerificationSuite computes the metric for Sunday's data (which may fall in the previous week's partition).</p>"},{"location":"design/dql-language/#extended-metrics","title":"Extended Metrics","text":"<p>Extended metrics wrap base metrics for time-series analysis:</p> <pre><code>day_over_day(average(price))     # Absolute % change vs yesterday\nweek_over_week(sum(revenue))     # Absolute % change vs last week\nstddev(average(price), n 7)      # Standard deviation over 7 days\n</code></pre> <p>Day-over-day computes <code>abs((today - yesterday) / yesterday)</code>. A result of <code>0.1</code> means 10% change.</p>"},{"location":"design/dql-language/#arithmetic","title":"Arithmetic","text":"<p>Combine metrics with arithmetic operators:</p> <pre><code>null_count(email) / num_rows()   # Null percentage\nsum(revenue) - sum(cost)         # Profit\naverage(price) * 1.1             # 10% markup\n</code></pre> <p>Math functions:</p> <pre><code>abs(day_over_day(price) - 1.0)   # Distance from no change\nsqrt(variance(score))            # Standard deviation\n</code></pre> <p>Supported functions: <code>abs</code>, <code>sqrt</code>, <code>log</code>, <code>exp</code>.</p> <p>Utility functions:</p> <pre><code>coalesce(average(price), 0)      # Default if None\ncoalesce(sum(cost), sum(revenue), 0)  # First non-None\n</code></pre> <p>Supported: <code>coalesce(expr, expr, ...)</code>.</p>"},{"location":"design/dql-language/#tunables","title":"Tunables","text":"<p>Tunables define reusable values with explicit bounds for algorithmic optimization. All tunables require bounds specification to enable RL-based threshold tuning.</p> <p>Declare tunables at suite level:</p> <pre><code>suite \"Orders\" {\n    tunable NULL_THRESHOLD = 5% bounds [0%, 20%]\n    tunable MIN_ORDERS = 1000 bounds [100, 10000]\n    tunable VARIANCE_LIMIT = 0.5 bounds [0.1, 1.0]\n\n    check \"Completeness\" on orders {\n        assert null_count(email) / num_rows() &lt; NULL_THRESHOLD\n        assert num_rows() &gt;= MIN_ORDERS\n    }\n}\n</code></pre>"},{"location":"design/dql-language/#syntax","title":"Syntax","text":"<pre><code>tunable &lt;NAME&gt; = &lt;value&gt; bounds [&lt;min&gt;, &lt;max&gt;]\n</code></pre> Element Required Description <code>tunable</code> Yes Keyword to declare a tunable parameter <code>&lt;NAME&gt;</code> Yes Identifier for the tunable (uppercase convention) <code>&lt;value&gt;</code> Yes Initial/default value <code>bounds</code> Yes Keyword introducing the bounds specification <code>[&lt;min&gt;, &lt;max&gt;]</code> Yes Valid range for optimization (inclusive) <p>Semantics: - Algorithms can modify tunables within their declared bounds - Bounds are always required - no implicit or unbounded tunables - Bounds are inclusive: <code>[0%, 20%]</code> allows values from 0% to 20% - Initial value must be within the specified bounds</p> <p>Examples:</p> <pre><code># Percentage tunables\ntunable MAX_NULL_RATE = 5% bounds [0%, 20%]\n\n# Integer tunables\ntunable MIN_DAILY_ORDERS = 1000 bounds [100, 10000]\n\n# Decimal tunables\ntunable VARIANCE_THRESHOLD = 0.3 bounds [0.0, 1.0]\n</code></pre>"},{"location":"design/dql-language/#python-tunable-api","title":"Python Tunable API","text":"<p>Tunables are implemented as an extensible type hierarchy in <code>dqx/tunables.py</code>:</p> <pre><code>from dqx.tunables import TunableFloat, TunablePercent, TunableInt, TunableChoice\n\n# Define tunables with type-specific validation\nNULL_THRESHOLD = TunablePercent(\"NULL_THRESHOLD\", value=0.05, bounds=(0.0, 0.20))\nMIN_ORDERS = TunableInt(\"MIN_ORDERS\", value=1000, bounds=(100, 10000))\nTOLERANCE = TunableFloat(\"TOLERANCE\", value=0.001, bounds=(0.0001, 0.01))\nAGG_METHOD = TunableChoice(\n    \"AGG_METHOD\", value=\"mean\", choices=(\"mean\", \"median\", \"max\")\n)\n\n# Register with suite at construction\nsuite = VerificationSuite(\n    checks=[completeness_check],\n    db=db,\n    name=\"Orders\",\n    tunables=[NULL_THRESHOLD, MIN_ORDERS, TOLERANCE, AGG_METHOD],\n)\n\n\n# Use in assertions\n@check(name=\"Completeness\")\ndef completeness_check(mp: MetricProvider, ctx: Context):\n    ctx.assert_that(mp.null_count(\"email\") / mp.num_rows()).where(\n        name=\"Email null rate\"\n    ).is_lt(\n        NULL_THRESHOLD.value\n    )  # .value gets current value\n</code></pre> <p>Tunable Types:</p> Type Description Example <code>TunableFloat</code> Bounded float <code>TunableFloat(\"X\", value=0.5, bounds=(0.0, 1.0))</code> <code>TunablePercent</code> Percentage (0-1 internally) <code>TunablePercent(\"X\", value=0.05, bounds=(0.0, 0.20))</code> <code>TunableInt</code> Bounded integer <code>TunableInt(\"X\", value=100, bounds=(10, 1000))</code> <code>TunableChoice</code> Categorical <code>TunableChoice(\"X\", value=\"a\", choices=(\"a\", \"b\", \"c\"))</code> <p>RL Agent API:</p> <pre><code># Get all tunable parameters for action space\nparams = suite.get_tunable_params()\n# [\n#   {\"name\": \"NULL_THRESHOLD\", \"type\": \"percent\", \"value\": 0.05, \"bounds\": (0.0, 0.20)},\n#   {\"name\": \"MIN_ORDERS\", \"type\": \"int\", \"value\": 1000, \"bounds\": (100, 10000)},\n# ]\n\n# Modify with validation and history tracking\nsuite.set_param(\"NULL_THRESHOLD\", 0.03, agent=\"rl_optimizer\", reason=\"Episode 42\")\n\n# View change history\nhistory = suite.get_param_history(\"NULL_THRESHOLD\")\n# [TunableChange(timestamp=..., old_value=0.05, new_value=0.03, agent=\"rl_optimizer\", ...)]\n</code></pre>"},{"location":"design/dql-language/#percentage-semantics","title":"Percentage Semantics","text":"<p>Percentage literals convert to decimals: <code>5%</code> becomes <code>0.05</code>.</p> <pre><code>assert null_count(email) / num_rows() &lt; 5%   # 5% = 0.05\nassert day_over_day(num_rows()) &lt; 0.5        # 0.5 = 50% change (raw decimal)\n</code></pre> <p>Rule: Use <code>%</code> suffix for human-readable percentages. Use raw decimals for ratios. The interpreter treats <code>5%</code> and <code>0.05</code> identically\u2014the distinction is for readability.</p>"},{"location":"design/dql-language/#none-handling","title":"None Handling","text":"<p>None propagates through arithmetic:</p> <pre><code>sum(revenue) - sum(cost)   # None if either is None\n</code></pre> <p>None in a comparison fails the assertion:</p> <pre><code>assert average(price) &gt; 0   # Fails if average(price) is None\n</code></pre> <p>Division by zero returns None:</p> <pre><code>sum(a) / sum(b)   # None if sum(b) == 0\n0 / 0             # None (not NaN)\n</code></pre> <p>Use <code>coalesce</code> to provide defaults:</p> <pre><code>assert coalesce(average(price), 0) &gt;= 0\n</code></pre>"},{"location":"design/dql-language/#profiles","title":"Profiles","text":""},{"location":"design/dql-language/#profiles-removed-in-v060","title":"Profiles (Removed in v0.6.0)","text":"<p>Note: Profile definitions are no longer part of DQL syntax as of v0.6.0.</p> <p>Profiles are now defined in: - YAML configuration files (recommended for most use cases) - Python API using <code>SeasonalProfile</code> class (for programmatic control)</p> <p>See the Migration Guide for converting existing DQL profiles to YAML or Python API.</p> <p>Why the change? Profiles define runtime behavior (when/how to modify checks), which is conceptually separate from validation logic (what to check). This separation: - Enables environment-specific configuration without modifying DQL - Allows profile reuse across multiple suites - Follows the same pattern as tunables - Makes DQL simpler and more focused on validation logic</p> <p>YAML Example: <pre><code>profiles:\n  - name: \"Holiday Season\"\n    type: \"seasonal\"\n    start_date: \"2024-12-20\"\n    end_date: \"2025-01-05\"\n    rules:\n      - action: \"disable\"\n        target: \"check\"\n        name: \"Volume\"\n      - action: \"scale\"\n        target: \"tag\"\n        name: \"reconciliation\"\n        multiplier: 1.5\n</code></pre></p> <p>Python API Example: <pre><code>from dqx.profiles import SeasonalProfile, check, tag\n\nholiday = SeasonalProfile(\n    name=\"Holiday Season\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[\n        check(\"Volume\").disable(),\n        tag(\"reconciliation\").set(metric_multiplier=1.5),\n    ],\n)\n\nsuite = VerificationSuite(\n    dql=Path(\"suite.dql\"),\n    db=db,\n    profiles=[holiday],\n)\n</code></pre></p> <p>For full documentation on profiles, see Profiles Design.</p>"},{"location":"design/dql-language/#complete-example","title":"Complete Example","text":"<pre><code>suite \"E-Commerce Data Quality\" {\n    availability_threshold 80%\n\n    tunable MAX_NULL_RATE = 5% bounds [0%, 20%]\n    tunable MIN_ORDERS = 1000 bounds [100, 10000]\n\n    check \"Completeness\" on orders {\n        assert null_count(customer_id) == 0\n            name \"No null customer IDs\"\n            severity P0\n\n        assert null_count(email) / num_rows() &lt; MAX_NULL_RATE\n            name \"Email null rate below threshold\"\n\n        assert null_count(phone) / num_rows() &lt; MAX_NULL_RATE\n            name \"Phone null rate below threshold\"\n    }\n\n    check \"Volume\" on orders {\n        assert num_rows() &gt;= MIN_ORDERS\n            name \"At least minimum orders\"\n            tags [volume]\n\n        assert day_over_day(num_rows()) between 0.5 and 2.0\n            name \"Day-over-day stable\"\n            tags [volume, trend]\n    }\n\n    check \"Revenue\" on orders {\n        assert sum(amount) + sum(tax) is positive\n            name \"Total revenue positive\"\n            severity P0\n\n        assert sum(amount) / num_rows() between 10 and 500\n            name \"Average order value in range\"\n    }\n\n    check \"Cross-Dataset\" on orders, returns {\n        assert num_rows(dataset=returns) / num_rows(dataset=orders) &lt; 15%\n            name \"Return rate below 15%\"\n    }\n\n    check \"Stability\" on orders {\n        assert abs(day_over_day(average(price))) &lt; 0.1\n            name \"Price change within 10%\"\n            tags [stability]\n\n        assert sqrt(variance(amount)) / average(amount) &lt; 0.5\n            name \"Coefficient of variation acceptable\"\n            tags [stability]\n    }\n\n    # Note: Profiles are now defined in YAML configuration or via Python API\n    # See \"Profiles (Removed)\" section below for migration details\n}\n</code></pre>"},{"location":"design/dql-language/#history-file","title":"History File","text":"<p>DQL tracks changes in a separate history file to support algorithmic optimization and auditing. The history file is a JSONL (JSON Lines) file alongside the DQL source.</p> <p>File convention: - <code>suite.dql</code> \u2014 The specification (human + algorithm authored) - <code>suite.dql.history</code> \u2014 Change log (append-only)</p>"},{"location":"design/dql-language/#history-format","title":"History Format","text":"<pre><code>{\"ts\": \"2024-12-01T10:00:00Z\", \"action\": \"set_param\", \"param\": \"NULL_THRESHOLD\", \"old\": 0.10, \"new\": 0.05, \"agent\": \"rl_optimizer\", \"episode\": 42}\n{\"ts\": \"2024-12-15T14:30:00Z\", \"action\": \"set_param\", \"param\": \"MIN_ORDERS\", \"old\": 1000, \"new\": 800, \"agent\": \"autotuner\", \"reason\": \"seasonal adjustment\"}\n{\"ts\": \"2024-12-20T09:00:00Z\", \"action\": \"set_param\", \"param\": \"TOLERANCE\", \"old\": 0.001, \"new\": 0.002, \"agent\": \"human\", \"reason\": \"reduce false positives\"}\n</code></pre>"},{"location":"design/dql-language/#history-actions","title":"History Actions","text":"Action Description Fields <code>set_param</code> Tunable parameter changed <code>param</code>, <code>old</code>, <code>new</code>, <code>agent</code>, <code>reason</code>"},{"location":"design/dql-language/#agent-field","title":"Agent Field","text":"<p>The <code>agent</code> field identifies who made the change: - <code>human</code> \u2014 Manual edit by engineer - <code>rl_optimizer</code> \u2014 Reinforcement learning algorithm - <code>autotuner</code> \u2014 Threshold optimization algorithm - Custom agent names for other automation</p>"},{"location":"design/dql-language/#usage","title":"Usage","text":"<pre><code># View history\ndql history suite.dql\n\n# Rollback to previous state\ndql rollback suite.dql --to 2024-12-15\n\n# Export history for analysis\ndql history suite.dql --format csv &gt; changes.csv\n</code></pre> <p>The history file can be git-tracked separately or added to <code>.gitignore</code> depending on team preference.</p>"},{"location":"design/dql-language/#runtime","title":"Runtime","text":"<p>The interpreter executes DQL files directly against your database. Metric expressions pass to sympy for parsing, enabling full compatibility with DQX's Python runtime.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      DQL Source                             \u2502\n\u2502                     (suite.dql)                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Interpreter   \u2502\n                    \u2502   (via sympy)   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   DQX Runtime   \u2502\n                    \u2502   (database)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"design/dql-language/#installation","title":"Installation","text":"<pre><code>pip install dqx[dql]\n</code></pre>"},{"location":"design/dql-language/#commands","title":"Commands","text":"<p>Run a suite:</p> <pre><code>dql run suite.dql --connection databricks://host/warehouse --date 2024-12-25\n</code></pre> <p>Validate without executing:</p> <pre><code>dql check suite.dql\n</code></pre> <p>Watch for changes (re-runs on file modification):</p> <pre><code>dql run suite.dql --watch\n</code></pre> <p>Output formats:</p> <pre><code>dql run suite.dql --output json      # Machine-readable\ndql run suite.dql --output table     # Human-readable (default)\ndql run suite.dql --output summary   # Pass/fail counts only\n</code></pre>"},{"location":"design/dql-language/#configuration-file","title":"Configuration File","text":"<p>Store connection settings in <code>dqx.toml</code> to avoid repeating arguments:</p> <pre><code>[connection]\ntype = \"databricks\"\nhost = \"workspace.cloud.databricks.com\"\nhttp_path = \"/sql/1.0/warehouses/abc123\"\n\n[defaults]\ndate = \"today\"\noutput = \"table\"\n</code></pre> <p>Then run:</p> <pre><code>dql run suite.dql\n</code></pre>"},{"location":"design/dql-language/#python-api","title":"Python API","text":"<p>Execute DQL through VerificationSuite:</p> <pre><code>from dqx.api import VerificationSuite\nfrom dqx.common import ResultKey\nfrom datetime import date\nfrom pathlib import Path\n\nsuite = VerificationSuite(\n    dql=Path(\"suite.dql\"),\n    db=db,\n    config=Path(\"config.yaml\"),  # Optional: load tunables + profiles\n)\n\nsuite.run(datasources, ResultKey(date.today(), {}))\nresults = suite.collect_results()\n\nfor r in results:\n    print(f\"{r.check}/{r.assertion_name}: {r.status}\")\n</code></pre> <p>Or use inline DQL source:</p> <pre><code>dql_source = \"\"\"\nsuite \"Quick Check\" {\n    check \"Basic\" on orders {\n        assert num_rows() &gt; 0\n            name \"Has rows\"\n    }\n}\n\"\"\"\n\nsuite = VerificationSuite(\n    dql=dql_source,\n    db=db,\n)\n\nsuite.run(datasources, key)\nresults = suite.collect_results()\n</code></pre>"},{"location":"design/dql-language/#rl-agent-integration","title":"RL Agent Integration","text":"<p>DQL provides a programmatic API for reinforcement learning agents to optimize data quality checks:</p> <ol> <li>Read tunable parameters and their bounds</li> <li>Modify thresholds within bounds</li> <li>Run checks and observe outcomes</li> <li>Compute rewards from results and costs</li> <li>Log changes to history</li> </ol> <p>Action Space:</p> <p>The RL agent operates on a continuous, bounded action space derived from tunable parameters:</p> Component Type Description Threshold adjustments Continuous, bounded One dimension per <code>tunable</code> parameter <pre><code># Continuous action space derived from tunable parameters\naction_space = {\n    \"NULL_THRESHOLD\": (0.0, 0.20),  # from tunable [0%, 20%]\n    \"MIN_ORDERS\": (100, 10000),  # from tunable [100, 10000]\n    \"DOD_LIMIT\": (0.1, 1.0),  # from tunable [0.1, 1.0]\n}\n\n# Example action from policy\naction = {\n    \"NULL_THRESHOLD\": 0.03,  # Lower threshold (stricter)\n    \"MIN_ORDERS\": 1500,  # Raise minimum\n    \"DOD_LIMIT\": 0.4,  # Slightly tighter\n}\n</code></pre> <p>For threshold optimization, use bounded continuous control algorithms (PPO, SAC).</p> <p>Example DQL with tunable thresholds and costs:</p> <pre><code>suite \"Orders\" {\n    tunable NULL_THRESHOLD = 5% bounds [0%, 20%]\n    tunable MIN_ORDERS = 1000 bounds [100, 10000]\n    tunable DOD_LIMIT = 0.5 bounds [0.1, 1.0]\n\n    check \"Completeness\" on orders {\n        @cost(false_positive=1, false_negative=50)\n        assert null_count(email) / num_rows() &lt; NULL_THRESHOLD\n            name \"orders.completeness.email_null\"\n\n        @cost(false_positive=1, false_negative=100)\n        assert null_count(customer_id) == 0\n            name \"orders.completeness.customer_id_null\"\n            severity P0\n    }\n\n    check \"Volume\" on orders {\n        @cost(false_positive=5, false_negative=200)\n        assert num_rows() &gt;= MIN_ORDERS\n            name \"orders.volume.min_rows\"\n\n        @experimental\n        @cost(false_positive=2, false_negative=50)\n        assert day_over_day(num_rows()) &lt; DOD_LIMIT\n            name \"orders.volume.dod_stability\"\n    }\n}\n</code></pre> <p>RL Agent Implementation:</p> <pre><code>from dqx.dql import Suite, History\nfrom datetime import date, timedelta\nimport numpy as np\n\n\nclass DQThresholdAgent:\n    \"\"\"RL agent that optimizes DQL thresholds.\"\"\"\n\n    def __init__(self, suite_path: str, db):\n        self.suite = Suite.load(suite_path)\n        self.history = History(suite_path + \".history\")\n        self.db = db\n        self.episode = 0\n\n    def get_state(self) -&gt; np.ndarray:\n        \"\"\"Extract current thresholds as state vector.\"\"\"\n        params = self.suite.get_tunable_params()\n        return np.array([p.normalized_value for p in params])\n\n    def get_action_space(self) -&gt; list:\n        \"\"\"Get tunable parameters with bounds.\"\"\"\n        return [\n            {\"name\": p.name, \"bounds\": (p.min_bound, p.max_bound)}\n            for p in self.suite.get_tunable_params()\n        ]\n\n    def apply_action(self, actions: dict[str, float]):\n        \"\"\"Apply threshold changes from RL policy.\"\"\"\n        for name, new_value in actions.items():\n            old = self.suite.get_param(name)\n            self.suite.set_param(name, new_value)\n            self.history.log(\n                {\n                    \"action\": \"set_param\",\n                    \"param\": name,\n                    \"old\": old,\n                    \"new\": new_value,\n                    \"agent\": \"rl_optimizer\",\n                    \"episode\": self.episode,\n                }\n            )\n\n    def run_and_observe(self, target_date: date) -&gt; dict:\n        \"\"\"Execute checks and return structured results.\"\"\"\n        results = self.suite.run(db=self.db, date=target_date)\n        return {\n            \"assertions\": [\n                {\n                    \"name\": r.name,\n                    \"passed\": r.passed,\n                    \"value\": r.metric_value,\n                }\n                for r in results.assertions\n            ]\n        }\n\n    def compute_reward(self, results: dict, ground_truth: dict = None) -&gt; float:\n        \"\"\"Compute reward from results.\n\n        Simple reward: +1 for correct predictions, -1 for incorrect.\n        Can be extended to use @cost annotations when available.\n        \"\"\"\n        reward = 0.0\n        for a in results[\"assertions\"]:\n            if ground_truth and a[\"name\"] in ground_truth:\n                actual_issue = ground_truth[a[\"name\"]]\n                if a[\"passed\"] == (not actual_issue):\n                    reward += 1.0  # Correct prediction\n                else:\n                    reward -= 1.0  # Incorrect prediction\n        return reward\n\n    def save(self):\n        \"\"\"Persist changes to DQL file and history.\"\"\"\n        self.suite.save()\n        self.history.flush()\n</code></pre> <p>Training Loop:</p> <pre><code>agent = DQThresholdAgent(\"suite.dql\", db_connection)\n\nfor episode in range(1000):\n    agent.episode = episode\n    state = agent.get_state()\n\n    # RL policy selects threshold adjustments\n    actions = policy.select_action(state)\n    agent.apply_action(actions)\n\n    # Run on historical date for training\n    train_date = date.today() - timedelta(days=np.random.randint(1, 90))\n    results = agent.run_and_observe(train_date)\n\n    # Reward signal (with optional ground truth labels)\n    reward = agent.compute_reward(results, labeled_data.get(train_date))\n\n    # Policy gradient update\n    next_state = agent.get_state()\n    policy.update(state, actions, reward, next_state)\n\n    if episode % 100 == 0:\n        agent.save()\n</code></pre> <p>Key API Methods:</p> Method Description <code>Suite.load(path)</code> Parse DQL file into manipulable object <code>suite.get_tunable_params()</code> List parameters with bounds for action space <code>suite.set_param(name, value)</code> Modify threshold (validates bounds) <code>suite.get_param(name)</code> Get current value of a parameter <code>suite.get_param_history(name)</code> Get change history for a parameter <code>suite.run(db, date)</code> Execute checks, return structured results <code>suite.save()</code> Persist changes back to <code>.dql</code> file <code>History.log(entry)</code> Append to <code>.dql.history</code> file <p>Profile-Aware Training:</p> <p>Profiles apply multipliers to metrics during certain periods (holidays, promotions). The RL agent must account for this when training on historical data:</p> <pre><code>def run_and_observe(self, target_date: date) -&gt; dict:\n    \"\"\"Execute checks with profile context.\"\"\"\n    results = self.suite.run(db=self.db, date=target_date)\n\n    # Get active profiles for this date\n    active_profiles = self.suite.get_active_profiles(target_date)\n\n    return {\n        \"assertions\": [...],\n        \"profiles\": [\n            {\"name\": p.name, \"multipliers\": p.get_multipliers()}\n            for p in active_profiles\n        ],\n        \"is_special_period\": len(active_profiles) &gt; 0,\n    }\n</code></pre> <p>Two training strategies:</p> Strategy Description When to use Profile-aware Include profile context in state, learn separate thresholds When profiles are stable and well-defined Profile-excluded Train only on non-profile dates When you want single robust threshold <pre><code># Strategy 1: Profile-aware state\ndef get_state(self) -&gt; np.ndarray:\n    params = [p.normalized_value for p in self.suite.get_tunable_params()]\n\n    # Add profile indicators to state\n    profile_active = [\n        1.0 if self.suite.is_profile_active(p.name, self.current_date) else 0.0\n        for p in self.suite.get_profiles()\n    ]\n\n    return np.array(params + profile_active)\n\n\n# Strategy 2: Exclude profile periods from training\ndef sample_training_date(self) -&gt; date:\n    while True:\n        d = date.today() - timedelta(days=np.random.randint(1, 90))\n        if not self.suite.get_active_profiles(d):\n            return d  # Only train on \"normal\" periods\n</code></pre> <p>Profile multipliers in reward computation:</p> <p>The interpreter applies multipliers before comparison, so the RL agent sees post-multiplier results. This is important:</p> <pre><code>check \"Volume\" on orders {\n    assert num_rows() &gt;= 1000\n        tags [volume]\n}\n\nprofile \"Holiday\" {\n    from 2024-12-20\n    to   2024-12-31\n    scale tag \"volume\" by 2.0\n}\n</code></pre> <pre><code># On holiday: actual rows = 600\n# Multiplier 2.0 applied: 600 * 2.0 = 1200\n# Comparison: 1200 &gt;= 1000 \u2192 PASS\n\n# The agent sees:\n{\n    \"name\": \"orders.volume.min_rows\",\n    \"passed\": True,\n    \"raw_value\": 600,  # Before multiplier\n    \"scaled_value\": 1200,  # After multiplier (used for comparison)\n    \"multiplier\": 2.0,\n    \"threshold\": 1000,\n}\n</code></pre> <p>Implication: Thresholds are defined for \"normal\" periods. Profiles handle anomalous periods. The RL agent should optimize thresholds for normal conditions\u2014profiles automatically adjust for special periods.</p> <p>Human-in-the-Loop Integration:</p> <p>The system supports multiple touchpoints for human oversight:</p> Touchpoint Purpose Frequency Review queue Approve/reject algorithm proposals Before deployment Ground truth labeling Improve reward signal accuracy Periodic batch Shadow mode Test changes without affecting production Before promotion Rollback Revert bad changes On-demand <p>1. Ground Truth Labeling (False Positive Annotation):</p> <p>When an assertion fails, humans label whether it was a real issue or false alarm:</p> <pre><code># View recent failures needing review\ndql alerts suite.dql --unlabeled --days 7\n\n# DATE        ASSERTION                        VALUE    THRESHOLD  STATUS\n# 2024-12-20  orders.volume.min_rows           950      &gt;= 1000    FAIL\n# 2024-12-21  orders.completeness.email_null   6.2%     &lt; 5%       FAIL\n\n# Human investigates and labels\ndql label suite.dql \\\n    --date 2024-12-20 \\\n    --assertion \"orders.volume.min_rows\" \\\n    --false-positive \\\n    --reason \"Planned vendor maintenance\"\n</code></pre> Alert Status <code>ground_truth</code> Meaning RL Impact FAIL <code>true</code> True positive (real issue) No penalty FAIL <code>false</code> False positive (false alarm) <code>-cost_fp</code> penalty PASS <code>true</code> False negative (missed issue) <code>-cost_fn</code> penalty PASS <code>false</code> True negative (correct) No penalty <p>2. Review Queue for Proposed Changes:</p> <pre><code># CLI for review workflow\ndql review suite.dql --pending              # List pending changes\ndql review suite.dql --approve &lt;entry_id&gt;   # Approve change\ndql review suite.dql --reject &lt;entry_id&gt; --reason \"too aggressive\"\n</code></pre> <p>3. Staged Deployment:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 RL Proposes \u2502 \u2500\u2500\u25b6 \u2502   Pending   \u2502 \u2500\u2500\u25b6 \u2502   Shadow    \u2502 \u2500\u2500\u25b6 \u2502 Production  \u2502\n\u2502   Change    \u2502     \u2502   Review    \u2502     \u2502    Mode     \u2502     \u2502    Live     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502                    \u2502\n                     Human reviews        Runs in parallel,\n                     and approves         alerts logged but\n                                          not actioned\n</code></pre> <pre><code># Shadow mode: run proposed thresholds alongside production\ndql run suite.dql --shadow suite_proposed.dql --date 2024-12-25\n\n# Promote after validation\ndql promote suite.dql --assertion \"orders.volume.dod_stability\"\n\n# Rollback if needed\ndql rollback suite.dql --to 2024-12-15\n</code></pre> <p>4. Annotation for Human Review:</p> <pre><code>check \"Critical\" on orders {\n    @required\n    @human_review                    # Any change requires approval\n    assert null_count(customer_id) == 0\n        name \"orders.critical.customer_id\"\n        severity P0\n}\n</code></pre> Annotation Behavior <code>@human_review</code> Threshold changes go to review queue <code>@auto_approve</code> Algorithm changes apply immediately (low-risk) <p>Multi-Checkpoint Joint Training:</p> <p>DQX checks data at multiple points in the system. Joint RL training optimizes across all checkpoints by learning correlations from historical data \u2014 no explicit DAG required:</p> <pre><code>class JointTrainingAgent:\n    \"\"\"RL agent that learns correlations across checkpoints.\"\"\"\n\n    def __init__(self, suites: list[str], db):\n        self.suites = {s: Suite.load(s) for s in suites}\n\n    def learn_correlations(self, days: int = 90):\n        \"\"\"Discover correlations from historical co-failures.\"\"\"\n        history = self.db.query(\n            \"\"\"\n            SELECT date, assertion_name, passed FROM dql_results\n            WHERE date &gt; current_date - interval '{days} days'\n        \"\"\"\n        )\n        failure_matrix = self.pivot_failures(history)\n        self.correlation_matrix = np.corrcoef(failure_matrix.T)\n\n    def get_state(self) -&gt; np.ndarray:\n        \"\"\"Combined state from all checkpoints.\"\"\"\n        params = []\n        for suite in self.suites.values():\n            params.extend([p.normalized_value for p in suite.get_tunable_params()])\n        return np.array(params)\n\n    def compute_reward(self, results: dict) -&gt; float:\n        \"\"\"Reward considers learned correlations.\"\"\"\n        reward = 0.0\n        for a in results[\"assertions\"]:\n            if not a[\"passed\"] and a.get(\"is_fp\"):\n                reward -= a[\"cost\"][\"false_positive\"]\n            # Bonus for catching correlated failures early\n            for corr_name, strength in self.get_correlated(a[\"name\"]):\n                if self.also_failed(results, corr_name):\n                    reward += 5 * strength\n        return reward\n</code></pre> <p>Key insight: Correlations are learned from failure patterns, not declared. The system discovers relationships automatically.</p> <p>Handling Partial Observations:</p> <p>In real systems, when one component fails, downstream checks often don't run:</p> <pre><code>Day 1: [ingestion \u2713] \u2192 [transform \u2713] \u2192 [serving \u2713]    # Full run\nDay 2: [ingestion \u2717] \u2192 [transform ?] \u2192 [serving ?]    # Stopped early\nDay 3: [ingestion \u2713] \u2192 [transform \u2717] \u2192 [serving ?]    # Partial run\n</code></pre> <p>Solution 1: Run Status as Signal</p> <pre><code>def run_and_observe(self, target_date: date) -&gt; dict:\n    results = {\"assertions\": [], \"run_status\": {}}\n    for name, suite in self.suites.items():\n        try:\n            result = suite.run(db=self.db, date=target_date)\n            results[\"assertions\"].extend(result.assertions)\n            results[\"run_status\"][name] = \"completed\"\n        except UpstreamFailure:\n            results[\"run_status\"][name] = \"skipped\"\n    return results\n\n\ndef get_state(self) -&gt; np.ndarray:\n    params = self.get_threshold_params()\n    run_mask = [1.0 if s == \"completed\" else 0.0 for s in self.run_status.values()]\n    return np.concatenate([params, run_mask])\n</code></pre> <p>Solution 2: Per-Checkpoint Training</p> <pre><code>class PerCheckpointAgents:\n    \"\"\"Each checkpoint trains independently on days it ran.\"\"\"\n\n    def train_episode(self, target_date: date):\n        for name, agent in self.agents.items():\n            if self.did_run(name, target_date):\n                agent.train_step(target_date)\n</code></pre> <p>Solution 3: Reward Shaping for Early Stops</p> <pre><code>def compute_reward(self, results: dict) -&gt; float:\n    reward = 0.0\n    for name, status in results[\"run_status\"].items():\n        if status == \"completed\":\n            reward += self.assertion_rewards(results, name)\n        elif status == \"skipped\":\n            reward += 5  # Bonus: upstream caught issue early\n    return reward\n</code></pre> Approach When to use Run status as signal Simple, works with existing data Per-checkpoint agents Loosely coupled checkpoints Reward shaping Always \u2014 \"skipped\" = early detection <p>Key insight: \"Skipped because upstream failed\" is valuable signal \u2014 catching issues early is the desired behavior.</p>"},{"location":"design/dql-language/#architecture","title":"Architecture","text":"<p>VerificationSuite parses the DQL AST and builds Python check functions dynamically:</p> <pre><code># DQL parsing happens in VerificationSuite.__init__()\nsuite = VerificationSuite(\n    dql=Path(\"suite.dql\"),\n    db=db,\n)\n\n# Internally:\n# 1. Parse DQL file to AST\n# 2. Extract tunables and create Tunable objects\n# 3. Build check functions from Check AST nodes\n# 4. Each check function evaluates metric expressions using sympy\n# 5. Assertions are registered in the dependency graph\n</code></pre> <p>VerificationSuite uses sympy to parse metric expressions. When it parses <code>abs(day_over_day(average(price)))</code>: 1. <code>abs</code> resolves to <code>sp.Abs</code> from the metric namespace</p> <ol> <li><code>day_over_day</code>, <code>average</code> resolve to provider method wrappers</li> <li><code>sympify()</code> builds the expression tree</li> </ol>"},{"location":"design/dql-language/#runtime-errors","title":"Runtime Errors","text":"<p>The interpreter reports errors with source location:</p> <pre><code>error: assertion failed\n  --&gt; suite.dql:15:9\n   |\n15 |     assert null_count(customer_id) == 0  severity P0\n   |            ^^^^^^^^^^^^^^^^^^^^^^^^^\n   |\n   = metric value: 42\n   = expected: == 0\n   = dataset: orders\n   = date: 2024-12-25\n</code></pre>"},{"location":"design/dql-language/#error-messages","title":"Error Messages","text":"<p>The interpreter reports errors with file location and context:</p> <pre><code>error[E001]: unknown metric 'avg'\n  --&gt; suite.dql:12:16\n   |\n12 |     assert avg(price) &gt; 0\n   |            ^^^ did you mean 'average'?\n\nerror[E002]: duplicate assertion name\n  --&gt; suite.dql:18:9\n   |\n14 |         name \"Order count\"\n   |              -------------- first defined here\n18 |         name \"Order count\"\n   |              ^^^^^^^^^^^^^^ duplicate\n</code></pre> <p>Warning for unnamed assertions:</p> <pre><code>warning[W001]: assertion has no name\n  --&gt; suite.dql:22:5\n   |\n22 |     assert num_rows() &gt; 0\n   |     ^^^^^^^^^^^^^^^^^^^^^ consider adding a descriptive name\n</code></pre>"},{"location":"design/dql-language/#grammar-reference","title":"Grammar Reference","text":"<pre><code>(* === Top-level === *)\nsuite       = \"suite\" STRING \"{\" suite_body \"}\"\nsuite_body  = (metadata | tunable | check)*\n\nmetadata    = \"availability_threshold\" PERCENT\n\n(* === Tunables === *)\ntunable     = \"tunable\" IDENT \"=\" expr \"bounds\" \"[\" expr \",\" expr \"]\"\n\n(* === Checks and Assertions === *)\ncheck       = \"check\" STRING \"on\" datasets \"{\" assertion+ \"}\"\ndatasets    = ident (\",\" ident)*\n\nannotation  = \"@\" IDENT [\"(\" ann_args \")\"]\nann_args    = IDENT \"=\" (NUMBER | STRING) (\",\" IDENT \"=\" (NUMBER | STRING))*\n\nassertion   = annotation* \"assert\" expr condition modifiers*\ncondition   = comparison | \"between\" bound \"and\" bound | \"is\" keyword\ncomparison  = (\"&lt;\" | \"&lt;=\" | \"&gt;\" | \"&gt;=\" | \"==\" | \"!=\") expr\nbound       = bound_term ((\"*\"|\"/\") bound_term)*   (* restricted to avoid 'and' ambiguity *)\nbound_term  = NUMBER | PERCENT | ident | call\nkeyword     = \"positive\" | \"negative\" | \"None\" | \"not\" \"None\"\nmodifiers   = name | tolerance | severity | tags | sample\nname        = \"name\" STRING\ntolerance   = (\"tolerance\" | \"+/-\" | \"\u00b1\") NUMBER\nseverity    = \"severity\" SEVERITY\ntags        = \"tags\" \"[\" IDENT (\",\" IDENT)* \"]\"\nsample      = \"sample\" (PERCENT | NUMBER \"rows\") [\"seed\" NUMBER]\n\n(* === Expressions === *)\nexpr        = term ((\"+\"|\"-\") term)*\nterm        = factor ((\"*\"|\"/\") factor)*\nfactor      = \"-\" factor | NUMBER | PERCENT | call | \"(\" expr \")\" | ident | \"None\" | STRING\ncall        = qualified_ident \"(\" [args] \")\"\nargs        = arg (\",\" arg)*\narg         = named_arg | list_arg | expr\nnamed_arg   = \"lag\" \"=\" NUMBER | \"dataset\" \"=\" ident | \"order_by\" \"=\" ident | \"n\" \"=\" NUMBER\nlist_arg    = \"[\" ident (\",\" ident)* \"]\"\nqualified_ident = IDENT (\".\" IDENT)*\n\n(* === Tokens === *)\nSTRING      = '\"' (ESC | [^\"\\\\])* '\"'\nESC         = '\\\\' [\"\\\\nrt]           (* \\\" \\\\ \\n \\r \\t *)\nNUMBER      = [0-9]+ ('.' [0-9]+)?\nPERCENT     = NUMBER '%'\nSEVERITY    = 'P0' | 'P1' | 'P2' | 'P3'\nIDENT       = [a-zA-Z_] [a-zA-Z0-9_]*\nident       = IDENT | '`' [^`]+ '`'           (* backticks escape reserved words *)\nCOMMENT     = '#' [^\\n]*                  (* Python-style comments *)\n</code></pre>"},{"location":"design/dql-language/#grammar-notes","title":"Grammar Notes","text":"<p>Named arguments: Function arguments use specific keywords (<code>lag</code>, <code>dataset</code>, <code>order_by</code>, <code>n</code>) rather than arbitrary identifiers to avoid parser ambiguity.</p> <p>Between bounds: The <code>between A and B</code> condition restricts bounds to simple expressions (numbers, percentages, identifiers, function calls, and <code>*</code>/<code>/</code> operators). Full arithmetic with <code>+</code>/<code>-</code> would conflict with the <code>and</code> keyword. Use parenthesized comparisons for complex bounds: <code>&gt;= (A + B)</code>.</p> <p>Annotation values: Annotation arguments accept only NUMBER or STRING values, not full expressions.</p>"},{"location":"design/dql-language/#reserved-words","title":"Reserved Words","text":"<p>The following are reserved: <code>suite</code>, <code>check</code>, <code>assert</code>, <code>on</code>, <code>from</code>, <code>to</code>, <code>by</code>, <code>in</code>, <code>and</code>, <code>is</code>, <code>between</code>, <code>profile</code>, <code>type</code>, <code>tunable</code>, <code>bounds</code>, <code>name</code>, <code>severity</code>, <code>tags</code>, <code>tolerance</code>, <code>scale</code>, <code>disable</code>, <code>set</code>, <code>sample</code>, <code>seed</code>, <code>rows</code>, <code>lag</code>, <code>dataset</code>, <code>order_by</code>, <code>n</code>.</p> <p>Use backticks to escape column or dataset names that conflict:</p> <pre><code>check \"Test\" on `from` {              # 'from' as dataset name\n    assert count(`to`) &gt; 0            # 'to' as column name\n}\n</code></pre>"},{"location":"design/dql-language/#design-decisions","title":"Design Decisions","text":""},{"location":"design/dql-language/#direct-execution-architecture","title":"Direct Execution Architecture","text":"<p>DQL is parsed and executed directly by VerificationSuite. No compilation step. Metric expressions pass to sympy for parsing, enabling full compatibility with DQX's Python runtime.</p> <p>Benefits: - Faster iteration \u2014 No build step between edit and run - Better errors \u2014 Messages point to DQL source, not generated code - Full sympy support \u2014 All math functions (<code>abs</code>, <code>sqrt</code>, <code>log</code>, <code>exp</code>, <code>min</code>, <code>max</code>) work</p>"},{"location":"design/dql-language/#string-based-expressions","title":"String-Based Expressions","text":"<p>DQL metric expressions are strings that pass directly to <code>sympy.sympify()</code>. This design: - Matches DQX's existing <code>MetricExpressionParser</code> - Enables arbitrary arithmetic without grammar changes - Supports all whitelisted sympy functions</p> <pre><code>assert abs(day_over_day(average(price))) &lt; 0.1   # sympy handles this\nassert sqrt(variance(score)) &lt; 10                 # no special grammar needed\n</code></pre>"},{"location":"design/dql-language/#validation-before-execution","title":"Validation Before Execution","text":"<p>DQX validates before running:</p> <ul> <li>Unknown metric functions</li> <li>Invalid severity levels</li> <li>Duplicate names</li> <li>Type mismatches</li> </ul> <p>Errors surface immediately with source location.</p>"},{"location":"design/dql-language/#implementation-status","title":"Implementation Status","text":""},{"location":"design/dql-language/#implemented","title":"Implemented \u2705","text":"<p>The following features from this design are already implemented in the DQX codebase:</p>"},{"location":"design/dql-language/#conditions_1","title":"Conditions","text":"Condition Description Location <code>!= N</code> Not equal to N \u2705 <code>is_neq()</code> in <code>api.py</code> AssertionReady <code>is None</code> Value is None \u2705 <code>is_none()</code> in <code>api.py</code> AssertionReady <code>is not None</code> Value is not None \u2705 <code>is_not_none()</code> in <code>api.py</code> AssertionReady"},{"location":"design/dql-language/#utility-functions","title":"Utility Functions","text":"Function Description Location <code>coalesce(expr, ...)</code> Return first non-None value \u2705 <code>Coalesce</code> class in <code>functions.py</code>"},{"location":"design/dql-language/#metric-parameters","title":"Metric Parameters","text":"Parameter Description Location <code>order_by</code> for <code>first()</code> Sort before taking first value \u2705 <code>first()</code> in <code>provider.py</code>"},{"location":"design/dql-language/#annotations","title":"Annotations","text":"Annotation Description Location <code>@experimental</code> Mark algorithm-proposed assertions \u2705 <code>experimental</code> param in <code>api.py</code> AssertionDraft.where() <code>@required</code> Prevent removal by algorithms \u2705 <code>required</code> param in <code>api.py</code> AssertionDraft.where() <code>@cost(fp, fn)</code> False positive/negative costs for RL reward \u2705 <code>cost={\"fp\": N, \"fn\": M}</code> param in <code>api.py</code> AssertionDraft.where()"},{"location":"design/dql-language/#tunable-constants","title":"Tunable Constants","text":"Feature Description Location <code>tunable</code> constants Bounded parameters for RL optimization \u2705 <code>TunableFloat</code>, <code>TunablePercent</code>, <code>TunableInt</code>, <code>TunableChoice</code> in <code>tunables.py</code> <code>suite.get_tunable_params()</code> List tunable params with bounds \u2705 <code>VerificationSuite.get_tunable_params()</code> in <code>api.py</code> <code>suite.set_param(name, value)</code> Modify threshold within bounds \u2705 <code>VerificationSuite.set_param()</code> in <code>api.py</code> <code>suite.get_param_history(name)</code> Get change history for tunable \u2705 <code>VerificationSuite.get_param_history()</code> in <code>api.py</code>"},{"location":"design/dql-language/#not-yet-implemented","title":"Not Yet Implemented \u274c","text":"<p>The following features are specified in this design but require implementation:</p>"},{"location":"design/dql-language/#dql-core-parser-interpreter","title":"DQL Core (Parser &amp; Interpreter)","text":"Feature Description Priority Lexer/Tokenizer Tokenize DQL source into tokens High Parser Parse tokens into AST nodes High AST Nodes Suite, Check, Assertion, Profile, etc. High Interpreter Execute AST against DQX runtime High Sampling <code>sample N%</code> / <code>sample N rows</code> with optional <code>seed</code> Medium CLI (<code>dql run</code>, <code>dql check</code>) Command-line interface Medium"},{"location":"design/dql-language/#rl-agent-integration_1","title":"RL Agent Integration","text":"Feature Description Priority <code>Suite.load(path)</code> Parse DQL into manipulable Suite object High <code>suite.save()</code> Persist changes back to <code>.dql</code> file Medium"},{"location":"design/dql-language/#history-auditing","title":"History &amp; Auditing","text":"Feature Description Priority <code>.dql.history</code> file JSONL change log alongside DQL source Medium <code>History.log(entry)</code> Append action records Medium <code>dql history</code> command View change history Low <code>dql rollback --to DATE</code> Revert to previous state Low <code>dql review</code> command Approve/reject pending changes Low"},{"location":"design/dql-language/#human-in-the-loop","title":"Human-in-the-Loop","text":"Feature Description Priority <code>@human_review</code> annotation Changes require approval Low <code>@auto_approve</code> annotation Low-risk changes apply immediately Low <code>dql label</code> command Label false positives for reward tuning Low Shadow mode Test proposed changes without production impact Low <code>dql promote</code> command Promote experimental to production Low"},{"location":"design/dql-language/#future-extensions","title":"Future Extensions","text":""},{"location":"design/dql-language/#planned","title":"Planned","text":"Feature Description Example Row-level checks Assert condition on every row <code>assert each row: price &gt; 0</code> Named shortcuts Concise syntax for common patterns <code>assert unique(customer_id)</code> Freshness checks Data recency validation <code>assert freshness(updated_at) &lt; 1 hour</code> Timeliness checks SLA compliance validation <code>assert timeliness(partition_date) by 06:00 UTC</code> Schema validation Assert table structure <code>assert column email exists</code>"},{"location":"design/dql-language/#considered","title":"Considered","text":"Feature Description Example Statistical bounds Anomaly detection via z-score <code>assert average(price) within 3 stddev</code> Referential integrity Foreign key validation <code>assert values(order_id) in values(id, dataset=orders)</code> Distribution checks Categorical distribution matching <code>assert distribution(status) matches {...}</code> Percentile metrics P50, P95, P99 calculations <code>assert percentile(latency, 0.99) &lt; 500</code> LSP server IDE autocomplete and diagnostics \u2014"},{"location":"design/dql-language/#named-shortcuts-planned","title":"Named Shortcuts (Planned)","text":"<p>Syntactic sugar for common data quality patterns:</p> <pre><code>assert unique(customer_id)              # duplicate_count([customer_id]) == 0\nassert not_null(email)                  # null_count(email) == 0\nassert not_null(email, phone, address)  # Multiple columns\nassert positive(price)                  # minimum(price) &gt; 0\nassert non_negative(quantity)           # minimum(quantity) &gt;= 0\nassert in_set(status, [\"A\", \"B\", \"C\"])  # All values in allowed set\nassert not_empty()                      # num_rows() &gt; 0\n</code></pre>"},{"location":"design/dql-language/#freshness-timeliness-planned","title":"Freshness &amp; Timeliness (Planned)","text":"<pre><code># Freshness: How old is the newest data?\nassert freshness(updated_at) &lt; 2 hours\n    name \"Data refreshed recently\"\n\n# Timeliness: Did data arrive on schedule?\nassert timeliness(partition_date) by 06:00 UTC\n    name \"Daily load completed by SLA\"\n\n# Data lag: Time since partition date\nassert data_lag(partition_date) &lt; 18 hours\n    name \"Data available within 18 hours\"\n</code></pre>"},{"location":"design/dql-language/#row-level-checks-planned","title":"Row-Level Checks (Planned)","text":"<pre><code># All rows must satisfy condition\nassert each row: price &gt; 0\n    name \"All prices positive\"\n\nassert each row: status in [\"pending\", \"shipped\", \"delivered\"]\n    name \"Valid status values\"\n\n# With percentage tolerance (inspired by Great Expectations \"mostly\" parameter)\nassert 95% of rows: email matches \"^[^@]+@[^@]+$\"\n    name \"Most emails valid format\"\n</code></pre>"},{"location":"design/profiles/","title":"Profiles: Configurable Data Quality Rules","text":""},{"location":"design/profiles/#problem","title":"Problem","text":"<p>Data quality checks fail during holiday seasons. Order volumes drop. User behavior shifts. Metrics that pass on normal days fail on Christmas.</p> <p>Teams need three capabilities:</p> <ol> <li>Disable checks that do not apply during holidays</li> <li>Compensate metrics by scaling values to account for expected changes</li> <li>Adjust severity to reduce alert noise during known disruption periods</li> </ol>"},{"location":"design/profiles/#solution","title":"Solution","text":"<p>Profiles modify assertion behavior during specific periods. A profile activates based on the current date and applies rules: disable assertions, scale metric values, or adjust severity.</p> <pre><code>christmas = HolidayProfile(\n    name=\"Christmas 2024\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[\n        tag(\"xmas\").set(metric_multiplier=2.0),\n        tag(\"non-critical\").set(severity=\"P3\"),  # Downgrade during holidays\n        check(\"Volume Check\").disable(),\n    ],\n)\n\nsuite = VerificationSuite(\n    checks=[volume_check, quality_check], db=db, name=\"My Suite\", profiles=[christmas]\n)\n</code></pre>"},{"location":"design/profiles/#key-concepts","title":"Key Concepts","text":""},{"location":"design/profiles/#dqx-constructs","title":"DQX Constructs","text":"Construct Purpose Example Check Groups related assertions <code>@check(name=\"Volume Check\")</code> Assertion Single validation rule <code>ctx.assert_that(orders).is_gt(100)</code> Threshold Pass/fail boundary The <code>100</code> in <code>is_gt(100)</code> <p>A check contains one or more assertions. Each assertion compares a metric against a threshold.</p> <pre><code>@check(name=\"Volume Check\")\ndef volume_check(mp, ctx):\n    ctx.assert_that(mp.sum(\"orders\")).where(\n        name=\"Daily orders above minimum\", tags={\"volume\", \"xmas\"}\n    ).is_gt(100)\n</code></pre>"},{"location":"design/profiles/#profile","title":"Profile","text":"<p>A profile activates during a date range and applies rules to matching assertions.</p> <pre><code>@runtime_checkable\nclass Profile(Protocol):\n    name: str\n\n    def is_active(self, target_date: date) -&gt; bool: ...\n\n    @property\n    def rules(self) -&gt; list[Rule]: ...\n</code></pre> <p>The protocol enables future profile types with different activation logic.</p>"},{"location":"design/profiles/#rule","title":"Rule","text":"<p>A rule selects assertions and specifies an action.</p> <pre><code>@dataclass(frozen=True)\nclass Rule:\n    selector: Selector\n    disabled: bool = False\n    metric_multiplier: float = 1.0\n    severity: SeverityLevel | None = None\n</code></pre>"},{"location":"design/profiles/#selector","title":"Selector","text":"<p>Selectors identify which assertions a rule targets.</p> <p>AssertionSelector matches by check name and assertion name:</p> <pre><code>assertion(\"Volume Check\", \"Daily orders above minimum\")  # exact match\ncheck(\"Volume Check\")  # all assertions in check\n</code></pre> <p>TagSelector matches by tag:</p> <pre><code>tag(\"xmas\")\n</code></pre>"},{"location":"design/profiles/#detailed-design","title":"Detailed Design","text":""},{"location":"design/profiles/#core-types","title":"Core Types","text":"<p>Create <code>src/dqx/profiles.py</code>:</p> <pre><code>from __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom datetime import date\nfrom typing import Protocol, runtime_checkable\n\n\nSelector = AssertionSelector | TagSelector\n\n\n@dataclass(frozen=True)\nclass AssertionSelector:\n    \"\"\"Matches assertions by check and assertion name.\"\"\"\n\n    check: str\n    assertion: str | None = None\n\n    def matches(self, check_name: str, assertion_name: str) -&gt; bool:\n        if check_name != self.check:\n            return False\n        if self.assertion is None:\n            return True\n        return assertion_name == self.assertion\n\n\n@dataclass(frozen=True)\nclass TagSelector:\n    \"\"\"Matches assertions by tag.\"\"\"\n\n    tag: str\n\n    def matches(self, tags: frozenset[str]) -&gt; bool:\n        return self.tag in tags\n\n\n@dataclass(frozen=True)\nclass Rule:\n    \"\"\"Pairs a selector with an action.\"\"\"\n\n    selector: Selector\n    disabled: bool = False\n    metric_multiplier: float = 1.0\n    severity: SeverityLevel | None = None\n\n\n@runtime_checkable\nclass Profile(Protocol):\n    \"\"\"Base protocol for all profile types.\"\"\"\n\n    name: str\n\n    def is_active(self, target_date: date) -&gt; bool: ...\n\n    @property\n    def rules(self) -&gt; list[Rule]: ...\n\n\n@dataclass\nclass HolidayProfile:\n    \"\"\"Profile active during a date range.\"\"\"\n\n    name: str\n    start_date: date\n    end_date: date\n    rules: list[Rule] = field(default_factory=list)\n\n    def is_active(self, target_date: date) -&gt; bool:\n        return self.start_date &lt;= target_date &lt;= self.end_date\n</code></pre>"},{"location":"design/profiles/#builder-functions","title":"Builder Functions","text":"<p>Builders provide a fluent API for creating rules:</p> <pre><code>class RuleBuilder:\n    \"\"\"Constructs rules with a fluent interface.\"\"\"\n\n    def __init__(self, selector: Selector):\n        self._selector = selector\n\n    def disable(self) -&gt; Rule:\n        return Rule(selector=self._selector, disabled=True)\n\n    def set(\n        self, *, metric_multiplier: float = 1.0, severity: SeverityLevel | None = None\n    ) -&gt; Rule:\n        return Rule(\n            selector=self._selector,\n            metric_multiplier=metric_multiplier,\n            severity=severity,\n        )\n\n\ndef assertion(check: str, name: str | None = None) -&gt; RuleBuilder:\n    \"\"\"Select by check and assertion name.\"\"\"\n    return RuleBuilder(AssertionSelector(check=check, assertion=name))\n\n\ndef check(name: str) -&gt; RuleBuilder:\n    \"\"\"Select all assertions in a check.\"\"\"\n    return RuleBuilder(AssertionSelector(check=name, assertion=None))\n\n\ndef tag(name: str) -&gt; RuleBuilder:\n    \"\"\"Select assertions with a specific tag.\"\"\"\n    return RuleBuilder(TagSelector(tag=name))\n</code></pre>"},{"location":"design/profiles/#profile-resolution","title":"Profile Resolution","text":"<p>The evaluator resolves rules before evaluating each assertion:</p> <pre><code>@dataclass\nclass ResolvedOverrides:\n    \"\"\"Accumulated overrides from all matching rules.\"\"\"\n\n    disabled: bool = False\n    metric_multiplier: float = 1.0\n    severity: SeverityLevel | None = None\n\n\ndef resolve_overrides(\n    check_name: str,\n    assertion: AssertionNode,\n    profiles: list[Profile],\n    target_date: date,\n) -&gt; ResolvedOverrides:\n    \"\"\"Apply all matching rules from active profiles.\"\"\"\n\n    result = ResolvedOverrides()\n\n    for profile in profiles:\n        if not profile.is_active(target_date):\n            continue\n\n        for rule in profile.rules:\n            if not _matches(rule.selector, check_name, assertion):\n                continue\n\n            if rule.disabled:\n                result.disabled = True\n\n            result.metric_multiplier *= rule.metric_multiplier\n\n            if rule.severity is not None:\n                result.severity = rule.severity\n\n    return result\n\n\ndef _matches(\n    selector: Selector,\n    check_name: str,\n    assertion: AssertionNode,\n) -&gt; bool:\n    match selector:\n        case AssertionSelector():\n            return selector.matches(check_name, assertion.name)\n        case TagSelector():\n            return selector.matches(assertion.tags)\n</code></pre>"},{"location":"design/profiles/#metric-multiplier","title":"Metric Multiplier","text":"<p>The multiplier scales the computed metric value before comparison. This compensates for expected metric changes during the profile period.</p> <p>Example: Orders drop 50% during Christmas.</p> <pre><code># Assertion: orders &gt; 100\n# Christmas day: orders = 60\n\n# Without profile: 60 &gt; 100 \u2192 FAILED\n# With metric_multiplier=2.0: 60 \u00d7 2.0 = 120 &gt; 100 \u2192 PASSED\n</code></pre> <p>The evaluator applies the multiplier:</p> <pre><code># In Evaluator.visit():\nmatch node._metric:\n    case Success(value):\n        adjusted = value * overrides.metric_multiplier\n        passed = node.validator.fn(adjusted)\n        node._result = \"PASSED\" if passed else \"FAILED\"\n</code></pre>"},{"location":"design/profiles/#severity-override","title":"Severity Override","text":"<p>The severity override changes an assertion's priority level. Use it to reduce alert noise during periods of expected disruption.</p> <p>Example: Non-critical checks trigger pages during normal operations but should only log during holidays.</p> <pre><code># Assertion defined with severity P1\nctx.assert_that(orders).where(name=\"Order count\", severity=\"P1\", tags={\"non-critical\"})\n\n# During Christmas, downgrade to P3\ntag(\"non-critical\").set(severity=\"P3\")\n</code></pre> <p>The last matching rule determines severity. Unlike multipliers, severities do not compound.</p> <pre><code>rules = [\n    tag(\"volume\").set(severity=\"P2\"),\n    tag(\"xmas\").set(severity=\"P3\"),\n]\n# Assertion with both tags: severity = P3 (last match wins)\n</code></pre>"},{"location":"design/profiles/#rule-ordering","title":"Rule Ordering","text":"<p>Rules apply in definition order. Later rules compound with earlier ones (multipliers multiply, severity uses last match):</p> <pre><code>rules = [\n    tag(\"volume\").set(metric_multiplier=1.5),\n    assertion(\"Check\", \"Orders\").set(metric_multiplier=2.0),\n]\n# For \"Orders\" with tag \"volume\": multiplier = 1.5 \u00d7 2.0 = 3.0\n</code></pre>"},{"location":"design/profiles/#usage-examples","title":"Usage Examples","text":""},{"location":"design/profiles/#holiday-profile","title":"Holiday Profile","text":"<pre><code>from datetime import date\nfrom dqx.profiles import HolidayProfile, tag, assertion, check\n\nchristmas = HolidayProfile(\n    name=\"Christmas 2024\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[\n        tag(\"xmas\").set(metric_multiplier=2.0),\n        check(\"Volume Check\").disable(),\n    ],\n)\n</code></pre>"},{"location":"design/profiles/#checks-with-tags","title":"Checks with Tags","text":"<pre><code>@check(name=\"Volume Check\")\ndef volume_check(mp, ctx):\n    ctx.assert_that(mp.sum(\"orders\")).where(\n        name=\"Daily orders above minimum\", tags={\"volume\", \"xmas\"}\n    ).is_gt(100)\n\n\n@check(name=\"Quality Check\")\ndef quality_check(mp, ctx):\n    ctx.assert_that(mp.average(\"error_rate\")).where(\n        name=\"Error rate below threshold\", tags={\"quality\"}\n    ).is_lt(0.05)\n</code></pre>"},{"location":"design/profiles/#running-with-profiles","title":"Running with Profiles","text":"<pre><code>suite = VerificationSuite(\n    checks=[volume_check, quality_check],\n    db=db,\n    name=\"Daily Checks\",\n    profiles=[christmas],\n)\n\nkey = ResultKey(date(2024, 12, 25), tags={})\nsuite.run(datasources, key)  # Profile activates, rules apply\n</code></pre>"},{"location":"design/profiles/#multiple-profiles","title":"Multiple Profiles","text":"<pre><code>christmas = HolidayProfile(\n    name=\"Christmas\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[tag(\"xmas\").set(metric_multiplier=2.0)],\n)\n\nblack_friday = HolidayProfile(\n    name=\"Black Friday\",\n    start_date=date(2024, 11, 29),\n    end_date=date(2024, 12, 2),\n    rules=[check(\"Volume Check\").set(metric_multiplier=3.0)],\n)\n\nsuite = VerificationSuite(\n    checks=[...],\n    db=db,\n    name=\"Suite\",\n    profiles=[christmas, black_friday],\n)\n</code></pre>"},{"location":"design/profiles/#files-to-modify","title":"Files to Modify","text":"File Change <code>src/dqx/profiles.py</code> Create: Profile protocol, selectors, rules, builders <code>src/dqx/api.py</code> Add <code>profiles</code> to <code>VerificationSuite</code> <code>src/dqx/evaluator.py</code> Resolve and apply <code>metric_multiplier</code> before validation <code>src/dqx/__init__.py</code> Export profile types"},{"location":"design/profiles/#testing-strategy","title":"Testing Strategy","text":""},{"location":"design/profiles/#unit-tests","title":"Unit Tests","text":"<ol> <li>Selector matching \u2014 AssertionSelector and TagSelector match correctly</li> <li>Rule application \u2014 metric_multiplier compounds across rules</li> <li>Profile activation \u2014 date range logic works</li> <li>Rule ordering \u2014 later rules compound with earlier ones</li> </ol>"},{"location":"design/profiles/#integration-tests","title":"Integration Tests","text":"<ol> <li>Disabled assertion \u2014 assertion skipped when rule disables it</li> <li>Scaled metric \u2014 assertion uses adjusted value</li> <li>Multiple profiles \u2014 rules from all active profiles apply</li> <li>No active profile \u2014 assertions evaluate normally</li> </ol>"},{"location":"design/profiles/#future-extensions","title":"Future Extensions","text":"<p>The <code>Profile</code> protocol enables additional profile types:</p> <ul> <li>MaintenanceProfile \u2014 relax checks during scheduled maintenance</li> <li>RegionProfile \u2014 apply region-specific multipliers</li> <li>ABTestProfile \u2014 test different configurations</li> </ul> <p>Each implements <code>is_active()</code> and <code>rules</code> with its own activation logic.</p>"},{"location":"migration/dql-profiles-to-yaml/","title":"Migrating DQL Profiles to YAML Configuration","text":"<p>This guide helps you migrate from DQL profile syntax (removed in v0.6.0) to YAML configuration or Python API.</p>"},{"location":"migration/dql-profiles-to-yaml/#why-the-change","title":"Why the Change?","text":"<p>Profiles define runtime behavior (when/how to modify assertions), which is conceptually separate from validation logic (what to validate). Moving profiles out of DQL provides:</p> <ul> <li>Separation of concerns: DQL files define validation logic only</li> <li>Environment flexibility: Use same DQL with different configs per environment</li> <li>Easier maintenance: Update profile dates/multipliers without touching DQL</li> <li>Consistency: Profiles follow same pattern as tunables (YAML config)</li> </ul>"},{"location":"migration/dql-profiles-to-yaml/#migration-steps","title":"Migration Steps","text":""},{"location":"migration/dql-profiles-to-yaml/#step-1-identify-dql-files-with-profiles","title":"Step 1: Identify DQL Files with Profiles","text":"<p>Search for <code>profile</code> keyword in your <code>.dql</code> files:</p> <pre><code>grep -r \"profile \\\"\" *.dql\n</code></pre>"},{"location":"migration/dql-profiles-to-yaml/#step-2-extract-profile-definitions","title":"Step 2: Extract Profile Definitions","text":"<p>For each profile block in DQL, extract: - Profile name - Date range (from/to) - Rules (disable/scale/set_severity)</p>"},{"location":"migration/dql-profiles-to-yaml/#step-3-convert-to-yaml","title":"Step 3: Convert to YAML","text":"<p>Use this mapping:</p> DQL Syntax YAML Syntax <code>profile \"Name\" { from DATE to DATE ... }</code> <code>name: \"Name\"</code><code>start_date: \"DATE\"</code><code>end_date: \"DATE\"</code> <code>disable check \"CheckName\"</code> <code>action: \"disable\"</code><code>target: \"check\"</code><code>name: \"CheckName\"</code> <code>disable assertion \"Name\" in \"CheckName\"</code> Use check-level disable or add tags <code>scale check \"CheckName\" by MULTIPLIER</code> <code>action: \"scale\"</code><code>target: \"check\"</code><code>name: \"CheckName\"</code><code>multiplier: MULTIPLIER</code> <code>scale tag \"TagName\" by MULTIPLIER</code> <code>action: \"scale\"</code><code>target: \"tag\"</code><code>name: \"TagName\"</code><code>multiplier: MULTIPLIER</code> <code>set severity check \"CheckName\" to P0</code> <code>action: \"set_severity\"</code><code>target: \"check\"</code><code>name: \"CheckName\"</code><code>severity: \"P0\"</code> <code>set severity tag \"TagName\" to P0</code> <code>action: \"set_severity\"</code><code>target: \"tag\"</code><code>name: \"TagName\"</code><code>severity: \"P0\"</code>"},{"location":"migration/dql-profiles-to-yaml/#step-4-create-config-file","title":"Step 4: Create Config File","text":"<p>Create <code>config.yaml</code> next to your DQL file:</p> <pre><code>profiles:\n  - name: \"Your Profile Name\"\n    type: \"seasonal\"\n    start_date: \"YYYY-MM-DD\"\n    end_date: \"YYYY-MM-DD\"\n    rules:\n      # Add converted rules here\n</code></pre>"},{"location":"migration/dql-profiles-to-yaml/#step-5-remove-profile-blocks-from-dql","title":"Step 5: Remove Profile Blocks from DQL","text":"<p>Delete all <code>profile</code> blocks from your <code>.dql</code> files.</p>"},{"location":"migration/dql-profiles-to-yaml/#step-6-update-verificationsuite-creation","title":"Step 6: Update VerificationSuite Creation","text":"<p>Before: <pre><code>from dqx.dql import Interpreter\n\ninterp = Interpreter(db=db)\nresults = interp.run(Path(\"suite.dql\"), datasources, date.today())\n</code></pre></p> <p>After: <pre><code>from dqx.api import VerificationSuite\nfrom dqx.common import ResultKey\nfrom datetime import date\n\nsuite = VerificationSuite(\n    dql=Path(\"suite.dql\"),\n    db=db,\n    config=Path(\"config.yaml\"),  # Add this\n)\nkey = ResultKey(date.today(), {})\nsuite.run(datasources, key)\nresults = suite.collect_results()\n</code></pre></p>"},{"location":"migration/dql-profiles-to-yaml/#complete-example","title":"Complete Example","text":""},{"location":"migration/dql-profiles-to-yaml/#before-dql-with-profiles","title":"Before (DQL with profiles)","text":"<p><code>banking.dql</code>: <pre><code>suite \"Banking Transactions\" {\n    tunable MAX_NULL_RATE = 1% bounds [0%, 5%]\n\n    check \"Volume\" on transactions {\n        assert num_rows() &gt;= 10000\n            name \"Min daily transactions\"\n            tags [volume]\n    }\n\n    check \"Reconciliation\" on transactions, settlements {\n        assert abs(sum(amount, dataset=transactions) - sum(amount, dataset=settlements)) &lt; 1000\n            name \"Amount balance\"\n            tags [reconciliation, critical]\n    }\n\n    profile \"Holiday Season\" {\n        from 2024-12-20\n        to 2025-01-05\n\n        disable check \"Volume\"\n        scale tag \"reconciliation\" by 1.5\n        set severity tag \"critical\" to P0\n    }\n}\n</code></pre></p> <p>Python code: <pre><code>from dqx.dql import Interpreter\n\ninterp = Interpreter(db=db)\nresults = interp.run(\n    Path(\"banking.dql\"),\n    datasources={\"transactions\": tx_ds, \"settlements\": settle_ds},\n    date=date(2024, 12, 25),\n)\n</code></pre></p>"},{"location":"migration/dql-profiles-to-yaml/#after-dql-without-profiles","title":"After (DQL without profiles)","text":"<p><code>banking.dql</code>: <pre><code>suite \"Banking Transactions\" {\n    tunable MAX_NULL_RATE = 1% bounds [0%, 5%]\n\n    check \"Volume\" on transactions {\n        assert num_rows() &gt;= 10000\n            name \"Min daily transactions\"\n            tags [volume]\n    }\n\n    check \"Reconciliation\" on transactions, settlements {\n        assert abs(sum(amount, dataset=transactions) - sum(amount, dataset=settlements)) &lt; 1000\n            name \"Amount balance\"\n            tags [reconciliation, critical]\n    }\n}\n</code></pre></p> <p><code>banking_config.yaml</code>: <pre><code>tunables:\n  MAX_NULL_RATE: 0.01  # Override if needed\n\nprofiles:\n  - name: \"Holiday Season\"\n    type: \"seasonal\"\n    start_date: \"2024-12-20\"\n    end_date: \"2025-01-05\"\n    rules:\n      - action: \"disable\"\n        target: \"check\"\n        name: \"Volume\"\n\n      - action: \"scale\"\n        target: \"tag\"\n        name: \"reconciliation\"\n        multiplier: 1.5\n\n      - action: \"set_severity\"\n        target: \"tag\"\n        name: \"critical\"\n        severity: \"P0\"\n</code></pre></p> <p>Python code: <pre><code>from dqx.api import VerificationSuite\nfrom dqx.common import ResultKey\nfrom pathlib import Path\n\nsuite = VerificationSuite(\n    dql=Path(\"banking.dql\"),\n    db=db,\n    config=Path(\"banking_config.yaml\"),\n)\n\nsuite.run(\n    datasources=[tx_ds, settle_ds],\n    key=ResultKey(date(2024, 12, 25), {}),\n)\n\nresults = suite.collect_results()\n</code></pre></p>"},{"location":"migration/dql-profiles-to-yaml/#alternative-python-api","title":"Alternative: Python API","text":"<p>If you prefer programmatic configuration:</p> <pre><code>from dqx.api import VerificationSuite\nfrom dqx.profiles import SeasonalProfile, check, tag\nfrom datetime import date\nfrom pathlib import Path\n\nholiday = SeasonalProfile(\n    name=\"Holiday Season\",\n    start_date=date(2024, 12, 20),\n    end_date=date(2025, 1, 5),\n    rules=[\n        check(\"Volume\").disable(),\n        tag(\"reconciliation\").set(metric_multiplier=1.5),\n        tag(\"critical\").set(severity=\"P0\"),\n    ],\n)\n\nsuite = VerificationSuite(\n    dql=Path(\"banking.dql\"),\n    db=db,\n    profiles=[holiday],\n)\n</code></pre>"},{"location":"migration/dql-profiles-to-yaml/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migration/dql-profiles-to-yaml/#error-exactly-one-of-checks-or-dql-must-be-provided","title":"Error: \"Exactly one of 'checks' or 'dql' must be provided\"","text":"<p>Make sure you're using the <code>dql</code> parameter, not <code>checks</code>:</p> <pre><code># Wrong:\nsuite = VerificationSuite(checks=Path(\"suite.dql\"), db=db, name=\"Suite\")\n\n# Correct:\nsuite = VerificationSuite(dql=Path(\"suite.dql\"), db=db)\n</code></pre>"},{"location":"migration/dql-profiles-to-yaml/#error-unexpected-token-or-syntax-error-with-profile","title":"Error: \"Unexpected token\" or syntax error with profile","text":"<p>DQL no longer supports profile syntax. You need to:</p> <ol> <li>Remove the <code>profile</code> block from your DQL file</li> <li>Create a YAML config file with the profile definition</li> <li>Pass the config file to VerificationSuite via the <code>config</code> parameter</li> </ol>"},{"location":"migration/dql-profiles-to-yaml/#error-profile-configuration-invalid","title":"Error: \"Profile configuration invalid\"","text":"<p>Check your YAML syntax: - Dates must be in ISO 8601 format: <code>\"YYYY-MM-DD\"</code> - All required fields must be present (<code>name</code>, <code>type</code>, <code>start_date</code>, <code>end_date</code>, <code>rules</code>) - Rule actions must be: <code>\"disable\"</code>, <code>\"scale\"</code>, or <code>\"set_severity\"</code> - Targets must be: <code>\"check\"</code> or <code>\"tag\"</code> - <code>type</code> must be <code>\"seasonal\"</code> (currently the only supported type)</p>"},{"location":"migration/dql-profiles-to-yaml/#profiles-not-applied","title":"Profiles Not Applied","text":"<p>Verify: 1. Profile date range includes your execution date 2. Profile target names match check/tag names exactly (case-sensitive) 3. Config file is passed to <code>VerificationSuite</code> constructor: <code>config=Path(\"config.yaml\")</code> 4. Config file contains valid YAML with <code>profiles:</code> section</p>"},{"location":"migration/dql-profiles-to-yaml/#result-format-changed","title":"Result Format Changed","text":"<p>If you were using <code>Interpreter</code> and accessing <code>SuiteResults</code> or <code>AssertionResult</code>:</p> <p>Before: <pre><code>results: SuiteResults = interp.run(...)\nfor assertion in results.assertions:\n    print(assertion.assertion_name, assertion.passed)\n</code></pre></p> <p>After: <pre><code>results = suite.collect_results()\nfor result in results:\n    print(result.assertion_name, result.status == \"PASSED\")\n</code></pre></p>"},{"location":"migration/dql-profiles-to-yaml/#need-help","title":"Need Help?","text":"<p>Open an issue on GitHub.</p>"},{"location":"migration/dql-profiles-to-yaml/#summary","title":"Summary","text":"<p>The migration from DQL profiles to YAML configuration:</p> <ol> <li>Removes profile blocks from DQL files</li> <li>Creates YAML config files with profile definitions</li> <li>Updates Python code to use <code>VerificationSuite(dql=...)</code> instead of <code>Interpreter</code></li> <li>Provides more flexibility for environment-specific configuration</li> </ol> <p>This change makes DQL files focused on validation logic while keeping runtime behavior configuration separate and reusable.</p>"}]}