import datetime as dt

import pyarrow as pa
import pytest
import sympy as sp
from returns.maybe import Some
from returns.result import Failure, Success
from rich.console import Console

from dqx import specs
from dqx.api import VerificationSuite, check
from dqx.common import Context, ResultKey
from dqx.provider import MetricProvider
from dqx.display import GraphDisplay
from dqx.extensions.pyarrow_ds import ArrowDataSource
from dqx.orm.repositories import InMemoryMetricDB


@check(datasets=["ds1"])
def simple_checks(mp: MetricProvider, ctx: Context) -> None:
    ctx.assert_that(mp.null_count("delivered")).is_leq(100)
    ctx.assert_that(mp.minimum("quantity")).is_leq(2.5)
    ctx.assert_that(mp.average("price")).is_geq(10.0)
    ctx.assert_that(mp.ext.day_over_day(specs.Average("tax"))).is_geq(0.5)


@check(label="Delivered null percentage", datasets=["ds1"])
def null_percentage(mp: MetricProvider, ctx: Context) -> None:
    null_count = mp.null_count("delivered", dataset="ds1")
    nr = mp.num_rows()
    ctx.assert_that(null_count / nr).on(label="null percentage is less than 40%").is_leq(0.4)


@check
def manual_day_over_day(mp: MetricProvider, ctx: Context) -> None:
    tax_avg = mp.average("tax")
    tax_avg_lag = mp.average("tax", key=ctx.key.lag(1))
    ctx.assert_that(tax_avg / tax_avg_lag).on().is_eq(1.0, tol=0.01)


@check(label="Rate of change", datasets=["ds2"])
def rate_of_change(mp: MetricProvider, ctx: Context) -> None:
    tax_avg = mp.ext.day_over_day(specs.Maximum("tax"))
    rate = sp.Abs(tax_avg - 1.0)
    ctx.assert_that(rate).on(label="Maximum tax rate change is less than 20%").is_leq(0.2)


@check(datasets=["ds1"])
def sketch_check(mp: MetricProvider, ctx: Context) -> None:
    ctx.assert_that(mp.approx_cardinality("address")).is_geq(100)

@check(datasets=["ds1", "ds2"])
def cross_dataset_check(mp: MetricProvider, ctx: Context) -> None:
    tax_avg_1= mp.average("tax", dataset="ds1")
    tax_avg_2 = mp.average("tax", dataset="ds2")
    # Allow for identical datasets (difference can be 0)
    ctx.assert_that(sp.Abs(tax_avg_1 / tax_avg_2 - 1)).is_lt(0.2, tol=0.01)

def test_verification_suite(commerce_data_c1: pa.Table, commerce_data_c2: pa.Table) -> None:
    db = InMemoryMetricDB()
    ds1 = ArrowDataSource(commerce_data_c1)
    ds2 = ArrowDataSource(commerce_data_c2)

    key = ResultKey(yyyy_mm_dd=dt.date.fromisoformat("2025-01-15"), tags={})
    checks = [simple_checks, manual_day_over_day, rate_of_change, null_percentage, sketch_check, cross_dataset_check]

    # Run once for yesterday
    suite = VerificationSuite(checks, db, name="Simple test suite")
    suite.run({"ds1": ds1, "ds2": ds2}, key.lag(1))

    # Run for today
    suite = VerificationSuite(checks, db, name="Simple test suite")
    ctx = suite._context

    suite.run({"ds1": ds1, "ds2": ds2}, key)

    display = GraphDisplay()
    tree = display.inspect_tree(ctx._graph)
    Console().print(tree)

    # # Check for failures - we expect some assertions to fail based on the test data
    # failed_assertions = []
    # for assertion in ctx._graph.assertions():
    #     if assertion._value is not None:
    #         match assertion._value:
    #             case Some(Failure(msg)):
    #                 check_name = ""
    #                 # Find which check this assertion belongs to
    #                 for check in ctx._graph.checks():
    #                     if assertion in check.children:
    #                         check_name = check.name
    #                         break
    #                 failed_assertions.append(f"{check_name}: {assertion.label or 'unnamed assertion'} - {msg}")

    # # We expect some failures in simple_checks based on the test data
    # # The test passes as long as the validation logic works correctly
    # expected_failures = [
    #     "simple_checks: unnamed assertion - Assertion failed: x_1 = 347.0 does not satisfy ≤ 100",
    #     "simple_checks: unnamed assertion - Assertion failed: x_2 = 12.0 does not satisfy ≤ 2.5"
    # ]

    # # Print actual failures to debug
    # print("Actual failures:")
    # for failure in failed_assertions:
    #     print(f"  - {failure}")

    # # Verify we got the expected failures
    # for expected in expected_failures:
    #     assert any(expected in failure for failure in failed_assertions), f"Expected failure not found: {expected}. Actual failures: {failed_assertions}"


@check(label="Chained assertions test", datasets=["ds1"])
def chained_assertions_check(mp: MetricProvider, ctx: Context) -> None:
    """Test chained assertions functionality."""
    # Basic chaining - ratio should be between 0.95 and 1.05
    ratio = mp.average("price") / mp.average("tax")
    ctx.assert_that(ratio).is_geq(0.95).is_leq(1.05)

    # Chaining with on() method
    quantity = mp.average("quantity")
    ctx.assert_that(quantity).on(label="Quantity range check").is_gt(0).is_lt(100)

    # Multiple chains
    tax_sum = mp.sum("tax")
    ctx.assert_that(tax_sum).is_positive().is_lt(10000).is_geq(100)


def test_chained_assertions(commerce_data_c1: pa.Table) -> None:
    """Test that chained assertions create multiple assertion nodes."""
    db = InMemoryMetricDB()
    ds1 = ArrowDataSource(commerce_data_c1)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    # Create suite with chained assertions check
    suite = VerificationSuite([chained_assertions_check], db, name="Chained assertions test")

    # Create a new context and collect assertions without full dependency processing
    ctx = suite.collect(key)

    # Print the graph for visual verification - BEFORE checking assertion count
    display = GraphDisplay()
    tree = display.inspect_tree(ctx._graph)
    Console().print(tree)

    # Verify the graph structure
    assertions = list(ctx._graph.assertions())

    # Count assertions with validators (some might be empty intermediate nodes)
    assertions_with_validators = [a for a in assertions if hasattr(a, 'validator') and a.validator is not None]

    # We expect:
    # - 2 assertions for the ratio check (is_geq and is_leq)
    # - 2 assertions for the quantity check (is_gt and is_lt)
    # - 3 assertions for the tax_sum check (is_positive, is_lt, is_geq)
    # Total: 7 assertions with validators
    assert len(assertions_with_validators) == 7, f"Expected 7 assertions with validators, got {len(assertions_with_validators)}"

    # Verify that all assertions with validators actually have them
    for assertion in assertions_with_validators:
        assert assertion.validator is not None, "Assertion should have a validator"

    # Now run to ensure it works end-to-end
    ctx = suite.run({"ds1": ds1}, key)


# Additional comprehensive tests from test_chained_assertions_validation.py
@check(datasets=["ds1", "ds2"])
def failing_chained_assertion(mp: MetricProvider, ctx: Context) -> None:
    """Test case where chained assertion should fail."""
    # This evaluates to 0.0, which should fail is_geq(0.01)
    tax_avg_1 = mp.average("tax", datasets=["ds1"])
    tax_avg_2 = mp.average("tax", datasets=["ds2"])
    ctx.assert_that(sp.Abs(tax_avg_1 / tax_avg_2 - 1)).is_lt(0.2, tol=0.01).is_geq(0.01)


@check(datasets=["ds1"])
def passing_chained_assertion(mp: MetricProvider, ctx: Context) -> None:
    """Test case where chained assertion should pass."""
    # This should pass both conditions
    price_avg = mp.average("price", datasets=["ds1"])
    tax_avg = mp.average("tax", datasets=["ds1"])
    # Ensure ratio is around 2.0, which satisfies both conditions
    ctx.assert_that(price_avg / tax_avg).is_geq(1.5).is_leq(2.5)


@check(datasets=["dataset"])
def boundary_value_tests(mp: MetricProvider, ctx: Context) -> None:
    """Test boundary values with tolerance."""
    value = mp.average("boundary_value")

    # Test exact boundary with tolerance
    ctx.assert_that(value).on(label="Exact boundary 1.0").is_eq(1.0, tol=0.01)

    # Test just above boundary
    ctx.assert_that(value).on(label="Should pass >= 0.99").is_geq(0.99)

    # Test just below boundary
    ctx.assert_that(value).on(label="Should fail > 1.01").is_gt(1.01)


@check(datasets=["dataset"])
def multiple_chain_combinations(mp: MetricProvider, ctx: Context) -> None:
    """Test various chaining combinations."""
    metric = mp.average("test_value")

    # Chain 1: Should pass (value is 50)
    ctx.assert_that(metric).on(label="Between 40 and 60").is_gt(40).is_lt(60)

    # Chain 2: Should fail (value is 50, not <= 45)
    ctx.assert_that(metric).on(label="Should fail <= 45").is_geq(40).is_leq(45)

    # Chain 3: Three conditions
    ctx.assert_that(metric).on(label="Three conditions").is_positive().is_lt(100).is_geq(50)


@check(datasets=["dataset"])
def nan_and_infinity_handling(mp: MetricProvider, ctx: Context) -> None:
    """Test NaN and infinity handling."""
    # Test NaN: Use variance of constant column (returns 0) and divide by itself
    zero_var = mp.variance("zero_column")  # This will be 0 (variance of constants)
    ctx.assert_that(zero_var / zero_var).on(label="NaN check").is_geq(0)

    # Test infinity: Divide by minimum of zero column
    one_value = mp.average("one_column")  # Returns 1.0
    zero_min = mp.minimum("zero_column")  # Returns 0.0
    ctx.assert_that(one_value / zero_min).on(label="Infinity check").is_positive()


def test_failing_chained_assertion_should_fail() -> None:
    """Test that the chained assertion bug is fixed - should fail when value is 0.0."""
    # Create identical data for both datasets
    data = pa.table({
        "tax": [10.0, 20.0, 30.0, 40.0, 50.0]
    })

    db = InMemoryMetricDB()
    ds1 = ArrowDataSource(data)
    ds2 = ArrowDataSource(data)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    suite = VerificationSuite([failing_chained_assertion], db, name="Failing chained assertion test")

    # This should raise an exception because the assertion should fail
    with pytest.raises(Exception) as exc_info:
        ctx = suite.run({"ds1": ds1, "ds2": ds2}, key)

        # Check all assertions and find failures
        failed_assertions = []
        for assertion in ctx._graph.assertions():
            # The _value is Maybe[Result[float, str]]
            match assertion._value:
                case Some(value):
                    match value:
                        case Failure(msg):
                            failed_assertions.append(str(msg))

        if failed_assertions:
            raise AssertionError(f"Assertions failed: {failed_assertions}")

    # Verify the error message contains the expected failure
    assert "does not satisfy ≥ 0.01" in str(exc_info.value)


def test_passing_chained_assertion_should_pass() -> None:
    """Test that valid chained assertions pass correctly."""
    # Create data where price/tax ratio is 2.0
    data = pa.table({
        "price": [20.0, 40.0, 60.0, 80.0, 100.0],  # avg = 60
        "tax": [10.0, 20.0, 30.0, 40.0, 50.0]      # avg = 30
    })

    db = InMemoryMetricDB()
    ds1 = ArrowDataSource(data)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    suite = VerificationSuite([passing_chained_assertion], db, name="Passing chained assertion test")
    ctx = suite.run({"ds1": ds1}, key)

    # All assertions should pass
    for assertion in ctx._graph.assertions():
        if assertion.validator:
            match assertion._value:
                case Some(Success(_)):
                    pass  # Good, assertion passed
                case Some(Failure(msg)):
                    assert False, f"Assertion failed unexpectedly: {msg}"
                case _:
                    assert False, "Assertion not evaluated"


def test_boundary_values_with_tolerance() -> None:
    """Test boundary value handling with tolerance."""
    data = pa.table({
        "boundary_value": [0.99, 1.01, 1.0, 0.995, 1.005]  # avg = 1.0
    })

    db = InMemoryMetricDB()
    ds = ArrowDataSource(data)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    suite = VerificationSuite([boundary_value_tests], db, name="Boundary value test")

    # This should have mixed results
    ctx = suite.run({"dataset": ds}, key)

    # Collect results
    results = {}
    for assertion in ctx._graph.assertions():
        if assertion.validator and assertion.label:
            match assertion._value:
                case Some(Success(_)):
                    results[assertion.label] = True
                case Some(Failure(_)):
                    results[assertion.label] = False

    # Check expected results
    assert results.get("Exact boundary 1.0", False)  # 1.0 with tolerance 0.01
    assert results.get("Should pass >= 0.99", False)  # 1.0 >= 0.99
    assert not results.get("Should fail > 1.01", True)  # 1.0 is not > 1.01


def test_multiple_chain_combinations() -> None:
    """Test various chaining combinations."""
    data = pa.table({
        "test_value": [45, 50, 55, 50, 50]  # avg = 50
    })

    db = InMemoryMetricDB()
    ds = ArrowDataSource(data)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    suite = VerificationSuite([multiple_chain_combinations], db, name="Multiple chains test")
    ctx = suite.run({"dataset": ds}, key)

    # Collect results by label
    results: dict[str, list[bool]] = {}
    failures = []
    for assertion in ctx._graph.assertions():
        if assertion.validator:
            match assertion._value:
                case Some(Success(_)):
                    if assertion.label:
                        base_label = assertion.label
                        if base_label not in results:
                            results[base_label] = []
                        results[base_label].append(True)
                case Some(Failure(msg)):
                    if assertion.label:
                        base_label = assertion.label
                        if base_label not in results:
                            results[base_label] = []
                        results[base_label].append(False)
                        failures.append(f"{base_label}: {msg}")

    # Verify results
    # "Between 40 and 60" should have all True (50 > 40 and 50 < 60)
    assert all(results.get("Between 40 and 60", [])), "Between 40 and 60 should pass"

    # "Should fail <= 45" should have at least one False (50 is not <= 45)
    assert not all(results.get("Should fail <= 45", [True])), f"Should fail <= 45 should have failed. Failures: {failures}"

    # "Three conditions" should all pass (50 > 0, 50 < 100, 50 >= 50)
    assert all(results.get("Three conditions", [])), "Three conditions should all pass"


def test_nan_handling() -> None:
    """Test handling of NaN values in assertions."""
    # Create data that will produce NaN
    data = pa.table({
        "nan_column": [None, None, None, None, None],  # All nulls
        "zero_column": [0.0, 0.0, 0.0, 0.0, 0.0],
        "one_column": [1.0, 1.0, 1.0, 1.0, 1.0]
    })

    db = InMemoryMetricDB()
    ds = ArrowDataSource(data)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    suite = VerificationSuite([nan_and_infinity_handling], db, name="NaN handling test")
    ctx = suite.run({"dataset": ds}, key)

    # Check for NaN/infinity/complex handling
    edge_case_handled = False

    for assertion in ctx._graph.assertions():
        match assertion._value:
            case Some(Failure(msg)):
                failure_msg = str(msg)
                # Check for various edge case error messages
                if any(err in failure_msg for err in ["NaN", "inf", "complex", "infinity"]):
                    edge_case_handled = True

    # At least one edge case should be handled (complex/infinity error)
    assert edge_case_handled, "Edge cases (NaN/infinity/complex) should be properly handled"


def test_assertion_error_messages() -> None:
    """Test that assertion failures have clear error messages."""
    @check
    def detailed_failure_check(mp: MetricProvider, ctx: Context) -> None:
        value = mp.average("test_column")
        ctx.assert_that(value).on(
            label="Customer satisfaction score"
        ).is_geq(4.5)  # Will fail since average is 3.0

    data = pa.table({
        "test_column": [1.0, 2.0, 3.0, 4.0, 5.0]  # avg = 3.0
    })

    db = InMemoryMetricDB()
    ds = ArrowDataSource(data)

    key = ResultKey(yyyy_mm_dd=dt.date.today(), tags={})

    suite = VerificationSuite([detailed_failure_check], db, name="Error message test")
    ctx = suite.run({"dataset": ds}, key)

    # Find the failure message
    for assertion in ctx._graph.assertions():
        if assertion.validator:
            match assertion._value:
                case Some(Failure(msg)):
                    error_msg = str(msg)
                    # Check that error message contains all important parts
                    assert "Customer satisfaction score" in error_msg
                    assert "3" in error_msg  # The actual value
                    assert "≥ 4.5" in error_msg  # The expected condition
                    assert "does not satisfy" in error_msg
